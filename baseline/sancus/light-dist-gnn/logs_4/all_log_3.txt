15:46:26.646804 [3] proc begin: <DistEnv 3/4 nccl>
16:08:59.891765 [3] proc begin: <DistEnv 3/4 nccl>
16:10:30.560343 [3] proc begin: <DistEnv 3/4 nccl>
17:06:50.103070 [3] proc begin: <DistEnv 3/4 nccl>
17:06:56.081913 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
17:06:56.085356 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  467254 KB |  486390 KB |  524678 KB |   57424 KB |
|       from large pool |  467254 KB |  486388 KB |  524655 KB |   57400 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| Active memory         |  467254 KB |  486390 KB |  524678 KB |   57424 KB |
|       from large pool |  467254 KB |  486388 KB |  524655 KB |   57400 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  507904 KB |  507904 KB |  507904 KB |       0 B  |
|       from large pool |  505856 KB |  505856 KB |  505856 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18121 KB |   21515 KB |   59053 KB |   40932 KB |
|       from large pool |   18121 KB |   19468 KB |   36513 KB |   18391 KB |
|       from small pool |       0 KB |    2047 KB |   22540 KB |   22540 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      53    |      36    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      53    |      36    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      24    |      14    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |      11    |      11    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:07:03.304479 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:04.020437 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:04.563728 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:05.110190 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:05.651441 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:06.193470 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:06.733000 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:07.277592 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:07.818912 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:08.365644 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:08.914162 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:09.510789 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:10.062711 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:10.612029 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:11.158143 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:11.702485 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:12.246917 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:12.788543 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:13.334458 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:13.882281 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:14.428626 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:15.027229 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:15.571901 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:16.119749 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:16.663715 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:17.210148 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:17.754611 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:18.302538 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:18.844687 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:19.391827 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:19.936923 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:20.536176 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:21.081372 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:21.624872 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:22.170786 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:22.709812 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:23.253723 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:23.796154 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:24.338884 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:24.883411 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:25.425994 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:26.023229 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:26.566031 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:27.108116 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:27.645791 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:28.184857 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:28.722619 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:29.263828 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:29.800373 [3] Warning: no training nodes in this partition! Backward fake loss.
17:07:30.340580 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:47.134056 [3] proc begin: <DistEnv 3/4 nccl>
17:17:48.826421 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
17:17:48.827432 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3856 KB |    3878 KB |    3929 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      66 KB |      88 KB |     139 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3856 KB |    3878 KB |    3929 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      66 KB |      88 KB |     139 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18671 KB |   18707 KB |   18808 KB |  139776 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1981 KB |    2045 KB |    2118 KB |  139776 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:17:50.337962 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.507473 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.580192 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.642906 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.709119 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.772002 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.831644 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.893144 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.956583 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.021363 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.084857 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.157459 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.228347 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.298244 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.381845 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.447813 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.507838 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.577936 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.646247 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.712695 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.780162 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.856631 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.923716 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.990982 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.056854 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.129168 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.210681 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.285537 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.349957 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.414850 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.491272 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.559871 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.627183 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.697217 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.782691 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.886792 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.991599 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.069237 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.147432 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.223465 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.297834 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.369829 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.445472 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.519040 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.589680 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.680402 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.784802 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.893573 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.995651 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:54.106787 [3] Warning: no training nodes in this partition! Backward fake loss.
13:29:51.791466 [3] proc begin: <DistEnv 3/4 nccl>
13:30:15.137878 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
13:30:15.170017 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  357506 KB |  359328 KB |  362988 KB |    5482 KB |
|       from large pool |  355910 KB |  357731 KB |  361372 KB |    5461 KB |
|       from small pool |    1596 KB |    1598 KB |    1616 KB |      20 KB |
|---------------------------------------------------------------------------|
| Active memory         |  357506 KB |  359328 KB |  362988 KB |    5482 KB |
|       from large pool |  355910 KB |  357731 KB |  361372 KB |    5461 KB |
|       from small pool |    1596 KB |    1598 KB |    1616 KB |      20 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  385024 KB |  385024 KB |  385024 KB |       0 B  |
|       from large pool |  382976 KB |  382976 KB |  382976 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   27517 KB |   27745 KB |   34367 KB |    6850 KB |
|       from large pool |   27065 KB |   27065 KB |   32527 KB |    5461 KB |
|       from small pool |     452 KB |    1820 KB |    1840 KB |    1388 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      53    |      36    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      40    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      53    |      36    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      40    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:33:33.365119 [3] proc begin: <DistEnv 3/4 nccl>
13:33:41.454348 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
13:33:41.458634 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  357506 KB |  359328 KB |  362988 KB |    5482 KB |
|       from large pool |  355910 KB |  357731 KB |  361372 KB |    5461 KB |
|       from small pool |    1596 KB |    1598 KB |    1616 KB |      20 KB |
|---------------------------------------------------------------------------|
| Active memory         |  357506 KB |  359328 KB |  362988 KB |    5482 KB |
|       from large pool |  355910 KB |  357731 KB |  361372 KB |    5461 KB |
|       from small pool |    1596 KB |    1598 KB |    1616 KB |      20 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  385024 KB |  385024 KB |  385024 KB |       0 B  |
|       from large pool |  382976 KB |  382976 KB |  382976 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   27517 KB |   27745 KB |   34367 KB |    6850 KB |
|       from large pool |   27065 KB |   27065 KB |   32527 KB |    5461 KB |
|       from small pool |     452 KB |    1820 KB |    1840 KB |    1388 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      53    |      36    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      40    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      53    |      36    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      40    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:35:26.576413 [3] proc begin: <DistEnv 3/4 nccl>
13:35:46.659428 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
13:35:46.663416 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  467254 KB |  486390 KB |  524678 KB |   57424 KB |
|       from large pool |  467254 KB |  486388 KB |  524655 KB |   57400 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| Active memory         |  467254 KB |  486390 KB |  524678 KB |   57424 KB |
|       from large pool |  467254 KB |  486388 KB |  524655 KB |   57400 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  507904 KB |  507904 KB |  507904 KB |       0 B  |
|       from large pool |  505856 KB |  505856 KB |  505856 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18121 KB |   21515 KB |   59053 KB |   40932 KB |
|       from large pool |   18121 KB |   19468 KB |   36513 KB |   18391 KB |
|       from small pool |       0 KB |    2047 KB |   22540 KB |   22540 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      53    |      36    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      53    |      36    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      24    |      14    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |      11    |      11    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:35:57.957568 [3] Warning: no training nodes in this partition! Backward fake loss.
13:35:58.674016 [3] Warning: no training nodes in this partition! Backward fake loss.
13:35:59.219410 [3] Warning: no training nodes in this partition! Backward fake loss.
13:35:59.759963 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:00.302624 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:00.845657 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:01.390479 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:01.964663 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:02.536819 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:03.101347 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:03.645286 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:04.246938 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:04.794326 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:05.342007 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:05.889017 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:06.437428 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:06.986772 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:07.536824 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:08.089552 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:08.636652 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:09.185654 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:09.782358 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:10.327364 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:10.874776 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:11.421002 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:11.970289 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:12.516989 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:13.067186 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:13.615654 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:14.169035 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:14.716746 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:15.314895 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:15.859417 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:16.402938 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:16.942333 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:17.482420 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:18.025884 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:18.562426 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:19.107547 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:19.647893 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:20.191271 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:20.784202 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:21.332480 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:21.875885 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:22.416431 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:22.962292 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:23.502637 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:24.042133 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:24.582866 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:25.126811 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:25.666805 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:26.259937 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:26.799546 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:27.345267 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:27.888205 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:28.431461 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:28.972501 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:29.516731 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:30.057723 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:30.597598 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:31.141781 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:31.734110 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:32.276778 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:32.818959 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:33.357884 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:33.902652 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:34.447462 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:34.994380 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:35.541965 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:36.096663 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:36.643574 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:37.252359 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:37.796745 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:38.344824 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:38.891395 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:39.435394 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:39.978558 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:40.520081 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:41.059779 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:41.596540 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:42.135226 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:42.727619 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:43.266641 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:43.804180 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:44.343758 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:44.882954 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:45.425096 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:45.963095 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:46.500871 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:47.042296 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:47.580957 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:48.176705 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:48.713269 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:49.253466 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:49.789957 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:50.328025 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:50.865290 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:51.408373 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:51.951773 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:52.491603 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:53.035031 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:53.629129 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:54.170615 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:54.710791 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:55.254125 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:55.793473 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:56.335817 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:56.873198 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:57.413728 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:57.958594 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:58.498292 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:59.094628 [3] Warning: no training nodes in this partition! Backward fake loss.
13:36:59.633259 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:00.177620 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:00.717058 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:01.261699 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:01.802264 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:02.366494 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:02.938878 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:03.506189 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:04.051134 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:04.648081 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:05.198044 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:05.743804 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:06.298067 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:06.848678 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:07.396269 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:07.942328 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:08.487944 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:09.038294 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:09.581973 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:10.179594 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:10.717707 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:11.259184 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:11.795280 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:12.333840 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:12.872348 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:13.415002 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:13.959525 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:14.501348 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:15.042954 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:15.637543 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:16.182092 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:16.722325 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:17.265693 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:17.803915 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:18.344835 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:18.885588 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:19.424585 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:19.966882 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:20.508198 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:21.103874 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:21.644290 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:22.190603 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:22.729760 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:23.274160 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:23.814737 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:24.357319 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:24.899822 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:25.441343 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:25.984250 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:26.577693 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:27.123328 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:27.663431 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:28.204478 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:28.747065 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:29.293846 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:29.839802 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:30.386584 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:30.936225 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:31.485195 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:32.087268 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:32.629895 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:33.177569 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:33.715097 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:34.254414 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:34.792752 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:35.330795 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:35.867680 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:36.405129 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:36.945625 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:37.542383 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:38.084582 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:38.625037 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:39.164893 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:39.702954 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:40.243117 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:40.779576 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:41.318716 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:41.859215 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:42.396629 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:42.985671 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:43.527004 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:44.066062 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:44.605263 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:45.146165 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:45.681500 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:46.220987 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:46.762553 [3] Warning: no training nodes in this partition! Backward fake loss.
13:37:47.302394 [3] Warning: no training nodes in this partition! Backward fake loss.
13:39:46.276108 [3] proc begin: <DistEnv 3/4 nccl>
13:39:47.792714 [3] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 3, |V|: 42336, |E|: 334102>
13:39:47.795321 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   26261 KB |   27623 KB |   30350 KB |    4088 KB |
|       from large pool |   22491 KB |   23851 KB |   26571 KB |    4080 KB |
|       from small pool |    3770 KB |    3772 KB |    3779 KB |       8 KB |
|---------------------------------------------------------------------------|
| Active memory         |   26261 KB |   27623 KB |   30350 KB |    4088 KB |
|       from large pool |   22491 KB |   23851 KB |   26571 KB |    4080 KB |
|       from small pool |    3770 KB |    3772 KB |    3779 KB |       8 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   47104 KB |   47104 KB |   47104 KB |       0 B  |
|       from large pool |   43008 KB |   43008 KB |   43008 KB |       0 B  |
|       from small pool |    4096 KB |    4096 KB |    4096 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   20842 KB |   22323 KB |   28249 KB |    7407 KB |
|       from large pool |   20517 KB |   20517 KB |   24597 KB |    4080 KB |
|       from small pool |     325 KB |    1882 KB |    3652 KB |    3327 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      37    |      20    |
|       from large pool |       2    |       3    |       5    |       3    |
|       from small pool |      15    |      18    |      32    |      17    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      37    |      20    |
|       from large pool |       2    |       3    |       5    |       3    |
|       from small pool |      15    |      18    |      32    |      17    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       2    |       2    |       2    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       7    |       3    |
|       from large pool |       2    |       2    |       5    |       3    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:55:32.312200 [3] proc begin: <DistEnv 3/4 nccl>
13:55:38.264292 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
13:55:38.267375 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  467254 KB |  486390 KB |  524678 KB |   57424 KB |
|       from large pool |  467254 KB |  486388 KB |  524655 KB |   57400 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| Active memory         |  467254 KB |  486390 KB |  524678 KB |   57424 KB |
|       from large pool |  467254 KB |  486388 KB |  524655 KB |   57400 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  507904 KB |  507904 KB |  507904 KB |       0 B  |
|       from large pool |  505856 KB |  505856 KB |  505856 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18121 KB |   21515 KB |   59053 KB |   40932 KB |
|       from large pool |   18121 KB |   19468 KB |   36513 KB |   18391 KB |
|       from small pool |       0 KB |    2047 KB |   22540 KB |   22540 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      53    |      36    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      53    |      36    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      24    |      14    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |      11    |      11    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:55:43.804548 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:44.513479 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:45.050086 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:45.591362 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:46.131237 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:46.673922 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:47.213485 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:47.759044 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:48.297222 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:48.841171 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:49.382424 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:49.975982 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:50.517807 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:51.056162 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:51.594790 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:52.135020 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:52.672740 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:53.207787 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:53.743987 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:54.278587 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:54.816768 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:55.406655 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:55.944055 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:56.480979 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:57.021295 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:57.558320 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:58.094096 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:58.634747 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:59.171133 [3] Warning: no training nodes in this partition! Backward fake loss.
13:55:59.710286 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:00.241926 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:00.835434 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:01.375190 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:01.941732 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:02.506980 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:03.063940 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:03.603883 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:04.144207 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:04.683313 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:05.221278 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:05.760634 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:06.350136 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:06.890769 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:07.427272 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:07.965851 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:08.504423 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:09.041690 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:09.580972 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:10.117304 [3] Warning: no training nodes in this partition! Backward fake loss.
13:56:10.657909 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:09.576104 [3] proc begin: <DistEnv 3/4 nccl>
13:59:11.811050 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
13:59:11.812324 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3856 KB |    3878 KB |    3929 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      66 KB |      88 KB |     139 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3856 KB |    3878 KB |    3929 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      66 KB |      88 KB |     139 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18671 KB |   18707 KB |   18808 KB |  139776 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1981 KB |    2045 KB |    2118 KB |  139776 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:59:13.630290 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:13.849293 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:13.927957 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.027901 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.103135 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.180908 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.254902 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.330349 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.411693 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.516181 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.615870 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.727015 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.830487 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.918260 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.999323 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.098279 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.193979 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.298525 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.396642 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.500079 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.608118 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.717813 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.821774 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.924340 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.023539 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.121204 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.220672 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.322474 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.421422 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.526205 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.632648 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.716818 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.799581 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.872746 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.941519 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.006045 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.077999 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.160201 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.255479 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.352782 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.447117 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.533950 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.604977 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.703750 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.810349 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.912718 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:18.017629 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:18.088861 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:18.158138 [3] Warning: no training nodes in this partition! Backward fake loss.
13:59:18.251202 [3] Warning: no training nodes in this partition! Backward fake loss.
15:17:46.791662 [3] proc begin: <DistEnv 3/4 nccl>
15:17:48.268430 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
15:17:48.269590 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3856 KB |    3878 KB |    3929 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      66 KB |      88 KB |     139 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3856 KB |    3878 KB |    3929 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      66 KB |      88 KB |     139 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18671 KB |   18707 KB |   18808 KB |  139776 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1981 KB |    2045 KB |    2118 KB |  139776 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:18:13.982366 [3] proc begin: <DistEnv 3/4 nccl>
15:18:15.476574 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
15:18:15.477509 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3856 KB |    3878 KB |    3929 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      66 KB |      88 KB |     139 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3856 KB |    3878 KB |    3929 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      66 KB |      88 KB |     139 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18671 KB |   18707 KB |   18808 KB |  139776 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1981 KB |    2045 KB |    2118 KB |  139776 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:18:16.854953 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.001514 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.017658 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.038040 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.055858 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.072955 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.088176 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.103458 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.115773 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.131035 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.143351 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.159517 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.172286 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.183239 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.197987 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.212301 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.224195 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.240996 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.252344 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.266217 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.281884 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.297566 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.312420 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.328773 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.348362 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.364340 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.376267 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.392837 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.406866 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.421530 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.440377 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.457386 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.477042 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.494720 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.513490 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.534436 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.549977 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.562063 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.577478 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.592270 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.605174 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.623236 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.642529 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.665747 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.684744 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.705156 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.725679 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.745125 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.765891 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.784938 [3] Warning: no training nodes in this partition! Backward fake loss.
16:04:46.946924 [3] proc begin: <DistEnv 3/4 nccl>
16:04:51.282146 [3] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 3, |V|: 42336, |E|: 334102>
16:04:51.301178 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  26261 KiB |  27623 KiB |  30346 KiB |   4084 KiB |
|       from large pool |  22491 KiB |  23851 KiB |  26571 KiB |   4080 KiB |
|       from small pool |   3770 KiB |   3772 KiB |   3775 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         |  26261 KiB |  27623 KiB |  30346 KiB |   4084 KiB |
|       from large pool |  22491 KiB |  23851 KiB |  26571 KiB |   4080 KiB |
|       from small pool |   3770 KiB |   3772 KiB |   3775 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |  26258 KiB |  27581 KiB |  30228 KiB |   3969 KiB |
|       from large pool |  22491 KiB |  23814 KiB |  26460 KiB |   3969 KiB |
|       from small pool |   3767 KiB |   3767 KiB |   3768 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  47104 KiB |  47104 KiB |  47104 KiB |      0 B   |
|       from large pool |  43008 KiB |  43008 KiB |  43008 KiB |      0 B   |
|       from small pool |   4096 KiB |   4096 KiB |   4096 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  20842 KiB |  22323 KiB |  28245 KiB |   7403 KiB |
|       from large pool |  20517 KiB |  20517 KiB |  24597 KiB |   4080 KiB |
|       from small pool |    325 KiB |   1882 KiB |   3648 KiB |   3323 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |       2    |       3    |       5    |       3    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |       2    |       3    |       5    |       3    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       2    |       2    |       2    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       7    |       3    |
|       from large pool |       2    |       2    |       5    |       3    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:13:51.599003 [3] proc begin: <DistEnv 3/4 nccl>
16:13:52.216540 [3] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 3, |V|: 42336, |E|: 334102>
16:13:52.282231 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  26261 KiB |  27623 KiB |  30346 KiB |   4084 KiB |
|       from large pool |  22491 KiB |  23851 KiB |  26571 KiB |   4080 KiB |
|       from small pool |   3770 KiB |   3772 KiB |   3775 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         |  26261 KiB |  27623 KiB |  30346 KiB |   4084 KiB |
|       from large pool |  22491 KiB |  23851 KiB |  26571 KiB |   4080 KiB |
|       from small pool |   3770 KiB |   3772 KiB |   3775 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |  26258 KiB |  27581 KiB |  30228 KiB |   3969 KiB |
|       from large pool |  22491 KiB |  23814 KiB |  26460 KiB |   3969 KiB |
|       from small pool |   3767 KiB |   3767 KiB |   3768 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  47104 KiB |  47104 KiB |  47104 KiB |      0 B   |
|       from large pool |  43008 KiB |  43008 KiB |  43008 KiB |      0 B   |
|       from small pool |   4096 KiB |   4096 KiB |   4096 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  20842 KiB |  22323 KiB |  28245 KiB |   7403 KiB |
|       from large pool |  20517 KiB |  20517 KiB |  24597 KiB |   4080 KiB |
|       from small pool |    325 KiB |   1882 KiB |   3648 KiB |   3323 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |       2    |       3    |       5    |       3    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |       2    |       3    |       5    |       3    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       2    |       2    |       2    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       7    |       3    |
|       from large pool |       2    |       2    |       5    |       3    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:34:42.609413 [3] proc begin: <DistEnv 3/4 nccl>
16:34:43.241215 [3] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 3, |V|: 42336, |E|: 334102>
16:34:43.262516 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  26261 KiB |  27623 KiB |  30346 KiB |   4084 KiB |
|       from large pool |  22491 KiB |  23851 KiB |  26571 KiB |   4080 KiB |
|       from small pool |   3770 KiB |   3772 KiB |   3775 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         |  26261 KiB |  27623 KiB |  30346 KiB |   4084 KiB |
|       from large pool |  22491 KiB |  23851 KiB |  26571 KiB |   4080 KiB |
|       from small pool |   3770 KiB |   3772 KiB |   3775 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |  26258 KiB |  27581 KiB |  30228 KiB |   3969 KiB |
|       from large pool |  22491 KiB |  23814 KiB |  26460 KiB |   3969 KiB |
|       from small pool |   3767 KiB |   3767 KiB |   3768 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  47104 KiB |  47104 KiB |  47104 KiB |      0 B   |
|       from large pool |  43008 KiB |  43008 KiB |  43008 KiB |      0 B   |
|       from small pool |   4096 KiB |   4096 KiB |   4096 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  20842 KiB |  22323 KiB |  28245 KiB |   7403 KiB |
|       from large pool |  20517 KiB |  20517 KiB |  24597 KiB |   4080 KiB |
|       from small pool |    325 KiB |   1882 KiB |   3648 KiB |   3323 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |       2    |       3    |       5    |       3    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |       2    |       3    |       5    |       3    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       2    |       2    |       2    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       7    |       3    |
|       from large pool |       2    |       2    |       5    |       3    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:59:21.941107 [3] proc begin: <DistEnv 3/4 nccl>
15:59:25.460753 [3] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 3, |V|: 42336, |E|: 334102>
15:59:25.480852 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  26261 KiB |  27623 KiB |  30346 KiB |   4084 KiB |
|       from large pool |  22491 KiB |  23851 KiB |  26571 KiB |   4080 KiB |
|       from small pool |   3770 KiB |   3772 KiB |   3775 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         |  26261 KiB |  27623 KiB |  30346 KiB |   4084 KiB |
|       from large pool |  22491 KiB |  23851 KiB |  26571 KiB |   4080 KiB |
|       from small pool |   3770 KiB |   3772 KiB |   3775 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |  26258 KiB |  27581 KiB |  30228 KiB |   3969 KiB |
|       from large pool |  22491 KiB |  23814 KiB |  26460 KiB |   3969 KiB |
|       from small pool |   3767 KiB |   3767 KiB |   3768 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  47104 KiB |  47104 KiB |  47104 KiB |      0 B   |
|       from large pool |  43008 KiB |  43008 KiB |  43008 KiB |      0 B   |
|       from small pool |   4096 KiB |   4096 KiB |   4096 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  20842 KiB |  22323 KiB |  28245 KiB |   7403 KiB |
|       from large pool |  20517 KiB |  20517 KiB |  24597 KiB |   4080 KiB |
|       from small pool |    325 KiB |   1882 KiB |   3648 KiB |   3323 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |       2    |       3    |       5    |       3    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |       2    |       3    |       5    |       3    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       2    |       2    |       2    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       7    |       3    |
|       from large pool |       2    |       2    |       5    |       3    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:00:28.142634 [3] proc begin: <DistEnv 3/4 nccl>
16:00:28.252819 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
16:00:28.263498 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:00:29.583163 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.340329 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.348350 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.356504 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.366361 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.373204 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.377610 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.381712 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.388542 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.394545 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.398571 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.403999 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.408004 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.411821 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.416078 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.421458 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.425225 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.428918 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.432588 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.436330 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.440177 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.445060 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.449019 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.452930 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.456802 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.460276 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.464011 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.467835 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.473873 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.479203 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.482880 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.487711 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.491545 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.495233 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.499226 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.503364 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.507678 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.511754 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.517897 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.524985 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.531352 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.536905 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.542775 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.549257 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.554590 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.560791 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.567102 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.573245 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.580216 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.587492 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.594598 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.600022 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.604155 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.608130 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.614168 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.621606 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.627153 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.631073 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.635162 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.640711 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.644569 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.652951 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.658546 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.664297 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.670311 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.676602 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.682168 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.686918 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.691734 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.696537 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.701192 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.708501 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.713593 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.718635 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.723424 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.727524 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.733964 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.738412 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.742351 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.746350 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.750945 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.756376 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.760541 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.766637 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.773813 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.779194 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.784099 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.790390 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.794887 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.799198 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.803388 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.810946 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.818886 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.822546 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.829231 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.834522 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.838669 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.842755 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.846840 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.850580 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.854487 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.861568 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.865477 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.869362 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.873141 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.878260 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.883310 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.887187 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.891330 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.895193 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.900215 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.907908 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.913369 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.917505 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.922228 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.927594 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.935043 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.940131 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.944077 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.948257 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.952721 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.957899 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.961793 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.965817 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.969775 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.973991 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.978200 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.982055 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.985823 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.989734 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.993854 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.999282 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.003290 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.007250 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.011166 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.018332 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.022537 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.027035 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.031323 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.035739 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.039668 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.044340 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.048684 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.052724 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.056531 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.060240 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.064154 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.069527 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.075352 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.079292 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.083334 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.088876 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.094722 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.099084 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.104479 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.109623 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.122846 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.129614 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.134368 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.138752 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.149138 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.159856 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.163965 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.168350 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.174275 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.179662 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.184091 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.188021 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.191734 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.195608 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.199432 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.205474 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.211423 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.217697 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.223906 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.228610 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.232857 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.236943 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.240662 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.244360 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.248265 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.253111 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.257278 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.261182 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.265344 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.269340 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.273396 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.277470 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.281580 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.285567 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.289643 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.294864 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.298806 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.304041 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.310999 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.315819 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.319991 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.324024 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.327692 [3] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.331601 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:23.660581 [3] proc begin: <DistEnv 3/4 nccl>
16:05:23.726352 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
16:05:23.737704 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:05:24.918248 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.608829 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.616396 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.622741 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.628794 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.633040 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.636956 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.642635 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.647035 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.650845 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.654490 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.661035 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.666590 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.671721 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.677418 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.681093 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.684956 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.688948 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.692591 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.696091 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.700522 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.707518 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.712812 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.716662 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.720688 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.726159 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.732517 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.737777 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.743244 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.748335 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.752050 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.757153 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.761064 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.764565 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.768453 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.772772 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.778482 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.783832 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.788235 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.792322 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.796021 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.800709 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.805436 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.810677 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.815294 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.820198 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.824146 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.828203 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.833569 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.839074 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.844146 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.848968 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.854617 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.862925 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.868042 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.872166 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.877848 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.883339 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.888067 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.892195 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.896397 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.901510 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.905343 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.909456 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.913390 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.917336 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.921353 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.925209 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.929538 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.933813 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.937616 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.942320 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.946145 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.950112 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.953946 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.960211 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.966287 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.970994 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.977385 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.983418 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.987666 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.992654 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.996532 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.000412 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.006019 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.012379 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.020008 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.026148 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.031999 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.037262 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.041860 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.046708 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.052589 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.058320 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.062438 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.066503 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.070631 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.074443 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.078407 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.082214 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.087227 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.094173 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.098267 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.102165 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.105768 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.111657 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.115926 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.121010 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.125695 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.129887 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.133743 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.144006 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.148798 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.152712 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.156820 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.160798 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.164291 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.168229 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.174133 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.180176 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.186199 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.192535 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.196247 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.200135 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.203965 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.208039 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.211927 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.215714 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.219620 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.223431 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.227427 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.232205 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.236038 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.239791 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.243584 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.247494 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.251342 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.255346 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.259363 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.263217 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.266865 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.271705 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.275761 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.279620 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.283569 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.287207 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.290784 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.294743 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.298932 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.303263 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.306891 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.311316 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.315376 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.319267 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.322892 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.327297 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.332491 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.336723 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.340282 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.343996 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.348026 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.352804 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.356333 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.360077 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.365049 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.368777 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.374625 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.380502 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.386382 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.390635 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.394342 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.399101 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.402983 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.406717 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.412762 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.416654 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.420540 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.424293 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.428298 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.432154 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.437518 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.445491 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.450258 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.454159 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.458063 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.461830 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.465379 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.469012 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.472693 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.477844 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.483631 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.490445 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.494987 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.499070 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.503195 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.508942 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.514615 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.521036 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.526492 [3] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.530989 [3] Warning: no training nodes in this partition! Backward fake loss.
16:11:46.312368 [3] proc begin: <DistEnv 3/4 nccl>
16:12:00.507840 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:12:00.525672 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:46:58.059031 [3] proc begin: <DistEnv 3/4 nccl>
16:46:58.085904 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
16:46:58.095419 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:46:59.305287 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.949701 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.958138 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.963567 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.967569 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.973429 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.978354 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.982321 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.986184 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.990273 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.994549 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.999963 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.004449 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.009163 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.013446 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.017823 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.023782 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.030267 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.036166 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.040614 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.044610 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.049889 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.054038 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.059583 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.065353 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.069686 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.073585 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.077385 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.081390 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.085244 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.089147 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.093768 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.097889 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.101714 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.107554 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.111540 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.115456 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.120747 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.126221 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.131296 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.138010 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.143442 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.147455 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.151079 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.155390 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.159491 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.164625 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.169488 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.175678 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.181221 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.187019 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.192367 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.196511 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.200628 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.204754 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.208524 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.212291 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.216525 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.220568 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.225746 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.231969 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.239055 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.244399 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.250360 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.255810 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.259677 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.265064 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.270360 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.275639 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.279620 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.283533 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.290026 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.295792 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.302577 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.307241 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.311129 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.315135 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.319200 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.323327 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.329246 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.333700 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.338860 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.342745 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.349108 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.353855 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.357861 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.363634 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.367374 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.373001 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.378065 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.382211 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.388582 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.392614 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.396502 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.402051 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.406247 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.410184 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.414062 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.417660 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.421433 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.428011 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.435981 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.441222 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.446655 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.450809 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.454716 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.458453 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.463861 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.470086 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.474700 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.478805 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.483959 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.487697 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.491368 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.494939 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.498930 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.503119 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.507028 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.510896 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.514472 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.518635 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.523639 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.527558 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.531365 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.535115 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.539034 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.542763 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.546581 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.550227 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.553940 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.557816 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.563034 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.567275 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.571080 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.575116 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.579184 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.585429 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.591315 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.595195 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.599178 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.602920 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.607804 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.614162 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.619646 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.623937 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.628080 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.631715 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.635539 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.639548 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.643271 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.646900 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.651532 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.655272 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.658701 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.662231 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.665976 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.669742 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.673915 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.677639 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.681236 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.684847 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.689597 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.696054 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.700319 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.704233 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.708084 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.711592 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.715312 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.719135 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.722968 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.726790 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.731543 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.735279 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.738893 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.742642 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.746125 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.749882 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.753832 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.759336 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.764337 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.768455 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.774035 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.778315 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.782568 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.788731 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.793176 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.797502 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.801551 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.805846 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.810220 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.814183 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.819298 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.823062 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.826794 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.830756 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.834496 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.838393 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.842370 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.846492 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.850337 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:46.076401 [3] proc begin: <DistEnv 3/4 nccl>
16:47:46.126834 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
16:47:46.136325 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:47:48.096221 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.734069 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.741911 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.748303 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.753077 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.758037 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.762507 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.766760 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.775017 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.779132 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.784308 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.792166 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.797956 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.802127 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.806084 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.810280 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.814274 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.818258 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.822270 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.826216 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.830480 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.835807 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.839932 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.844677 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.848714 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.854143 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.862815 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.866809 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.870943 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.876991 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.882880 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.892937 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.899207 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.905188 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.910179 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.913951 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.917937 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.921753 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.925430 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.929232 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.933304 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.939773 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.945534 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.950257 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.954632 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.959264 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.965415 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.969343 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.976244 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.982542 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.986712 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.992031 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.996227 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.002234 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.006166 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.009929 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.014846 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.020467 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.026773 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.032069 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.036376 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.042104 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.045854 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.049514 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.053414 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.057681 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.062625 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.066487 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.070506 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.084212 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.090755 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.099486 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.103767 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.109353 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.114821 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.118839 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.123175 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.127529 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.131669 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.135768 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.140946 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.148680 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.153327 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.157530 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.161746 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.165581 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.171572 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.177120 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.180955 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.184555 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.189578 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.198177 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.203416 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.208609 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.213137 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.218123 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.222342 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.226445 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.230220 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.234311 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.238458 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.245565 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.251092 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.255138 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.258910 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.262902 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.266904 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.270806 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.274810 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.278595 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.282301 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.289047 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.293448 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.297293 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.301195 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.305072 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.308857 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.312819 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.317690 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.322246 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.326390 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.331845 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.337183 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.342788 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.349291 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.356098 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.364676 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.381332 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.386125 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.390683 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.395911 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.402478 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.408636 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.413337 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.418161 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.428794 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.437934 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.443745 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.455141 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.462462 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.467155 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.476305 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.482543 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.487217 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.496701 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.536206 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.555918 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.563296 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.568519 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.576925 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.581596 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.588225 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.593149 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.597842 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.602444 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.607609 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.613027 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.617839 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.622546 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.627970 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.632423 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.638433 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.643217 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.649337 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.654017 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.658510 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.662935 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.667541 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.672024 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.676170 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.679968 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.685927 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.690687 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.695290 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.699793 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.703527 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.707725 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.711528 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.715456 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.719216 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.723605 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.729309 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.733701 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.737889 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.741853 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.745893 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.751718 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.756358 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.760806 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.765172 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.773532 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.779499 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.784298 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.789141 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.792861 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.796650 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.800726 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.808816 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.815303 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.821745 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:25.075369 [3] proc begin: <DistEnv 3/4 nccl>
16:49:25.147925 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
16:49:25.158026 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:49:26.305426 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.164715 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.172727 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.180301 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.184671 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.188677 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.192830 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.196778 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.200506 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.204359 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.208245 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.213266 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.217454 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.221893 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.227536 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.234127 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.238964 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.243345 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.246944 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.252085 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.257294 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.262348 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.266001 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.269636 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.274138 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.280660 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.286329 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.290513 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.294751 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.298447 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.302485 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.309618 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.316447 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.321705 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.328722 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.333896 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.339227 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.345103 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.350718 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.354808 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.360506 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.366125 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.370368 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.376263 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.382037 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.386214 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.390652 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.396667 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.400703 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.404563 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.408429 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.413829 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.418055 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.423688 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.428130 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.432666 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.437452 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.441860 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.446498 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.452410 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.457218 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.462465 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.466507 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.470898 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.477360 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.481734 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.485325 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.489654 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.493828 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.497870 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.501726 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.508295 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.513639 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.517065 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.520600 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.525812 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.529983 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.534064 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.538005 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.541719 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.545382 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.552881 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.558453 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.564020 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.568635 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.573236 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.577132 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.583184 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.589509 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.593857 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.598342 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.603700 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.607956 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.611926 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.615823 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.621063 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.626113 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.630065 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.634299 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.639917 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.645271 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.650038 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.657686 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.663881 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.668194 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.673324 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.678016 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.682136 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.687949 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.692113 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.696535 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.702797 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.708802 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.714624 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.720247 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.725056 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.729634 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.734552 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.739376 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.744268 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.749182 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.755373 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.760251 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.765134 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.769480 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.775733 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.782325 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.786621 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.790433 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.794122 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.798922 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.805588 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.809678 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.815182 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.822229 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.827720 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.832455 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.836732 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.842326 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.847978 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.852225 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.857425 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.861402 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.865304 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.869377 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.873315 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.876989 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.880595 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.884442 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.888513 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.892261 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.897288 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.901616 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.905588 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.909333 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.912990 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.917438 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.923762 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.934608 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.940852 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.945941 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.953176 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.957444 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.961917 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.966437 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.970398 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.975315 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.979390 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.983361 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.987534 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.991415 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.996312 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.001045 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.005179 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.009076 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.013002 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.016989 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.020760 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.025236 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.029234 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.033138 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.041077 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.046171 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.050145 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.053793 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.058229 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.064464 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.070806 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.076235 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.080201 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.084062 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.089228 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.093157 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.096827 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.107362 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.122591 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.128495 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.132909 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.137147 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.141091 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:30.360041 [3] proc begin: <DistEnv 3/4 nccl>
16:52:30.458031 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
16:52:30.470744 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:52:31.722950 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.435779 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.443434 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.449672 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.453919 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.459304 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.463875 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.468619 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.473070 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.478704 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.483588 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.489167 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.493222 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.497436 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.501634 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.505698 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.509630 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.513630 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.517383 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.522619 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.527679 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.533076 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.536987 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.541133 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.544903 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.548600 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.552118 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.555695 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.559532 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.563657 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.567413 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.572735 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.576607 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.580728 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.584887 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.588756 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.593105 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.597622 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.601492 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.605384 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.609437 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.614619 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.620195 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.624493 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.628356 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.634024 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.640339 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.645163 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.649435 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.653083 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.657363 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.662934 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.667209 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.670879 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.674629 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.678488 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.682419 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.686328 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.690428 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.694282 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.699843 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.706885 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.711566 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.715744 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.719877 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.725904 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.729912 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.733671 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.737255 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.740871 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.744722 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.750010 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.754063 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.761605 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.767102 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.771256 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.775706 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.779429 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.783351 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.787461 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.793229 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.800129 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.805114 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.809744 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.813648 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.817408 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.821354 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.825141 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.829976 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.833828 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.837748 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.843363 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.847552 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.851633 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.855699 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.859861 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.864053 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.868249 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.872471 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.876705 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.881074 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.886726 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.890918 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.894925 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.899230 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.903432 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.907682 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.911571 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.915290 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.918932 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.922929 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.928479 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.932629 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.936443 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.940214 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.944385 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.948228 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.952104 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.955809 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.959456 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.963032 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.968536 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.973392 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.977298 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.981265 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.986016 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.991976 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.997312 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.001525 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.005492 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.009271 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.014640 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.018694 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.022424 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.027977 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.031874 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.035831 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.039665 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.043612 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.047462 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.051160 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.056367 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.062336 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.067669 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.071778 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.075875 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.080050 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.083959 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.087853 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.091708 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.097304 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.102854 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.107037 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.110996 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.114893 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.118960 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.123610 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.130069 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.136302 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.142062 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.147098 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.151948 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.158223 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.163321 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.167615 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.172227 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.176195 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.180240 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.184271 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.189471 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.194805 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.200616 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.204535 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.208564 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.212931 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.217103 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.221104 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.226634 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.231918 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.236158 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.241445 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.246606 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.250643 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.254883 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.265153 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.269972 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.274205 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.281961 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.287502 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.292077 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.297785 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.303801 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.307525 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.311733 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.315842 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.319845 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.323804 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.327451 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.331141 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.334940 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:43.061035 [3] proc begin: <DistEnv 3/4 nccl>
16:52:43.090535 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
16:52:43.099902 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:52:44.403987 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.184068 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.190792 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.195967 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.200932 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.207543 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.211634 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.215715 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.219935 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.223743 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.227646 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.232918 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.239318 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.247432 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.253997 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.258482 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.263565 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.268622 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.273822 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.283670 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.290644 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.299157 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.304076 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.309204 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.314361 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.319080 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.323886 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.328918 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.334138 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.340106 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.347581 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.357506 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.363547 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.369968 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.376226 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.380772 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.385142 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.389099 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.393189 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.397225 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.401207 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.406738 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.411123 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.415177 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.419071 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.423082 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.426934 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.430906 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.434859 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.439095 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.443879 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.449319 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.455139 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.459293 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.463091 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.468428 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.475602 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.481269 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.485297 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.489245 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.493665 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.499347 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.503176 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.507127 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.513099 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.519877 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.525841 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.530800 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.536036 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.541064 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.546166 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.552643 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.557397 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.562275 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.567023 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.573248 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.579862 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.585237 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.590184 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.595099 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.601591 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.612192 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.617240 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.621975 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.626544 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.631905 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.635659 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.639818 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.644171 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.648455 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.652437 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.657435 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.661542 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.665367 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.669245 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.673188 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.677326 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.681244 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.685030 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.688871 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.692637 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.698804 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.703052 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.707145 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.711315 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.719558 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.724787 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.730943 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.735492 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.741138 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.746365 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.754458 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.760137 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.763949 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.767853 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.772396 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.776805 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.780919 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.785100 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.789017 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.792719 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.798267 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.802743 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.814450 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.820097 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.825018 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.829430 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.834161 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.839632 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.847249 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.852277 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.857788 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.861855 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.865812 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.869790 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.873580 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.879787 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.885922 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.890802 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.895217 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.900815 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.908942 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.918986 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.923420 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.928887 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.934966 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.939837 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.944298 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.948330 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.953875 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.957951 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.964479 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.968355 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.972715 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.977249 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.981649 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.985888 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.989902 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.994306 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.998338 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.002241 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.007674 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.012356 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.016810 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.020806 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.024532 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.028817 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.032747 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.036645 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.040830 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.044732 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.049992 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.054124 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.059561 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.065678 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.072017 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.075939 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.079797 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.084274 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.088642 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.093224 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.101556 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.115783 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.121620 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.126908 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.131825 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.135959 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.140037 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.143938 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.148078 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.152096 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.165661 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.173838 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.180014 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.190072 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.195227 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.199949 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.205750 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.211599 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.215548 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:29.497162 [3] proc begin: <DistEnv 3/4 nccl>
16:53:29.550475 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
16:53:29.560107 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:53:30.841140 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.545684 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.553861 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.560086 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.565860 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.572108 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.577949 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.584012 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.590642 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.597052 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.603150 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.610074 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.616321 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.622278 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.634863 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.642946 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.650321 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.655854 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.661528 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.668425 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.674113 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.683285 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.690408 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.696188 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.704314 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.710819 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.716644 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.723642 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.729358 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.736046 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.742573 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.748915 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.754741 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.760193 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.765458 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.771058 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.776119 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.781154 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.786210 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.792904 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.798237 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.803858 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.809167 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.818007 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.823318 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.828932 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.834937 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.842069 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.847774 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.853162 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.858683 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.864037 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.869435 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.874735 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.879842 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.884815 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.890613 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.896133 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.902128 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.907937 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.913422 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.918812 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.924337 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.930319 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.937360 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.942897 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.948225 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.954992 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.960530 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.965900 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.972256 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.979870 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.985801 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.991457 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.998446 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.004119 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.009514 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.015228 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.020571 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.026008 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.031427 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.036841 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.042515 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.048053 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.054845 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.061563 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.067015 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.072759 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.078550 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.083756 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.088860 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.094008 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.104700 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.110987 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.116392 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.121905 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.127059 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.134156 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.141217 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.146576 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.151955 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.157375 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.162634 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.167967 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.177038 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.183456 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.190225 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.195654 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.204247 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.212795 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.221438 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.227515 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.233393 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.239236 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.245139 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.251227 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.260694 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.266118 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.272053 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.280057 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.286686 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.293310 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.299543 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.308302 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.313890 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.319639 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.325120 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.330419 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.336499 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.343791 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.349591 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.355612 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.361278 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.366856 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.374473 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.381176 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.386779 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.395791 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.403927 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.412323 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.420076 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.425598 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.431017 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.436825 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.442521 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.449431 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.455886 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.461885 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.467239 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.473118 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.478912 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.484610 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.490620 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.496135 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.502833 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.508666 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.519001 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.525971 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.532586 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.537462 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.542825 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.548159 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.553157 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.558170 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.563186 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.568291 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.580626 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.589057 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.597010 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.603732 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.630693 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.638393 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.644925 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.649908 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.655560 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.660925 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.665935 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.670999 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.676090 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.681049 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.686084 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.691162 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.697174 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.706771 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.712956 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.718990 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.724900 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.730699 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.737198 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.743833 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.749610 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.755722 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.761055 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.766592 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.775374 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.781713 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.787164 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.792826 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.798037 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.807209 [3] Warning: no training nodes in this partition! Backward fake loss.
16:55:22.556224 [3] proc begin: <DistEnv 3/4 nccl>
16:55:29.576550 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:55:29.593189 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:17:57.811970 [3] proc begin: <DistEnv 3/4 nccl>
19:17:57.944180 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
19:17:57.964963 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:17:59.370834 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.193730 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.202817 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.210173 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.215922 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.222355 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.228578 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.234015 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.239800 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.245750 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.251217 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.256785 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.262553 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.268164 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.273374 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.278916 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.287493 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.294225 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.301369 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.306961 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.312631 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.318311 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.325066 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.331721 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.338022 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.343604 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.349557 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.356483 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.363032 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.368678 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.376839 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.383013 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.389359 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.396049 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.404898 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.412287 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.420435 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.427377 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.434566 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.441847 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.448228 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.455498 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.461503 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.469537 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.475632 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.481680 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.487576 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.494587 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.502301 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.509459 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.518014 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.525423 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.531437 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.536960 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.542971 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.551230 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.556878 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.562194 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.567895 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.574187 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.579650 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.585195 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.593766 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.598880 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.604256 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.612712 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.619603 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.629053 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.638473 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.646091 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.654284 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.662161 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.672478 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.684766 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.699255 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.714621 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.727782 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.736029 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.743442 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.750427 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.756406 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.767044 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.777920 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.785051 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.794704 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.801653 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.813585 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.820068 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.826319 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.835602 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.844852 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.852511 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.859621 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.866492 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.873583 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.880461 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.890907 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.898320 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.906603 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.914530 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.922650 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.928914 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.935060 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.942149 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.948815 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.955364 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.961824 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.967899 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.973615 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.980349 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.985938 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.993081 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.999284 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.005368 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.011304 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.019040 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.025232 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.030923 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.037029 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.051530 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.058487 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.065072 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.071984 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.077852 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.084455 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.090316 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.097849 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.105319 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.113701 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.119891 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.125723 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.131390 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.137270 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.143610 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.149382 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.155510 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.163175 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.168659 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.174947 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.181157 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.186500 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.194167 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.201391 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.207101 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.212864 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.218429 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.227179 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.233618 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.241643 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.248785 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.255887 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.261614 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.267117 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.272631 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.278118 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.287407 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.293367 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.299562 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.306098 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.311674 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.317161 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.322618 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.328024 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.333228 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.338640 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.343781 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.349285 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.354595 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.364935 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.371844 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.377757 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.383425 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.388924 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.394489 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.400455 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.406639 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.412035 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.420889 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.428637 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.435288 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.441835 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.447679 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.453516 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.459054 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.464849 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.471073 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.477014 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.483173 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.489084 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.496067 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.503251 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.509333 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.514787 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.520796 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.526988 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.533775 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.539174 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.545393 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.555181 [3] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.566080 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:17.801377 [3] proc begin: <DistEnv 3/4 nccl>
19:23:17.854386 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
19:23:17.864382 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:23:19.176553 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.881885 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.890857 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.896453 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.903854 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.909844 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.918554 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.925785 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.932107 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.937074 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.948163 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.953757 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.959384 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.964568 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.970210 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.976250 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.981383 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.989948 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.996489 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.004084 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.009694 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.016856 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.023619 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.030642 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.037372 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.043039 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.050539 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.057957 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.064882 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.070821 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.079743 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.087071 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.093043 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.099146 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.104802 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.110258 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.115816 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.122817 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.128016 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.133172 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.138502 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.143877 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.149026 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.154250 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.159498 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.167806 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.175954 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.182591 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.188426 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.193731 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.199475 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.205828 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.211507 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.217075 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.223088 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.230063 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.235351 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.243462 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.248761 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.253998 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.259773 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.265263 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.270691 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.276044 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.281434 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.286843 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.292483 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.301400 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.308586 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.314305 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.320141 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.325783 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.331306 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.336755 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.341816 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.350525 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.358896 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.366169 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.372086 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.377677 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.384121 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.390064 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.396937 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.403780 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.409357 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.414775 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.421527 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.430871 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.436580 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.442218 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.447953 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.453858 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.459233 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.464899 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.470745 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.476495 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.482081 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.487753 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.492902 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.498137 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.504374 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.509873 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.515283 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.522323 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.529051 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.534630 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.540740 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.548603 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.554154 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.559332 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.564589 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.573827 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.580512 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.587457 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.595302 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.602705 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.609651 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.616317 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.621733 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.629933 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.639849 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.646831 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.653245 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.659205 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.664929 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.670316 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.677608 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.684245 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.690927 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.696626 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.702494 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.707970 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.715669 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.721344 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.726895 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.736050 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.742054 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.747652 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.753121 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.758597 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.768511 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.774843 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.780320 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.785812 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.791163 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.798831 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.808441 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.821330 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.831204 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.836711 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.842032 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.847484 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.853060 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.858866 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.864001 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.869341 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.875874 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.882798 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.887853 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.893288 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.898459 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.904307 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.909778 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.915073 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.920387 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.926467 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.932364 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.937315 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.945818 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.952483 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.958122 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.968366 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.977852 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.986461 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.993297 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.999332 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.011737 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.025857 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.034555 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.043939 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.058452 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.065633 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.071901 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.077289 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.082805 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.089672 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.096614 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.105245 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.111909 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.117463 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.122764 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.128118 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.133707 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.140787 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.150262 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.155601 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.160915 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.166180 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.174580 [3] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.180257 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:45.858931 [3] proc begin: <DistEnv 3/4 nccl>
20:00:45.981681 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
20:00:45.991307 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:00:47.251613 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.952371 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.961315 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.971973 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.977801 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.983885 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.989425 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.995267 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.001027 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.006411 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.012161 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.018053 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.023337 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.028862 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.034711 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.040481 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.046085 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.051606 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.057062 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.062461 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.067878 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.073446 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.082869 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.089599 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.096705 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.103489 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.112083 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.117309 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.122844 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.128575 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.133981 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.139841 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.145590 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.150818 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.157464 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.163178 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.168693 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.174249 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.180523 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.186637 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.192264 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.198570 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.204858 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.210705 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.217301 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.223422 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.229583 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.235264 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.241903 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.248110 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.253601 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.259410 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.266483 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.273429 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.280226 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.285480 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.291169 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.296684 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.302061 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.307627 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.312909 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.318019 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.323129 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.328243 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.333305 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.338747 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.345274 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.351672 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.357095 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.362267 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.367743 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.377540 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.386281 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.392495 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.398703 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.404403 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.413549 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.421903 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.428049 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.433853 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.439452 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.444913 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.450542 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.456014 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.463797 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.471708 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.477911 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.483042 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.488625 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.495263 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.502028 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.507978 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.513817 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.519252 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.524977 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.530636 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.535943 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.541950 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.552098 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.558367 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.565066 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.570840 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.576779 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.582247 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.590185 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.597357 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.604199 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.611214 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.617008 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.622970 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.630507 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.637662 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.642908 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.649200 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.655779 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.661911 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.669093 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.676439 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.682757 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.689209 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.696486 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.703183 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.709128 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.714897 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.720911 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.729712 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.735462 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.741396 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.747995 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.753846 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.759880 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.765309 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.770927 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.778990 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.784683 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.789988 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.795894 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.804785 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.810273 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.820040 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.827236 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.833154 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.838923 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.844556 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.850355 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.858866 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.865186 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.871072 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.876266 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.881711 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.887019 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.895043 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.908577 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.916266 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.922976 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.928708 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.934335 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.943123 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.949690 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.957860 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.963708 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.970574 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.976327 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.981540 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.988285 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.995270 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.006243 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.011841 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.017450 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.026318 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.033107 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.040027 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.048720 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.056696 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.062914 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.068377 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.075321 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.080833 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.086552 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.092198 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.097725 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.103352 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.108805 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.114551 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.119784 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.126013 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.132470 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.144042 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.150443 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.157602 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.163541 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.171273 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.178789 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.185323 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.191530 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.198711 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.205284 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.211937 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.218791 [3] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.225482 [3] Warning: no training nodes in this partition! Backward fake loss.
20:48:25.826378 [3] proc begin: <DistEnv 3/4 nccl>
20:48:37.339758 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:48:37.418204 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:26:54.580420 [3] proc begin: <DistEnv 3/4 nccl>
20:27:05.948634 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:27:06.025509 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:01:27.862136 [3] proc begin: <DistEnv 3/4 nccl>
17:01:28.069336 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
17:01:28.086253 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:01:29.988404 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.243328 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.326030 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.415190 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.578440 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.713712 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.891630 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.941452 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.054376 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.226219 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.355822 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.490848 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.562736 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.743240 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.852514 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.028881 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.087438 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.224234 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.381434 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.492329 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.633322 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.705196 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.882443 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.949774 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.109894 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.169500 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.244473 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.318853 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.375082 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.382592 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.389101 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.397272 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.405578 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.418476 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.428393 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.435387 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.442636 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.449088 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.459886 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.486899 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.501640 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.509257 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.517240 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.525117 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.533130 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.545846 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.558493 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.572582 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.585033 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.593806 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.603677 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.618223 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.637032 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.675033 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.886601 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:35.103066 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:35.258667 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:35.484188 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:35.639069 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:35.861512 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:35.986819 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:36.217856 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:36.422281 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:36.660489 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:36.813549 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:37.096850 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:37.298297 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:37.488439 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:37.650039 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:37.885590 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.051049 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.223442 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.433898 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.608811 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.754141 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.949777 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:39.141660 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:39.423861 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:39.623843 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:39.849059 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:39.984867 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:40.237229 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:40.420101 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:40.675145 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:40.854673 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.070131 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.213849 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.381788 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.511476 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.744730 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.911920 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.085555 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.231134 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.407555 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.567731 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.729552 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.915724 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.124780 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.237500 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.421133 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.569245 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.742440 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.900459 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.035171 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.182362 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.360770 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.546616 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.593254 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.733519 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.905707 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.019035 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.152633 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.226137 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.424222 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.577692 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.727714 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.802519 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.979913 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.092565 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.263291 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.304386 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.488226 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.620759 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.817387 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.857818 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.028993 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.170938 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.258494 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.388387 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.447847 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.533961 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.622573 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.692028 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.865009 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.934336 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.119772 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.193578 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.344010 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.509778 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.627381 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.768153 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.830498 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.023040 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.154869 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.308500 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.358625 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.435302 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.512704 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.598005 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.683387 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.776988 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.866129 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.038979 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.180986 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.365383 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.416261 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.582628 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.756528 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.927725 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.005317 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.108100 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.260628 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.396284 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.548229 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.586514 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.777482 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.900932 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.911752 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.990544 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.072487 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.089910 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.098024 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.107522 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.116729 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.132667 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.150449 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.165386 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.199367 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.329263 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.343677 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.448581 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.601595 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.795405 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.940945 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.144659 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.269814 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.494149 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.610626 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.761379 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.797735 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.972613 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.131182 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.251979 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.371174 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.476484 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.649464 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.796536 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.940846 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:55.010792 [3] Warning: no training nodes in this partition! Backward fake loss.
17:01:55.149967 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:41.026758 [3] proc begin: <DistEnv 3/4 nccl>
17:16:41.125825 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
17:16:41.141823 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:16:43.534530 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.638191 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.740638 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.758102 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.773233 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.797682 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.814768 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.828671 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.840875 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.853494 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.863504 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.912519 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:45.183038 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:45.379862 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:45.630678 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:45.887498 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.079746 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.326779 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.562012 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.777531 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.999852 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:47.167600 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:47.387670 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:47.494335 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:47.736263 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:47.931026 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.131766 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.364629 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.501820 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.775912 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.954180 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:49.185453 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:49.302950 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:49.534590 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:49.667837 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:49.923662 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.095194 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.343004 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.543502 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.607748 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.624753 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.648954 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.686144 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.870673 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:51.141975 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:51.418860 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:51.693065 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:51.913825 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:52.204947 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:52.509116 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:52.758406 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:53.109197 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:53.403033 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:53.630077 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:53.957782 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:54.151337 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:54.452488 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:54.715933 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:54.977970 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:55.253944 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:55.580528 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:55.835264 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:56.096594 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:56.387330 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:56.576442 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:56.767487 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:56.927011 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.101190 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.232226 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.451585 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.676996 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.840005 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:58.013234 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:58.242658 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:58.388013 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:58.634206 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:58.851045 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.001110 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.230972 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.434204 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.582853 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.790952 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.975809 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:00.127745 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:00.339606 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:00.549808 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:00.677943 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:00.929030 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.154278 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.276367 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.459763 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.649473 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.822357 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.015272 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.243174 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.443517 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.537744 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.633672 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.821760 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.104526 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.258419 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.345440 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.364186 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.386196 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.413448 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.436587 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.460798 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.615096 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.656275 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.778533 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.815791 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.836552 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.859589 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.875261 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.891195 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.914144 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.949223 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:04.149222 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:04.423820 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:04.624743 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:04.860286 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.087262 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.291036 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.384366 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.613768 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.796849 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.972488 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.116030 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.374618 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.542546 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.740063 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.960762 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.177559 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.196975 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.214191 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.242773 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.266264 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.397803 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.658231 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.863255 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.106413 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.318979 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.532273 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.790213 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.996173 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.095369 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.221403 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.298852 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.376090 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.482704 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.689338 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.914582 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.123085 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.342377 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.554979 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.747697 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.896495 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:11.135204 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:11.325271 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:11.499866 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:11.665802 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:11.929173 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.011123 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.151250 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.386487 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.591200 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.650479 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.680621 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.717754 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.846688 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.878879 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.904477 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:13.119213 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:13.405205 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:13.676113 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:13.851979 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:14.171943 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:14.506490 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:14.772774 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:15.086462 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:15.383922 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:15.615744 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:15.859280 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:16.137902 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:16.428300 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:16.628303 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:16.879768 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:17.153381 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:17.413144 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:17.699508 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:17.943841 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:18.165003 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:18.356678 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:18.582125 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:18.809713 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.001221 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.194347 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.428225 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.559348 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.580758 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:46.967822 [3] proc begin: <DistEnv 3/4 nccl>
22:21:47.480535 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
22:21:47.504223 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:21:49.159784 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.049178 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.063323 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.072629 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.082390 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.091888 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.104660 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.113928 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.123275 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.134333 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.143859 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.162768 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.173787 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.185493 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.195182 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.205214 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.217839 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.232880 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.244617 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.253864 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.264107 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.274447 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.285480 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.298959 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.313511 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.324740 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.334887 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.344200 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.353786 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.363024 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.372545 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.382021 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.391230 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.400839 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.413354 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.432659 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.446454 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.455573 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.469525 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.482758 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.492281 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.502120 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.511699 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.520846 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.530199 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.539676 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.549570 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.566527 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.577708 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.588456 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.601871 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.612803 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.624718 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.636302 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.648723 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.658812 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.669226 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.680059 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.690202 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.700763 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.711153 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.722353 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.732927 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.743295 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.754185 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.764547 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.774928 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.785283 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.795765 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.806272 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.816512 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.828732 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.844302 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.854295 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.866838 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.877801 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.888831 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.899921 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.911207 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.921815 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.934199 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.945281 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.956208 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.972221 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.988990 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.009491 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.022844 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.032450 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.041741 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.059227 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.070543 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.079804 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.089405 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.098857 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.108339 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.120547 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.130994 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.143704 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.164117 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.178872 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.187737 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.197851 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.207380 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.216868 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.228383 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.237316 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.246640 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.258237 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.267309 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.276705 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.286306 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.296196 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.306746 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.316068 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.325790 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.334655 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.345064 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.357742 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.368568 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.377995 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.387292 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.397279 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.409040 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.418090 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.427383 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.436436 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.446303 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.455694 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.465212 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.474457 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.487205 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.498387 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.509320 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.519661 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.528674 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.537975 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.546861 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.558608 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.570146 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.579169 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.588456 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.598103 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.607339 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.616484 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.627951 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.637863 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.647174 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.656705 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.665910 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.675428 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.684779 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.694118 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.703825 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.712918 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.722152 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.731417 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.740605 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.750159 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.759371 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.768697 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.777880 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.787241 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.796510 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.805871 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.815311 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.824368 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.833718 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.843050 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.852143 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.861424 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.870658 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.880051 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.889800 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.899416 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.908499 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.917747 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.927069 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.936356 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.945781 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.955524 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.965150 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.995404 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.013479 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.027317 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.036888 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.048029 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.060302 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.070324 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.080432 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.090862 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.100855 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.111843 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.122292 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.132218 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.144226 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.154606 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.164416 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.175011 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.184846 [3] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.194803 [3] Warning: no training nodes in this partition! Backward fake loss.
22:22:57.592671 [3] proc begin: <DistEnv 3/4 nccl>
22:22:57.693334 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
22:22:57.706938 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:22:59.072609 [3] Warning: no training nodes in this partition! Backward fake loss.
22:22:59.966779 [3] Warning: no training nodes in this partition! Backward fake loss.
22:22:59.981853 [3] Warning: no training nodes in this partition! Backward fake loss.
22:22:59.996749 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.007840 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.017890 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.027542 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.036815 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.050591 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.061641 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.070812 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.080158 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.089720 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.101992 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.111465 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.120670 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.130069 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.139216 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.148787 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.158521 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.168202 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.179998 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.191072 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.200447 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.209867 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.219291 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.229690 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.239149 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.248371 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.257819 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.267239 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.276651 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.289464 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.300972 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.311051 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.320364 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.329909 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.339247 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.348425 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.357765 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.367708 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.377199 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.388968 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.402311 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.416635 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.428157 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.442205 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.453793 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.466887 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.477647 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.490588 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.503183 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.513057 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.526384 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.539964 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.550836 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.568085 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.580818 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.596835 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.609344 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.621355 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.637819 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.651404 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.660035 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.674430 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.684280 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.693043 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.702047 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.711949 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.720988 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.731029 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.739860 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.748905 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.759082 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.768485 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.777783 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.786706 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.796348 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.808376 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.818307 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.827356 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.836337 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.845431 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.854356 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.863251 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.872121 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.883839 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.894542 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.903207 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.918273 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.929222 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.938885 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.947874 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.956988 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.966128 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.976251 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.988885 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.998465 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.007381 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.016420 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.025490 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.034556 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.043905 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.052984 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.061817 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.070934 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.080056 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.089071 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.098140 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.108731 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.117894 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.126823 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.135790 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.145043 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.154073 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.164408 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.174005 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.183471 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.192340 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.201837 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.210660 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.219591 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.228781 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.239290 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.250405 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.263106 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.272365 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.282112 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.291311 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.300913 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.312050 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.321266 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.330887 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.340124 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.349675 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.358971 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.368742 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.377938 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.388249 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.399800 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.410752 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.422058 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.437377 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.446886 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.456297 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.465811 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.475116 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.484594 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.496706 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.505826 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.515359 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.524765 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.534336 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.543417 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.552891 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.574160 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.586000 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.602829 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.614640 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.624169 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.633897 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.649231 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.660864 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.670122 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.679893 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.700968 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.713247 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.725413 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.734463 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.743700 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.753246 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.763097 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.772501 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.781748 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.794186 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.803377 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.815370 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.824501 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.833936 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.845684 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.854556 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.863674 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.872980 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.882096 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.891119 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.900332 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.909596 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.918726 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.931133 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.942920 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.952088 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.961678 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.974973 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.985819 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.996782 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:02.007652 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:02.017493 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:02.027526 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:02.036659 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:02.046388 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:49.931954 [3] proc begin: <DistEnv 3/4 nccl>
22:23:49.987747 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
22:23:49.999515 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:23:51.513993 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.223913 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.237518 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.247266 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.256643 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.269801 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.282718 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.291963 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.301395 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.310883 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.322462 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.331480 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.340464 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.349516 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.358706 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.373930 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.384277 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.392978 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.401972 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.412711 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.421851 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.432418 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.443356 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.452635 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.461734 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.470846 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.479733 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.488887 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.500476 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.509301 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.526038 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.536216 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.545332 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.555589 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.566556 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.575847 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.585026 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.593983 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.602777 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.611737 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.620947 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.633294 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.643952 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.654007 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.664317 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.673529 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.682684 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.691591 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.704179 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.715144 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.728646 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.737793 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.747386 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.756719 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.766976 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.776443 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.790895 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.802306 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.811027 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.820403 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.829758 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.838693 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.847857 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.857115 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.871075 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.885384 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.895300 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.904445 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.915048 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.924269 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.938900 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.948347 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.957741 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.972252 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.995513 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.007340 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.017603 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.029081 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.039080 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.048172 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.057421 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.066692 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.075953 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.085061 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.094572 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.104224 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.115911 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.127244 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.141826 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.156043 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.168644 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.179756 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.192186 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.202511 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.211932 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.221037 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.230134 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.239217 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.250732 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.260267 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.269720 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.278743 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.287903 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.297133 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.306216 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.315212 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.324322 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.333639 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.345857 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.355331 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.368862 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.380349 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.389808 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.399000 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.411779 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.420494 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.429700 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.444144 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.453218 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.463083 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.478483 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.489338 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.501757 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.516551 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.526342 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.535581 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.545024 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.555092 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.564477 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.576176 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.585723 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.595083 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.604400 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.613771 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.622955 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.632267 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.641758 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.651021 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.660107 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.669829 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.680159 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.693065 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.706668 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.715571 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.725291 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.734716 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.744161 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.756290 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.768959 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.779481 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.789183 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.798791 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.808124 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.817938 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.828190 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.838512 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.850548 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.860140 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.872703 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.882297 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.893014 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.905581 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.915178 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.924645 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.934312 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.946912 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.956270 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.966963 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.992414 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.001759 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.011524 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.020730 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.029984 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.039973 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.050037 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.060621 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.071037 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.080757 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.092071 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.102319 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.113067 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.123617 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.134136 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.143985 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.152736 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.162962 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.171574 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.180757 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.190068 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.199051 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.209866 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.219971 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.232758 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.241735 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.251308 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.262895 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.275298 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.288316 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.296931 [3] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.306134 [3] Warning: no training nodes in this partition! Backward fake loss.
22:24:49.247254 [3] proc begin: <DistEnv 3/4 nccl>
22:24:59.259896 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
22:24:59.273697 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:26:06.073116 [3] proc begin: <DistEnv 3/4 nccl>
22:26:12.255743 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
22:26:12.272618 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:27:25.104667 [3] proc begin: <DistEnv 3/4 nccl>
22:27:30.888343 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
22:27:30.914197 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:28:35.157402 [3] proc begin: <DistEnv 3/4 nccl>
22:28:41.832099 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
22:28:41.848897 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:33:09.626493 [3] proc begin: <DistEnv 3/4 nccl>
22:33:31.352551 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
22:33:31.369098 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:33:36.338980 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:37.353857 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:37.535647 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:37.716442 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:37.897212 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.078483 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.258653 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.440002 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.619655 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.799549 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.980144 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:39.159876 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:39.339164 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:39.522279 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:39.702814 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:39.882462 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.062215 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.242595 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.423184 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.603088 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.784442 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.964526 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:41.144892 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:41.329635 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:41.511126 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:41.691677 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:41.871822 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.052222 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.232226 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.414623 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.595128 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.775534 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.956222 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:43.137077 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:43.317967 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:43.500181 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:43.683865 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:43.864854 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.045759 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.226383 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.406586 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.587486 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.779549 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.965549 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:45.145978 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:45.326491 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:45.507226 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:45.687505 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:45.867856 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.050294 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.229820 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.410327 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.591920 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.774120 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.956178 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:47.136579 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:47.316324 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:47.496359 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:47.678114 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:47.858046 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.038678 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.219234 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.399088 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.579247 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.760293 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.941596 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:49.121208 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:49.301887 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:49.482329 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:49.662878 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:49.843685 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.025281 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.206476 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.387137 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.569836 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.749762 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.929938 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:51.111147 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:51.291960 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:51.472897 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:51.653322 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:51.834153 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.013852 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.194453 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.374940 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.556233 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.737336 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.917943 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:53.099323 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:53.280223 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:53.460807 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:53.641665 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:53.822412 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.002818 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.182716 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.363037 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.543713 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.723551 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.903490 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.084152 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.264635 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.445108 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.627375 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.807447 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.988252 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:56.168753 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:56.349294 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:56.529387 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:56.709707 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:56.891622 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.072176 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.252230 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.432938 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.613079 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.792879 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.973414 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:58.153785 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:58.333764 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:58.515124 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:58.695404 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:58.875139 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.055253 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.234774 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.415763 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.596295 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.776474 [3] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.956502 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:00.136635 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:00.316187 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:00.497100 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:00.676911 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:00.856621 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.036577 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.216582 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.398520 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.578849 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.760027 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.949713 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:02.138375 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:02.328390 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:02.517121 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:02.705127 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:02.893119 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.081997 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.268407 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.449838 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.630290 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.810649 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.991529 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:04.172277 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:04.353189 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:04.534143 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:04.714991 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:04.897542 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.078735 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.259600 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.441257 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.622000 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.802406 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.983524 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:06.164051 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:06.346933 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:06.528045 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:06.709060 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:06.890429 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.071724 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.252719 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.433988 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.615238 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.796648 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.978018 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:08.159211 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:08.340715 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:08.521600 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:08.704190 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:08.886560 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.067916 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.249783 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.431125 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.613157 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.793947 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.974567 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:10.155772 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:10.337104 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:10.517834 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:10.698382 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:10.879120 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.060510 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.241517 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.422475 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.603786 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.785060 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.965876 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:12.148026 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:12.328963 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:12.509596 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:12.689559 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:12.869886 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:13.049740 [3] Warning: no training nodes in this partition! Backward fake loss.
22:34:13.229957 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:03.172503 [3] proc begin: <DistEnv 3/4 nccl>
22:35:08.827439 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
22:35:08.847455 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:35:13.884661 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:15.434356 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:16.203103 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:16.972486 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:17.742151 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:18.511083 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:19.278739 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:20.053071 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:20.823590 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:21.592385 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:22.359842 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:23.128077 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:23.895981 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:24.666027 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:25.436609 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:26.206526 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:26.978925 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:27.748903 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:28.519680 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:29.289618 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:30.059892 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:30.831402 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:31.601346 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:32.373677 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:33.142264 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:33.911371 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:34.679386 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:35.449110 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:36.217941 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:36.986756 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:37.755720 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:38.524778 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:39.293771 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:40.062707 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:40.833305 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:41.603662 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:42.375056 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:43.144752 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:43.913550 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:44.683621 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:45.454039 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:46.225881 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:46.996542 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:47.766665 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:48.535696 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:49.304672 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:50.074940 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:50.844596 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:51.615924 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:52.385987 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:53.155877 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:53.926914 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:54.696934 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:55.467596 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:56.237555 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:57.007494 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:57.777287 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:58.548590 [3] Warning: no training nodes in this partition! Backward fake loss.
22:35:59.317900 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:00.087312 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:00.858330 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:01.629016 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:02.428507 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:03.233733 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:04.003940 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:04.774384 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:05.544956 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:06.316205 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:07.087272 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:07.857062 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:08.627748 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:09.398379 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:10.169864 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:10.941549 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:11.712190 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:12.482182 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:13.251829 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:14.021093 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:14.791503 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:15.562656 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:16.332857 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:17.102187 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:17.873037 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:18.643911 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:19.415086 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:20.186587 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:20.957396 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:21.726961 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:22.495007 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:23.267463 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:24.031417 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:24.799179 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:25.570123 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:26.337891 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:27.106999 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:27.876464 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:28.645892 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:29.414053 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:30.183950 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:30.969236 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:31.740570 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:32.512615 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:33.282816 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:34.052084 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:34.819859 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:35.588713 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:36.357639 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:37.126055 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:37.894982 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:38.664890 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:39.434770 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:40.204593 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:40.974509 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:41.742749 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:42.511398 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:43.278879 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:44.048781 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:44.817228 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:45.586096 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:46.354832 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:47.122985 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:47.892235 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:48.660780 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:49.429720 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:50.197439 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:50.966498 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:51.734071 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:52.500487 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:53.266825 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:54.034301 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:54.802871 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:55.577017 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:56.344979 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:57.111485 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:57.878325 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:58.645462 [3] Warning: no training nodes in this partition! Backward fake loss.
22:36:59.412662 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:00.181214 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:00.951508 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:01.738197 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:02.543223 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:03.327775 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:04.097703 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:04.867036 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:05.636458 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:06.406490 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:07.175919 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:07.947307 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:08.715658 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:09.484830 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:10.253943 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:11.024953 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:11.794629 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:12.564524 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:13.334518 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:14.104835 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:14.874494 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:15.644335 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:16.414917 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:17.184033 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:17.954522 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:18.725368 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:19.495080 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:20.264709 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:21.035084 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:21.804629 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:22.573288 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:23.342555 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:24.110150 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:24.879614 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:25.648299 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:26.416314 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:27.184873 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:27.953124 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:28.720955 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:29.488050 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:30.256136 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:31.023417 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:31.791285 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:32.560670 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:33.328156 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:34.095145 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:34.862807 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:35.630797 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:36.399245 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:37.167337 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:37.935052 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:38.704798 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:39.474941 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:40.243061 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:41.011730 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:41.778712 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:42.546845 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:43.314776 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:44.082264 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:44.849583 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:45.617988 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:46.385041 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:47.153708 [3] Warning: no training nodes in this partition! Backward fake loss.
22:37:47.919938 [3] Warning: no training nodes in this partition! Backward fake loss.
22:38:43.761779 [3] proc begin: <DistEnv 3/4 nccl>
22:38:49.055665 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
22:38:49.080696 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:38:54.362688 [3] Warning: no training nodes in this partition! Backward fake loss.
22:38:55.892667 [3] Warning: no training nodes in this partition! Backward fake loss.
22:38:56.659921 [3] Warning: no training nodes in this partition! Backward fake loss.
22:38:57.426643 [3] Warning: no training nodes in this partition! Backward fake loss.
22:38:58.195548 [3] Warning: no training nodes in this partition! Backward fake loss.
22:38:58.964828 [3] Warning: no training nodes in this partition! Backward fake loss.
22:38:59.733504 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:00.501988 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:01.271308 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:02.069314 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:02.872642 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:03.641062 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:04.407379 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:05.175400 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:05.944284 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:06.713587 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:07.482298 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:08.252399 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:09.023236 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:09.793134 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:10.564822 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:11.334808 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:12.106085 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:12.875409 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:13.646340 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:14.415699 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:15.185693 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:15.955307 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:16.724249 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:17.491151 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:18.258378 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:19.025487 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:19.793072 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:20.562984 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:21.329796 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:22.097489 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:22.864540 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:23.632118 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:24.399909 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:25.168704 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:25.938835 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:26.708440 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:27.477921 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:28.246025 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:29.016429 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:29.785605 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:30.555892 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:31.325330 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:32.094320 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:32.864242 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:33.634958 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:34.404636 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:35.175113 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:35.946499 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:36.715695 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:37.484832 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:38.255080 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:39.024727 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:39.794278 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:40.564180 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:41.332857 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:42.102852 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:42.873894 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:43.643419 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:44.413307 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:45.183045 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:45.953077 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:46.723804 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:47.493790 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:48.263151 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:49.032504 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:49.802987 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:50.573097 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:51.343011 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:52.112050 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:52.882434 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:53.650919 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:54.420727 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:55.190405 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:55.960749 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:56.731279 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:57.500793 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:58.270673 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:59.040187 [3] Warning: no training nodes in this partition! Backward fake loss.
22:39:59.809630 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:00.579995 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:01.348905 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:02.125744 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:02.926854 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:03.724598 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:04.492835 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:05.262851 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:06.032245 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:06.802018 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:07.572133 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:08.343535 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:09.112816 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:09.883286 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:10.653329 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:11.422958 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:12.193210 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:12.963013 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:13.732788 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:14.502794 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:15.272365 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:16.043096 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:16.813027 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:17.582208 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:18.351731 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:19.121538 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:19.890965 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:20.659206 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:21.426973 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:22.194523 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:22.961209 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:23.729594 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:24.497674 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:25.266157 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:26.033740 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:26.802231 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:27.570182 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:28.337411 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:29.105692 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:29.873850 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:30.643253 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:31.410948 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:32.179050 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:32.948815 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:33.718598 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:34.487539 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:35.257314 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:36.025939 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:36.796075 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:37.566920 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:38.336642 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:39.106081 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:39.876229 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:40.646985 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:41.416787 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:42.185018 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:42.953106 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:43.719825 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:44.487559 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:45.254888 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:46.022827 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:46.790908 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:47.558689 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:48.326388 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:49.094853 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:49.863538 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:50.632967 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:51.401707 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:52.169882 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:52.937552 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:53.704242 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:54.471854 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:55.238902 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:56.007010 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:56.774208 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:57.542237 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:58.309470 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:59.074237 [3] Warning: no training nodes in this partition! Backward fake loss.
22:40:59.840789 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:00.606986 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:01.373496 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:02.166369 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:02.969287 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:03.740290 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:04.509600 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:05.277661 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:06.046420 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:06.814927 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:07.583341 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:08.351519 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:09.120208 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:09.888944 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:10.657768 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:11.427148 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:12.194780 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:12.963629 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:13.731927 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:14.499877 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:15.267183 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:16.035723 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:16.804131 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:17.572463 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:18.340388 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:19.108255 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:19.878916 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:20.648136 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:21.415523 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:22.182921 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:22.950532 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:23.718243 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:24.484809 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:25.253239 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:26.021705 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:26.787924 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:27.554431 [3] Warning: no training nodes in this partition! Backward fake loss.
22:41:28.320801 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:23.373476 [3] proc begin: <DistEnv 3/4 nccl>
22:42:28.368728 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
22:42:28.391322 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:42:32.425382 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:33.867596 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:34.635825 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:35.403207 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:36.169487 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:36.936372 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:37.704116 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:38.470275 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:39.240297 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:40.006621 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:40.774746 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:41.542006 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:42.309435 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:43.077166 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:43.846183 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:44.614425 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:45.382532 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:46.150038 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:46.918357 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:47.685012 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:48.452028 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:49.219167 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:49.986905 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:50.754174 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:51.522121 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:52.290048 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:53.057652 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:53.825890 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:54.595146 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:55.362715 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:56.130334 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:56.898150 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:57.666489 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:58.432390 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:59.199968 [3] Warning: no training nodes in this partition! Backward fake loss.
22:42:59.969067 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:00.736778 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:01.505022 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:02.300292 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:03.103338 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:03.876887 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:04.646048 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:05.415222 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:06.185801 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:06.953810 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:07.723463 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:08.493556 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:09.263154 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:10.032621 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:10.801380 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:11.571341 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:12.340749 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:13.110084 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:13.879814 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:14.648938 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:15.418748 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:16.187552 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:16.956107 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:17.723969 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:18.492427 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:19.260264 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:20.028366 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:20.797376 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:21.564498 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:22.332669 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:23.101267 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:23.869956 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:24.639003 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:25.408145 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:26.176837 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:26.945744 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:27.715168 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:28.484529 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:29.253725 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:30.023510 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:30.793490 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:31.561410 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:32.344771 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:33.113348 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:33.883364 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:34.651121 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:35.418899 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:36.186005 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:36.954186 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:37.721732 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:38.489451 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:39.257301 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:40.026168 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:40.794301 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:41.564948 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:42.330146 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:43.098885 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:43.865842 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:44.633083 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:45.400179 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:46.167348 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:46.934625 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:47.700211 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:48.467583 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:49.234745 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:50.001238 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:50.767387 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:51.533949 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:52.300525 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:53.067637 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:53.834058 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:54.600235 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:55.367465 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:56.135963 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:56.904113 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:57.671187 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:58.437355 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:59.203817 [3] Warning: no training nodes in this partition! Backward fake loss.
22:43:59.970516 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:00.737833 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:01.513051 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:02.314339 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:03.104433 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:03.876475 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:04.645718 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:05.414405 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:06.183490 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:06.953282 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:07.721262 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:08.489570 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:09.257755 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:10.027428 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:10.796159 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:11.565946 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:12.334481 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:13.104595 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:13.872879 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:14.641616 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:15.409811 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:16.177789 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:16.946192 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:17.713680 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:18.483077 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:19.251294 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:20.021105 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:20.790209 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:21.557300 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:22.324610 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:23.092417 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:23.860556 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:24.627268 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:25.393683 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:26.161413 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:26.928569 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:27.696542 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:28.464909 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:29.232242 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:30.000414 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:30.767100 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:31.534440 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:32.302626 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:33.070168 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:33.841330 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:34.609539 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:35.377636 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:36.145573 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:36.914630 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:37.682345 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:38.451007 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:39.218998 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:39.986861 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:40.755563 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:41.523064 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:42.291315 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:43.059732 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:43.828891 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:44.597818 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:45.366442 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:46.134594 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:46.903618 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:47.672013 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:48.439447 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:49.206377 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:49.973971 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:50.741041 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:51.509084 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:52.276184 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:53.043664 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:53.811843 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:54.579739 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:55.348090 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:56.117776 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:56.886298 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:57.656019 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:58.424461 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:59.193690 [3] Warning: no training nodes in this partition! Backward fake loss.
22:44:59.961132 [3] Warning: no training nodes in this partition! Backward fake loss.
22:45:00.730765 [3] Warning: no training nodes in this partition! Backward fake loss.
22:45:01.499602 [3] Warning: no training nodes in this partition! Backward fake loss.
22:45:02.285783 [3] Warning: no training nodes in this partition! Backward fake loss.
22:45:03.088811 [3] Warning: no training nodes in this partition! Backward fake loss.
22:45:03.868517 [3] Warning: no training nodes in this partition! Backward fake loss.
22:45:04.639404 [3] Warning: no training nodes in this partition! Backward fake loss.
22:45:05.410174 [3] Warning: no training nodes in this partition! Backward fake loss.
22:45:06.181241 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:05.745858 [3] proc begin: <DistEnv 3/4 nccl>
22:47:11.338457 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
22:47:11.356546 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:47:16.653827 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:18.276736 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:19.046456 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:19.815656 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:20.586182 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:21.357351 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:22.128362 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:22.898158 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:23.667773 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:24.438817 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:25.207046 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:25.977505 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:26.748415 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:27.519011 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:28.290991 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:29.059933 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:29.829380 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:30.599110 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:31.369607 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:32.139372 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:32.908736 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:33.679825 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:34.451937 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:35.222554 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:35.992451 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:36.761880 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:37.529367 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:38.297456 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:39.064903 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:39.832947 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:40.600427 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:41.366528 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:42.134662 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:42.901946 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:43.668798 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:44.435141 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:45.201420 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:45.969759 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:46.735985 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:47.503537 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:48.269363 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:49.036951 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:49.804267 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:50.573668 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:51.341473 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:52.109945 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:52.877853 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:53.646378 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:54.413825 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:55.180929 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:55.949946 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:56.717516 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:57.485809 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:58.252358 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:59.019375 [3] Warning: no training nodes in this partition! Backward fake loss.
22:47:59.787652 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:00.556928 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:01.324974 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:02.123224 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:02.925486 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:03.695039 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:04.464980 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:05.235093 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:06.006066 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:06.775966 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:07.546137 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:08.315598 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:09.086985 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:09.857204 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:10.627634 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:11.396735 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:12.165993 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:12.938254 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:13.708663 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:14.478521 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:15.245522 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:16.013677 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:16.781260 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:17.549771 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:18.317671 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:19.085453 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:19.853408 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:20.623201 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:21.390756 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:22.157973 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:22.926790 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:23.694714 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:24.462553 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:25.230360 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:26.004205 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:26.769695 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:27.537446 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:28.304855 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:29.072756 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:29.840632 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:30.608067 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:31.375963 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:32.143634 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:32.911110 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:33.680007 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:34.449726 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:35.218210 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:35.986411 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:36.754025 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:37.521524 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:38.289917 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:39.058099 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:39.824664 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:40.594070 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:41.360874 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:42.127988 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:42.894518 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:43.661998 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:44.429057 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:45.195832 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:45.963043 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:46.729703 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:47.497534 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:48.264963 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:49.033505 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:49.802039 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:50.570841 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:51.337891 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:52.106707 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:52.875340 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:53.643672 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:54.411850 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:55.179816 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:55.948536 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:56.716260 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:57.484305 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:58.253260 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:59.022626 [3] Warning: no training nodes in this partition! Backward fake loss.
22:48:59.790715 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:00.559637 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:01.328263 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:02.101060 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:02.902908 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:03.695072 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:04.464744 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:05.232187 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:06.000682 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:06.769476 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:07.537505 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:08.305526 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:09.074187 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:09.841806 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:10.610990 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:11.378867 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:12.146667 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:12.915137 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:13.682822 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:14.451213 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:15.219356 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:15.987345 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:16.754789 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:17.522582 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:18.292732 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:19.060796 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:19.828497 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:20.596229 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:21.363786 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:22.132181 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:22.900739 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:23.668549 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:24.435860 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:25.203398 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:25.971082 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:26.739039 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:27.507034 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:28.274787 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:29.041755 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:29.810041 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:30.577487 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:31.345716 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:32.112826 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:32.880185 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:33.646641 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:34.414992 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:35.182799 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:35.950938 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:36.718472 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:37.485205 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:38.251491 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:39.018823 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:39.785519 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:40.552755 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:41.319730 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:42.086556 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:42.852823 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:43.619400 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:44.388144 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:45.156392 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:45.924518 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:46.693043 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:47.461737 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:48.230531 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:48.998017 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:49.765467 [3] Warning: no training nodes in this partition! Backward fake loss.
22:49:50.533825 [3] Warning: no training nodes in this partition! Backward fake loss.
00:51:54.822873 [3] proc begin: <DistEnv 3/4 nccl>
00:52:02.001267 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
00:52:02.024992 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

00:53:37.796571 [3] proc begin: <DistEnv 3/4 nccl>
00:53:43.583821 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
00:53:43.604487 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

00:55:14.176657 [3] proc begin: <DistEnv 3/4 nccl>
00:55:21.682000 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
00:55:21.702544 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:06:54.166744 [3] proc begin: <DistEnv 3/4 nccl>
02:06:54.264945 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
02:06:54.277913 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:06:55.773034 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.543384 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.562265 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.578355 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.590345 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.599733 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.609849 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.619183 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.629003 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.638166 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.647461 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.656979 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.666289 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.675723 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.685535 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.694965 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.704750 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.714371 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.723724 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.733234 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.742683 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.752413 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.763742 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.772830 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.782606 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.792328 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.802340 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.811661 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.821309 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.831990 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.841001 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.852361 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.861876 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.874939 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.887485 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.900488 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.909926 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.922293 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.933358 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.943893 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.953180 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.963258 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.972772 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.982342 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.991516 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.000994 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.010378 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.019586 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.029085 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.041245 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.050018 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.059613 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.069613 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.078698 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.087792 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.097232 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.106633 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.124658 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.136945 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.150303 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.159490 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.181854 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.194031 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.206309 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.217057 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.226699 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.236064 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.250767 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.260450 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.270005 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.279983 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.291787 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.304138 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.313396 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.322645 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.332345 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.341854 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.351130 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.360544 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.371817 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.380631 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.389949 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.401963 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.411594 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.420797 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.429991 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.439408 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.448577 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.457881 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.474567 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.489956 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.502131 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.512881 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.525700 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.536875 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.548443 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.563940 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.577470 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.587435 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.598966 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.609277 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.618778 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.628222 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.637490 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.646685 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.655912 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.665409 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.674741 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.684064 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.695858 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.705244 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.714802 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.723939 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.733060 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.742415 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.751627 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.760805 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.770059 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.779135 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.788400 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.797587 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.806622 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.815697 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.824753 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.837329 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.850905 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.863683 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.872823 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.882204 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.894717 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.904041 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.915409 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.928198 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.940980 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.950120 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.959481 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.971825 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.980695 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.989864 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.001836 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.010899 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.020047 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.029481 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.038518 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.047490 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.059750 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.069239 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.078425 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.087590 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.096856 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.106086 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.129989 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.141438 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.160808 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.177589 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.187144 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.196262 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.205494 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.214522 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.223781 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.233126 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.242452 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.266880 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.281005 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.293172 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.305452 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.314497 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.323958 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.335349 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.344756 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.354403 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.363480 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.372506 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.381991 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.391073 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.400384 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.409592 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.418718 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.427931 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.437109 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.446267 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.455388 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.464375 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.473722 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.484368 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.494374 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.503282 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.513887 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.530262 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.542038 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.553201 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.562620 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.571959 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.581369 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.590567 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.599803 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.608808 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.621110 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.630254 [3] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.639476 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:43.005197 [3] proc begin: <DistEnv 3/4 nccl>
02:07:43.044095 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
02:07:43.061939 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:07:44.588360 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.408522 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.426879 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.437198 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.447220 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.456907 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.468489 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.478181 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.488635 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.500490 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.510013 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.519582 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.528790 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.540939 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.550366 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.560828 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.571788 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.582225 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.591847 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.603519 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.612775 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.622650 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.636642 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.647145 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.656891 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.668531 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.677883 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.687303 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.696546 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.705791 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.715300 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.724632 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.734733 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.744491 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.754197 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.763888 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.773706 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.783353 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.794035 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.803720 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.815308 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.826328 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.836019 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.855085 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.868170 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.877749 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.890508 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.899984 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.909253 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.918693 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.929286 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.943730 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.958818 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.970202 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.979391 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.988614 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.999999 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.016280 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.027930 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.038225 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.048497 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.060457 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.074220 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.088563 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.100795 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.112399 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.122020 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.131333 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.143743 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.153972 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.165338 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.175690 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.186185 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.197026 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.207542 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.217995 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.228750 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.239430 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.249258 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.259101 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.269843 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.279618 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.290433 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.301567 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.312942 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.324232 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.333689 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.342893 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.355149 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.377658 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.396608 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.406857 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.416510 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.425980 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.435590 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.445386 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.455657 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.464983 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.474634 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.483827 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.493249 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.502822 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.515426 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.525034 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.534885 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.545961 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.555109 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.564289 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.573711 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.585451 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.596429 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.610072 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.620665 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.629778 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.639121 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.648848 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.660165 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.669217 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.681123 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.690527 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.699485 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.709086 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.718574 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.730819 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.740334 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.753277 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.767124 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.777955 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.789313 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.801384 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.813355 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.824331 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.839767 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.854276 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.865723 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.877106 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.891345 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.901732 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.912953 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.931575 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.952005 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.973761 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.984131 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.993129 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.002129 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.025715 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.038374 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.047596 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.056823 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.066117 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.081073 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.091623 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.103712 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.112692 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.121735 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.130622 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.139552 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.151575 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.162984 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.171879 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.187343 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.200307 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.210962 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.220558 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.231207 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.241231 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.250682 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.259934 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.269132 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.278187 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.287537 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.296659 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.305969 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.314960 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.326538 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.338319 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.350935 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.362903 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.372088 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.381698 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.390986 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.400138 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.409636 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.418915 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.428208 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.437771 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.447081 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.456221 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.465826 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.475321 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.484727 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.494990 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.504147 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.513430 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.522886 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.532271 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.541872 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.551126 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.560488 [3] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.570511 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:51.759284 [3] proc begin: <DistEnv 3/4 nccl>
02:08:51.839249 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
02:08:51.854956 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:08:53.386858 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.200601 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.215152 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.226725 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.236321 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.249239 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.258571 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.268249 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.280752 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.290164 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.299612 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.309417 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.319111 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.328824 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.341913 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.351004 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.360517 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.370339 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.381061 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.390673 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.402467 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.411900 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.421483 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.431253 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.440912 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.450145 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.459628 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.469114 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.478557 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.488835 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.498720 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.510100 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.519479 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.528916 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.539439 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.548920 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.563421 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.581626 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.594310 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.604475 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.616224 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.625303 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.634551 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.643915 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.653358 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.662992 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.678167 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.688684 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.698455 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.707903 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.717568 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.726993 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.737348 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.753798 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.775343 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.789365 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.801806 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.812604 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.822143 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.831414 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.840776 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.850084 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.859206 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.868358 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.877621 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.886748 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.899855 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.911440 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.921282 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.930591 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.939865 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.950273 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.959604 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.971242 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.981404 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.990726 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.999787 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.009297 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.018408 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.027600 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.037078 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.046751 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.055931 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.065548 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.074719 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.083806 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.093030 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.102164 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.111505 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.122014 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.131678 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.141229 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.151398 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.162072 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.171475 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.180662 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.189874 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.198865 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.208081 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.217604 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.226726 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.235939 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.245055 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.258432 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.274805 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.284172 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.294026 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.303616 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.314798 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.324059 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.337215 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.350576 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.360270 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.369534 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.378744 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.388042 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.397449 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.406494 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.415799 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.425376 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.434563 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.443941 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.453819 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.463289 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.473045 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.485397 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.498006 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.507197 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.516604 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.525921 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.535271 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.544919 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.554291 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.564787 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.577193 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.590205 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.599534 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.610526 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.620046 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.629953 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.639247 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.648550 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.657849 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.671962 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.681963 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.691503 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.701116 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.712948 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.725251 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.734564 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.753455 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.772435 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.793453 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.805320 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.816404 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.826596 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.835648 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.845484 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.856206 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.865859 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.875074 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.884407 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.894059 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.909331 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.920610 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.929901 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.939542 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.948924 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.962446 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.973420 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.984520 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.997031 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.006260 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.016248 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.029225 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.039672 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.049202 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.061677 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.073158 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.086347 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.099431 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.108521 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.118183 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.127326 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.136543 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.146233 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.155703 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.165336 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.174684 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.184172 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.193943 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.203176 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.212589 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.221945 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.231655 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.241184 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.250364 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.259827 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.269676 [3] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.279262 [3] Warning: no training nodes in this partition! Backward fake loss.
02:18:49.227236 [3] proc begin: <DistEnv 3/4 nccl>
02:18:53.780329 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
02:18:53.801128 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:18:59.100964 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:00.202529 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:00.485625 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:00.766872 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:01.045852 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:01.327374 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:01.620289 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:01.912769 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:02.205312 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:02.498019 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:02.788582 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:03.072453 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:03.353428 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:03.634095 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:03.914461 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:04.195418 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:04.476531 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:04.757400 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:05.037883 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:05.319071 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:05.601753 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:05.883356 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:06.164947 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:06.446081 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:06.728824 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:07.010384 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:07.291547 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:07.573028 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:07.854065 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:08.135355 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:08.417560 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:08.698600 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:08.980480 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:09.262415 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:09.545017 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:09.827207 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:10.109049 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:10.391969 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:10.672245 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:10.953662 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:11.235141 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:11.515663 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:11.797252 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:12.078694 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:12.359581 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:12.641045 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:12.922693 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:13.203507 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:13.484491 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:13.765797 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:14.046874 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:14.328102 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:14.609938 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:14.891912 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:15.174379 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:15.455632 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:15.737249 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:16.018793 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:16.299911 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:16.581568 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:16.862085 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:17.143304 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:17.424060 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:17.704995 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:17.985923 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:18.266567 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:18.548231 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:18.831510 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:19.112548 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:19.394074 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:19.675577 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:19.957207 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:20.237629 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:20.519460 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:20.800733 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:21.082033 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:21.362594 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:21.643184 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:21.924705 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:22.206677 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:22.488248 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:22.769386 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:23.050622 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:23.331430 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:23.613172 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:23.893956 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:24.175483 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:24.456775 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:24.738044 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:25.019031 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:25.299883 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:25.581039 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:25.862346 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:26.144105 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:26.425564 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:26.706597 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:26.988681 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:27.271043 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:27.552144 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:27.832576 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:28.114127 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:28.396016 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:28.676981 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:28.958707 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:29.240175 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:29.521358 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:29.802881 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:30.083940 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:30.365992 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:30.647216 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:30.929766 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:31.210744 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:31.491733 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:31.773040 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:32.054205 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:32.336665 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:32.618001 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:32.898429 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:33.180919 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:33.461708 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:33.743299 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:34.024461 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:34.305428 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:34.586625 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:34.868367 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:35.149126 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:35.429770 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:35.710651 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:35.991774 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:36.272751 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:36.554970 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:36.835775 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:37.117422 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:37.399309 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:37.680061 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:37.961385 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:38.243765 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:38.525267 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:38.807430 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:39.088980 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:39.370918 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:39.652090 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:39.933236 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:40.215443 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:40.497682 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:40.779264 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:41.061770 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:41.342415 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:41.623508 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:41.904103 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:42.184883 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:42.465336 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:42.745782 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:43.026113 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:43.307954 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:43.589129 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:43.869881 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:44.150498 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:44.431460 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:44.712724 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:44.994440 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:45.275821 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:45.556378 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:45.837434 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:46.118591 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:46.400768 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:46.683017 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:46.964839 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:47.246799 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:47.528180 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:47.809447 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:48.089965 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:48.371021 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:48.652554 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:48.932979 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:49.214343 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:49.496011 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:49.777438 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:50.058468 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:50.340307 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:50.621484 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:50.902718 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:51.184186 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:51.465343 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:51.746398 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:52.027635 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:52.308826 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:52.589597 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:52.869602 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:53.150727 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:53.431843 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:53.712622 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:53.992923 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:54.273933 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:54.555196 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:54.837363 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:55.118405 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:55.399901 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:55.680192 [3] Warning: no training nodes in this partition! Backward fake loss.
02:19:55.963022 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:06.389227 [3] proc begin: <DistEnv 3/4 nccl>
02:22:12.198382 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
02:22:12.224821 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:22:17.536466 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:18.851995 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:19.333706 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:19.813663 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:20.293052 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:20.774419 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:21.255682 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:21.737684 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:22.218999 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:22.699045 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:23.179294 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:23.660864 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:24.142609 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:24.626668 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:25.109410 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:25.590868 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:26.072803 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:26.554767 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:27.036494 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:27.516857 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:27.995974 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:28.476875 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:28.958343 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:29.440733 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:29.922236 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:30.404353 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:30.887372 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:31.369566 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:31.852657 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:32.334624 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:32.817022 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:33.299331 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:33.783036 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:34.266546 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:34.749250 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:35.230816 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:35.713626 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:36.196173 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:36.676868 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:37.159499 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:37.641510 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:38.122847 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:38.605359 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:39.088404 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:39.570231 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:40.050725 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:40.529178 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:41.009248 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:41.488332 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:41.967447 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:42.447549 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:42.926282 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:43.405263 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:43.884312 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:44.363143 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:44.842013 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:45.320544 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:45.798778 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:46.276675 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:46.755650 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:47.233789 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:47.713649 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:48.191924 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:48.669758 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:49.148170 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:49.627209 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:50.105935 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:50.585046 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:51.064583 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:51.542798 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:52.021207 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:52.500131 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:52.978751 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:53.457353 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:53.936528 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:54.415411 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:54.893531 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:55.372605 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:55.852420 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:56.331725 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:56.810505 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:57.289154 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:57.767793 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:58.246309 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:58.725177 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:59.204464 [3] Warning: no training nodes in this partition! Backward fake loss.
02:22:59.682929 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:00.162319 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:00.641022 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:01.120608 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:01.614687 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:02.116565 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:02.618433 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:03.104208 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:03.584467 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:04.063790 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:04.542887 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:05.021777 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:05.500661 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:05.980124 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:06.459061 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:06.938036 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:07.417904 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:07.896853 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:08.376620 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:08.855170 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:09.333575 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:09.812925 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:10.292058 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:10.771062 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:11.250541 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:11.729346 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:12.208063 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:12.687147 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:13.167466 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:13.646441 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:14.127740 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:14.607142 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:15.086519 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:15.566623 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:16.046335 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:16.525708 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:17.003881 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:17.480256 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:17.957938 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:18.435574 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:18.913197 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:19.390147 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:19.867974 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:20.345626 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:20.822557 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:21.301488 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:21.779215 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:22.257134 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:22.734696 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:23.212298 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:23.689572 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:24.167643 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:24.645695 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:25.124713 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:25.604024 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:26.083040 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:26.562235 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:27.040307 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:27.518098 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:27.996929 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:28.475738 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:28.954887 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:29.433881 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:29.912234 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:30.390847 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:30.869493 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:31.348276 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:31.826443 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:32.305535 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:32.783463 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:33.261636 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:33.742790 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:34.222439 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:34.701777 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:35.182049 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:35.661075 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:36.141166 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:36.620654 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:37.100509 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:37.579383 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:38.058972 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:38.538334 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:39.018110 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:39.495794 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:39.974654 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:40.452217 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:40.930734 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:41.409456 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:41.886786 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:42.364151 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:42.841914 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:43.320534 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:43.799714 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:44.279085 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:44.759006 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:45.238462 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:45.717249 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:46.196762 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:46.676163 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:47.155213 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:47.635451 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:48.115551 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:48.598170 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:49.078658 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:49.560362 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:50.041884 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:50.522004 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:51.002412 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:51.484626 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:51.966140 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:52.447366 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:52.928731 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:53.410197 [3] Warning: no training nodes in this partition! Backward fake loss.
02:23:53.892222 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:12.788612 [3] proc begin: <DistEnv 3/4 nccl>
02:25:17.880210 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
02:25:17.907948 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:25:23.311540 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:24.797870 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:25.688729 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:26.577326 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:27.463985 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:28.352269 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:29.239091 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:30.124810 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:31.009610 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:31.895071 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:32.780541 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:33.663832 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:34.549632 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:35.433515 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:36.317614 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:37.199564 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:38.082201 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:38.963399 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:39.848593 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:40.731620 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:41.614536 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:42.498060 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:43.378505 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:44.262374 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:45.144436 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:46.028231 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:46.909996 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:47.794044 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:48.675844 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:49.557729 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:50.440269 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:51.322164 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:52.205464 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:53.089239 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:53.969863 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:54.851952 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:55.732145 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:56.612202 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:57.493081 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:58.376265 [3] Warning: no training nodes in this partition! Backward fake loss.
02:25:59.259044 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:00.142482 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:01.025497 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:01.917211 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:02.836743 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:03.732796 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:04.616448 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:05.501607 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:06.384974 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:07.270373 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:08.155838 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:09.041917 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:09.928035 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:10.813998 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:11.701568 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:12.587432 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:13.473199 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:14.359525 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:15.243442 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:16.128474 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:17.013860 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:17.901478 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:18.786777 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:19.673518 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:20.558015 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:21.444397 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:22.331511 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:23.217627 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:24.103594 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:24.989370 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:25.874422 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:26.758456 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:27.643422 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:28.526547 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:29.413445 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:30.298654 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:31.185319 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:32.068649 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:32.955096 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:33.838644 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:34.723609 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:35.607756 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:36.490781 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:37.375416 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:38.258794 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:39.141349 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:40.024253 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:40.908305 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:41.792722 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:42.676700 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:43.561462 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:44.444554 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:45.327810 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:46.211766 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:47.096545 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:47.980869 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:48.865965 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:49.749909 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:50.633165 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:51.518469 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:52.402257 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:53.286463 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:54.171250 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:55.055018 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:55.938970 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:56.823020 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:57.708868 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:58.593723 [3] Warning: no training nodes in this partition! Backward fake loss.
02:26:59.477659 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:00.360366 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:01.240750 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:02.159480 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:03.069224 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:03.953651 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:04.835702 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:05.718925 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:06.605767 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:07.489983 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:08.374743 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:09.260440 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:10.145393 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:11.027833 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:11.911666 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:12.794697 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:13.679399 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:14.562826 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:15.447835 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:16.332256 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:17.217606 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:18.101244 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:18.986593 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:19.871441 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:20.756208 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:21.640210 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:22.525481 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:23.409974 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:24.294805 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:25.177800 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:26.062054 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:26.946141 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:27.830119 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:28.712797 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:29.595599 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:30.479620 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:31.362326 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:32.244620 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:33.127805 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:34.010741 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:34.895320 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:35.777875 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:36.660086 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:37.544733 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:38.427769 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:39.311048 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:40.194692 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:41.077406 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:41.961078 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:42.841526 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:43.722558 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:44.606985 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:45.489465 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:46.371757 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:47.255707 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:48.139244 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:49.020361 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:49.903476 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:50.788888 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:51.669788 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:52.551863 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:53.434602 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:54.318514 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:55.200415 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:56.080371 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:56.960787 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:57.842546 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:58.725057 [3] Warning: no training nodes in this partition! Backward fake loss.
02:27:59.606801 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:00.488756 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:01.371655 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:02.272515 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:03.192617 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:04.079613 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:04.965117 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:05.850250 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:06.736379 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:07.622414 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:08.508085 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:09.393538 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:10.281395 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:11.167392 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:12.052215 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:12.936494 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:13.822214 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:14.707330 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:15.591731 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:16.477848 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:17.362008 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:18.246977 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:19.130834 [3] Warning: no training nodes in this partition! Backward fake loss.
02:28:20.017507 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:32.518764 [3] proc begin: <DistEnv 3/4 nccl>
02:29:32.579471 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
02:29:32.594270 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:29:34.069084 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.838769 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.855285 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.869110 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.879667 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.890531 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.900951 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.913912 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.925151 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.937734 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.954222 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.969502 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.982339 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.991661 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.001106 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.010619 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.019540 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.032840 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.043150 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.053334 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.063419 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.073047 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.083256 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.094222 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.104605 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.115112 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.126034 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.136602 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.147025 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.157429 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.167719 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.178172 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.188384 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.198812 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.210436 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.223469 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.233161 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.244444 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.253420 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.262781 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.271920 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.282551 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.291964 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.301336 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.310597 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.325866 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.335221 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.344960 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.354395 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.369636 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.382099 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.390894 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.404684 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.414507 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.423779 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.434942 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.449958 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.462672 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.477203 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.490047 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.499138 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.508305 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.523636 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.533897 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.547696 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.565750 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.581170 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.590665 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.602855 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.611714 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.621843 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.630391 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.639281 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.647924 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.657560 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.667699 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.679062 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.688630 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.700462 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.709488 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.718506 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.727631 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.738270 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.746821 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.755525 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.766799 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.776345 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.785506 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.797061 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.808989 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.821260 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.830356 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.839269 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.848929 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.858890 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.868480 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.878283 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.887662 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.896835 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.908805 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.921828 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.934821 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.943928 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.953376 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.963339 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.974226 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.982991 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.992231 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.001435 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.012062 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.023986 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.033033 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.041935 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.050994 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.061363 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.073041 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.085618 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.097894 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.109928 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.123556 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.132979 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.144655 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.155249 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.164723 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.174104 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.183639 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.196153 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.206575 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.215738 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.225394 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.236197 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.245444 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.254582 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.263600 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.272805 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.282080 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.291463 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.301258 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.311748 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.324621 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.335913 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.348068 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.361710 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.371560 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.380739 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.390220 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.399327 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.408847 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.418514 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.427646 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.436714 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.445941 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.457487 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.468814 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.484392 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.496188 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.506347 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.515449 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.534782 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.545214 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.568563 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.580850 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.592881 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.602496 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.611516 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.620723 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.631842 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.641001 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.650123 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.659425 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.668595 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.677551 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.686457 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.695875 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.705371 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.714360 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.723336 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.732306 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.743653 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.754029 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.763226 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.775795 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.791626 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.804667 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.814389 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.823515 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.834987 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.849628 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.860298 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.869891 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.878938 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.888321 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.899956 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.911772 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.920544 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.929800 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.938879 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.948204 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.958843 [3] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.968506 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:11.234463 [3] proc begin: <DistEnv 3/4 nccl>
02:30:11.304771 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
02:30:11.317470 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:30:12.799246 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.568892 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.586034 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.598904 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.610861 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.620252 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.632963 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.646147 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.659508 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.668584 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.678014 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.687487 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.699345 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.710049 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.719512 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.728991 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.738421 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.747614 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.757440 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.770492 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.780977 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.795475 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.805746 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.817973 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.827616 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.837197 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.846798 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.856118 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.865716 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.875484 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.884890 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.897331 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.906944 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.916367 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.926726 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.936332 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.946194 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.958537 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.971118 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.983527 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.996835 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.009610 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.021054 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.030692 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.040108 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.050614 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.060381 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.072462 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.082585 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.092587 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.102417 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.112068 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.121739 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.131124 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.140713 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.150596 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.164850 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.184372 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.195945 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.206445 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.216181 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.225831 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.239357 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.249356 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.258867 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.274417 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.285387 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.295912 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.305563 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.316385 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.325655 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.335189 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.344744 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.358772 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.370952 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.380250 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.389787 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.400660 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.411891 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.421066 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.430776 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.441013 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.452803 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.463489 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.474276 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.484890 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.494830 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.504492 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.514036 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.531994 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.548300 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.558169 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.567721 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.577085 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.586395 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.595389 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.605204 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.618701 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.629901 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.639406 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.649204 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.660316 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.670166 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.679738 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.689040 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.700917 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.710112 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.720637 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.730088 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.739360 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.752289 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.761769 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.773705 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.782882 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.792425 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.802039 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.811247 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.820928 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.830186 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.839560 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.850946 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.867418 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.879735 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.889457 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.899364 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.908870 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.918409 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.927582 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.936983 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.946437 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.955808 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.965653 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.979863 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.992139 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.001550 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.011088 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.020658 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.030298 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.039545 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.048743 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.058610 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.067988 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.077627 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.086792 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.098154 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.107203 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.116667 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.128562 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.140315 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.149999 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.159397 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.181734 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.192227 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.204611 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.214477 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.223864 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.233107 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.257099 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.279987 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.291577 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.301070 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.310443 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.319644 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.329023 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.338291 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.350233 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.359303 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.368929 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.378120 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.387367 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.396525 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.406100 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.415210 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.426618 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.435620 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.445150 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.457841 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.469538 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.478803 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.493595 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.509438 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.523699 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.536079 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.546487 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.555827 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.565277 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.574839 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.584677 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.597171 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.608988 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.618213 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.627386 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.638074 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.648567 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.658072 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.667184 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.678024 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.687040 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.696405 [3] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.705818 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:49.157989 [3] proc begin: <DistEnv 3/4 nccl>
02:31:49.214942 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
02:31:49.227982 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:31:50.790754 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.570939 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.587162 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.596865 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.608901 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.625705 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.636212 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.646514 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.656054 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.665691 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.675031 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.687546 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.696750 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.709091 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.718283 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.727707 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.737174 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.746545 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.755975 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.765655 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.777498 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.787027 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.796775 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.806259 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.817343 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.827140 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.836342 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.846486 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.855756 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.867374 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.876539 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.885932 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.898685 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.910435 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.923727 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.933567 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.944520 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.957051 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.966887 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.977212 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.986859 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.996785 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.008736 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.019636 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.028909 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.039541 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.048912 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.058382 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.067678 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.076915 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.086305 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.095666 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.105405 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.122472 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.134596 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.143762 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.153335 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.165648 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.178574 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.190339 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.199642 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.209462 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.221474 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.231020 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.240124 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.249344 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.258510 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.267616 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.281012 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.290939 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.300761 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.310472 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.319550 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.328934 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.343288 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.352999 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.362228 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.378231 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.387623 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.396976 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.406377 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.415700 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.424952 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.435098 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.445331 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.457057 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.467681 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.477176 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.488028 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.516218 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.536874 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.550752 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.560181 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.570431 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.579866 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.589307 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.605449 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.616104 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.627243 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.636935 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.649318 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.658892 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.668150 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.677542 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.686814 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.699177 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.708436 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.717739 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.732516 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.742017 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.751040 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.762941 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.775912 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.785558 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.794629 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.804210 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.813871 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.823248 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.832369 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.841803 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.850644 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.866139 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.877380 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.888402 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.899167 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.908992 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.919349 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.929398 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.939732 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.950363 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.960796 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.973841 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.983357 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.992253 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.001319 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.010367 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.019345 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.028682 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.038093 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.047039 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.058012 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.070838 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.079685 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.090597 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.103977 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.119468 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.129931 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.139105 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.148208 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.157434 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.184148 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.195267 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.209627 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.222321 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.234786 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.243656 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.255342 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.264463 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.274005 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.289090 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.299083 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.308631 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.319704 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.330116 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.342142 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.351150 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.360448 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.369514 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.382499 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.394508 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.406595 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.415299 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.427237 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.436419 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.445692 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.454811 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.463938 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.473602 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.482791 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.492044 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.504618 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.513788 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.523016 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.531997 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.541246 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.553732 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.565989 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.583382 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.595800 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.605935 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.615146 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.624305 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.633647 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.642847 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.652023 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.661150 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.670433 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.681464 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.690876 [3] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.700198 [3] Warning: no training nodes in this partition! Backward fake loss.
02:32:59.900474 [3] proc begin: <DistEnv 3/4 nccl>
02:33:06.596892 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
02:33:06.613490 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:33:51.464521 [3] proc begin: <DistEnv 3/4 nccl>
02:33:57.707011 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
02:33:57.724068 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:35:06.675789 [3] proc begin: <DistEnv 3/4 nccl>
02:35:12.749322 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
02:35:12.763178 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:36:39.128613 [3] proc begin: <DistEnv 3/4 nccl>
02:36:39.218272 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
02:36:39.231033 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:36:40.827575 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.690043 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.726134 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.761710 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.790750 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.820434 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.848682 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.877604 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.906984 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.937055 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.962231 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.995376 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.025355 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.058338 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.079432 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.111596 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.135595 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.167581 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.196910 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.224989 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.254833 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.275021 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.305529 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.336297 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.355308 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.389029 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.411992 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.440789 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.473413 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.499961 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.529094 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.557455 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.586544 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.615372 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.642604 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.670326 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.701095 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.733725 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.767644 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.799291 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.830447 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.862621 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.894492 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.924309 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.957888 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.985654 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.019960 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.050700 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.081558 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.111181 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.155378 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.198779 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.228340 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.257666 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.291745 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.321771 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.343038 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.373509 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.413403 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.441946 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.469815 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.498212 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.528211 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.553983 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.582900 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.613787 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.643554 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.674406 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.704416 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.733216 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.757539 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.791752 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.812511 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.840936 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.871111 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.904723 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.935769 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.963496 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.985398 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.016017 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.045879 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.076337 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.101696 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.136114 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.188222 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.217598 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.241104 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.271779 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.305472 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.331258 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.361222 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.384659 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.427178 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.460541 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.489609 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.520367 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.543367 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.572609 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.601630 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.628820 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.658263 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.688375 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.716588 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.744921 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.777266 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.808724 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.838188 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.861549 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.891790 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.921059 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.950399 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.978636 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.013657 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.036094 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.065056 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.094258 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.124410 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.163180 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.210494 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.240486 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.268836 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.299588 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.329003 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.359218 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.388698 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.423372 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.452662 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.480758 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.509022 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.538485 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.568420 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.598252 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.628438 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.657449 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.680879 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.711168 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.740210 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.767746 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.797093 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.824600 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.850349 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.877027 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.897829 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.926160 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.956344 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.980856 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.011558 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.038820 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.068233 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.097419 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.127983 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.169585 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.203808 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.231329 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.257085 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.287703 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.321733 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.348869 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.375937 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.405363 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.443015 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.463297 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.493956 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.523957 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.551419 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.579315 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.611040 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.640719 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.664058 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.695083 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.724003 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.750857 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.781315 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.812452 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.846442 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.866665 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.900875 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.932364 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.961517 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.992003 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.019090 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.047522 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.074683 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.103756 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.133079 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.164744 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.218756 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.249624 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.277485 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.305248 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.334035 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.365466 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.394270 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.422688 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.458178 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.488714 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.517906 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.546694 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.577953 [3] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.607000 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:14.215018 [3] proc begin: <DistEnv 3/4 nccl>
02:37:14.304125 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
02:37:14.318471 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:37:15.670682 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.436596 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.462866 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.487242 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.507917 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.529545 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.555102 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.579893 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.603799 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.629664 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.658851 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.686334 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.710919 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.735345 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.758498 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.779117 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.800619 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.827320 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.850172 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.874581 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.901267 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.926645 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.948850 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.971822 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.989122 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.001947 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.016215 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.029672 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.051808 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.074690 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.097730 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.124272 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.144779 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.167360 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.195823 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.217455 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.243271 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.259776 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.283597 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.306763 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.325873 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.350293 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.371355 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.387599 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.410529 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.427949 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.450597 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.469741 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.494334 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.515761 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.538250 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.561104 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.586941 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.609923 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.632165 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.654367 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.675509 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.698726 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.717962 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.739885 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.762400 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.786380 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.811120 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.832471 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.853904 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.877538 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.898628 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.923961 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.947363 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.963902 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.981327 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.005419 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.029632 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.054261 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.077714 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.099420 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.122506 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.142855 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.163532 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.185447 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.211275 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.240439 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.259093 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.279858 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.300941 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.322127 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.338571 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.359262 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.378624 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.395894 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.419501 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.445250 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.470506 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.494697 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.518783 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.537788 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.562545 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.586346 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.611885 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.635353 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.658632 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.676213 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.698670 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.715880 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.738811 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.759054 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.782948 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.802281 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.825261 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.846750 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.870259 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.891198 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.915019 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.938051 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.962198 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.981475 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.006277 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.027102 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.047090 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.068065 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.089100 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.111052 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.135157 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.157314 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.182341 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.209755 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.241996 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.264378 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.281124 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.300059 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.313682 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.325326 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.338644 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.361363 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.383241 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.398782 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.410471 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.424663 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.448460 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.470244 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.491245 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.515710 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.537341 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.562447 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.586139 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.608240 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.630803 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.652381 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.676936 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.697649 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.715281 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.738621 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.762280 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.781483 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.804587 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.826917 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.850908 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.870850 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.888898 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.910635 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.934667 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.954791 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.979223 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.999651 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.022272 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.041784 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.066226 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.087827 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.111416 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.134209 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.152583 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.178141 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.202932 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.225733 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.255365 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.278297 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.302867 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.326123 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.349663 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.378362 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.406334 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.431108 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.458017 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.486500 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.509779 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.534211 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.554314 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.579143 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.601483 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.624414 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.650099 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.674939 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.697934 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.720993 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.736087 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.753702 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.771276 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.794316 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.814694 [3] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.838610 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:16.430853 [3] proc begin: <DistEnv 3/4 nccl>
02:38:16.466526 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
02:38:16.479187 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:38:18.170475 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.006996 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.027636 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.043649 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.061578 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.073364 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.083620 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.093639 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.103546 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.113466 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.123704 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.133771 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.145517 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.155507 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.169110 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.187177 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.206242 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.224056 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.247603 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.265857 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.286684 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.303288 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.322822 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.341527 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.360497 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.373534 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.383945 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.394101 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.403604 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.414179 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.424198 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.437423 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.455845 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.475394 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.500205 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.521738 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.539479 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.559638 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.578869 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.598333 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.615638 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.627313 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.637572 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.647067 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.656337 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.666473 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.676079 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.687269 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.697063 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.707677 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.718053 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.728734 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.742762 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.752238 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.763009 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.774292 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.788157 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.797904 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.808275 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.819325 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.835518 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.854225 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.871508 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.891455 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.911467 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.930139 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.947128 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.958423 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.976843 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.997670 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.009066 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.022797 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.032362 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.042244 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.052439 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.062746 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.072664 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.082528 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.097699 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.115334 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.136858 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.155868 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.174304 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.192595 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.216781 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.232279 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.241965 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.252043 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.263396 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.273381 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.283408 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.293389 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.304442 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.316640 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.327583 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.336958 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.355120 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.367113 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.377403 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.387361 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.397521 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.411525 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.421120 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.431214 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.448633 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.462170 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.472584 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.483125 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.499405 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.517854 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.536918 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.557285 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.575835 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.588795 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.607445 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.627883 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.647957 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.667934 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.692734 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.711530 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.730252 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.747976 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.769028 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.787834 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.806242 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.832436 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.852095 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.871438 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.890216 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.911007 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.922191 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.931552 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.941881 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.954693 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.968134 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.987215 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.008417 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.026463 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.046404 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.063963 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.084126 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.103305 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.122506 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.138549 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.148077 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.161926 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.172440 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.185402 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.198071 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.208427 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.218349 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.227966 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.238100 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.247770 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.258201 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.271169 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.290442 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.307184 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.318060 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.327851 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.337644 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.347559 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.357908 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.368385 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.378590 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.389104 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.399714 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.411132 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.422043 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.447814 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.466521 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.485650 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.507329 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.528115 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.540879 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.560873 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.574595 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.584598 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.594769 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.604964 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.616490 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.631291 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.641087 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.651505 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.665335 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.674816 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.684549 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.703008 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.716217 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.725983 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.736843 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.748209 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.757734 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.769969 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.779460 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.792788 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.802421 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.812138 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.826458 [3] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.837309 [3] Warning: no training nodes in this partition! Backward fake loss.
09:41:28.403756 [3] proc begin: <DistEnv 3/4 nccl>
09:42:00.210347 [3] proc begin: <DistEnv 3/4 nccl>
09:42:06.810111 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:42:06.829319 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:43:32.976988 [3] proc begin: <DistEnv 3/4 nccl>
09:43:40.152196 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:43:40.168557 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:45:34.965844 [3] proc begin: <DistEnv 3/4 nccl>
09:45:41.788600 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:45:41.807841 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:48:52.032677 [3] proc begin: <DistEnv 3/4 nccl>
09:48:56.747450 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:48:56.765964 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:49:45.634274 [3] proc begin: <DistEnv 3/4 nccl>
09:49:51.524565 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:49:51.541576 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:50:38.562563 [3] proc begin: <DistEnv 3/4 nccl>
09:50:43.690640 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:50:43.735730 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:52:42.344786 [3] proc begin: <DistEnv 3/4 nccl>
09:52:46.922022 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
09:52:46.953097 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:52:52.636513 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:53.663639 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:53.942572 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:54.222770 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:54.501699 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:54.780157 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:55.058409 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:55.340326 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:55.619269 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:55.898101 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:56.176788 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:56.455352 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:56.734696 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:57.013213 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:57.291501 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:57.571787 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:57.849828 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:58.128601 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:58.407450 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:58.685997 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:58.964336 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:59.245273 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:59.522863 [3] Warning: no training nodes in this partition! Backward fake loss.
09:52:59.801713 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:00.080485 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:00.358894 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:00.638716 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:00.918377 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:01.197359 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:01.476492 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:01.761116 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:02.052356 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:02.344379 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:02.637026 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:02.927257 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:03.214831 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:03.494754 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:03.774381 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:04.054826 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:04.334454 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:04.613860 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:04.893267 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:05.173087 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:05.451101 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:05.729362 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:06.007887 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:06.287866 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:06.568925 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:06.850387 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:07.130409 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:07.411900 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:07.692621 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:07.973239 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:08.253516 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:08.533848 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:08.813986 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:09.095696 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:09.376541 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:09.656926 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:09.937053 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:10.218150 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:10.498251 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:10.778625 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:11.059739 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:11.340042 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:11.619782 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:11.900785 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:12.183652 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:12.464623 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:12.744919 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:13.025298 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:13.305596 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:13.585479 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:13.865877 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:14.146293 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:14.426445 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:14.707552 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:14.988316 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:15.268898 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:15.548893 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:15.830087 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:16.111171 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:16.392098 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:16.672088 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:16.952459 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:17.233830 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:17.514375 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:17.796004 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:18.076232 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:18.357287 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:18.637048 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:18.916999 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:19.198241 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:19.479074 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:19.759434 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:20.039870 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:20.320034 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:20.600605 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:20.881761 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:21.161587 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:21.441306 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:21.721452 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:22.001215 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:22.282354 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:22.562665 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:22.843263 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:23.123773 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:23.404308 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:23.684174 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:23.964063 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:24.244036 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:24.524168 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:24.804094 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:25.083849 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:25.363815 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:25.644983 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:25.924895 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:26.204937 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:26.485186 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:26.766621 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:27.048238 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:27.329315 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:27.609777 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:27.889937 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:28.169133 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:28.447724 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:28.725877 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:29.003705 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:29.281636 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:29.560176 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:29.838893 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:30.117181 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:30.395923 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:30.674440 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:30.952820 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:31.231571 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:31.510048 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:31.789604 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:32.069194 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:32.348958 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:32.628517 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:32.908050 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:33.188753 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:33.468051 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:33.747047 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:34.026527 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:34.305893 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:34.585074 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:34.864414 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:35.144223 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:35.422877 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:35.702121 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:35.980582 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:36.259784 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:36.539130 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:36.818053 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:37.096979 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:37.377030 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:37.657287 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:37.937836 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:38.216343 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:38.494724 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:38.773337 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:39.052413 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:39.331409 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:39.611427 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:39.890497 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:40.168923 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:40.447712 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:40.726749 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:41.005761 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:41.284005 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:41.562342 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:41.841314 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:42.119540 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:42.398435 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:42.677022 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:42.956177 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:43.235315 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:43.513918 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:43.792767 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:44.071263 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:44.349685 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:44.628745 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:44.906987 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:45.185688 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:45.464660 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:45.743280 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:46.022263 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:46.301220 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:46.580371 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:46.859115 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:47.137320 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:47.416383 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:47.695892 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:47.974437 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:48.252950 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:48.532804 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:48.811651 [3] Warning: no training nodes in this partition! Backward fake loss.
09:53:49.090349 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:13.905709 [3] proc begin: <DistEnv 3/4 nccl>
09:54:18.671278 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
09:54:18.688603 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:54:22.811738 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:23.886349 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:24.167734 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:24.448943 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:24.732672 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:25.014291 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:25.295181 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:25.576296 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:25.858201 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:26.138765 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:26.419688 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:26.700018 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:26.981042 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:27.263724 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:27.545063 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:27.826264 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:28.108092 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:28.389400 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:28.670730 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:28.951600 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:29.232872 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:29.513133 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:29.794827 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:30.075597 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:30.356046 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:30.637329 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:30.918619 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:31.199871 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:31.479830 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:31.760584 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:32.041738 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:32.322551 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:32.603040 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:32.883477 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:33.164323 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:33.445202 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:33.725953 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:34.006458 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:34.286147 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:34.567497 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:34.848208 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:35.128247 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:35.408661 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:35.689066 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:35.969574 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:36.249951 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:36.530166 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:36.809513 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:37.089695 [3] Warning: no training nodes in this partition! Backward fake loss.
09:54:37.370536 [3] Warning: no training nodes in this partition! Backward fake loss.
09:55:31.588126 [3] proc begin: <DistEnv 3/4 nccl>
09:55:44.263771 [3] proc begin: <DistEnv 3/4 nccl>
09:55:48.660014 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
09:55:48.674388 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:55:53.216769 [3] Warning: no training nodes in this partition! Backward fake loss.
09:55:54.481624 [3] Warning: no training nodes in this partition! Backward fake loss.
09:55:54.960081 [3] Warning: no training nodes in this partition! Backward fake loss.
09:55:55.438581 [3] Warning: no training nodes in this partition! Backward fake loss.
09:55:55.918020 [3] Warning: no training nodes in this partition! Backward fake loss.
09:55:56.396455 [3] Warning: no training nodes in this partition! Backward fake loss.
09:55:56.878162 [3] Warning: no training nodes in this partition! Backward fake loss.
09:55:57.358028 [3] Warning: no training nodes in this partition! Backward fake loss.
09:55:57.841563 [3] Warning: no training nodes in this partition! Backward fake loss.
09:55:58.321375 [3] Warning: no training nodes in this partition! Backward fake loss.
09:55:58.803136 [3] Warning: no training nodes in this partition! Backward fake loss.
09:55:59.284226 [3] Warning: no training nodes in this partition! Backward fake loss.
09:55:59.766525 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:00.247286 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:00.726284 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:01.204370 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:01.683553 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:02.163249 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:02.665793 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:03.169865 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:03.671365 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:04.152796 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:04.633926 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:05.114187 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:05.594944 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:06.079022 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:06.558827 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:07.040156 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:07.522082 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:08.005456 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:08.488925 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:08.970477 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:09.451745 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:09.931930 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:10.412545 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:10.894205 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:11.376737 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:11.857967 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:12.340459 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:12.823139 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:13.303882 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:13.784883 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:14.264914 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:14.747570 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:15.227741 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:15.708237 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:16.190223 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:16.670064 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:17.150478 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:17.629611 [3] Warning: no training nodes in this partition! Backward fake loss.
09:56:52.868594 [3] proc begin: <DistEnv 3/4 nccl>
09:56:59.478100 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:56:59.494568 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:58:03.921101 [3] proc begin: <DistEnv 3/4 nccl>
09:58:10.870522 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:58:10.887213 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:00:01.521702 [3] proc begin: <DistEnv 3/4 nccl>
10:00:08.652772 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
10:00:08.669598 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:00:20.753678 [3] proc begin: <DistEnv 3/4 nccl>
10:00:26.866940 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
10:00:26.883704 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:02:52.205082 [3] proc begin: <DistEnv 3/4 nccl>
10:02:57.721999 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
10:02:57.739271 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:04:12.793983 [3] proc begin: <DistEnv 3/4 nccl>
10:04:18.632091 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
10:04:18.649517 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:25:34.432737 [3] proc begin: <DistEnv 3/4 nccl>
10:25:40.121460 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
10:25:40.141655 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:25:44.608149 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:46.182428 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:46.954884 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:47.727283 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:48.497234 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:49.266310 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:50.036051 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:50.806846 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:51.577860 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:52.350655 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:53.122129 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:53.891209 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:54.663314 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:55.434331 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:56.204368 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:56.974639 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:57.743901 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:58.512025 [3] Warning: no training nodes in this partition! Backward fake loss.
10:25:59.281829 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:00.051150 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:00.819298 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:01.588269 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:02.390978 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:03.189712 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:03.957682 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:04.725387 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:05.494279 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:06.262641 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:07.031891 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:07.801836 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:08.570789 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:09.338738 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:10.108057 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:10.876742 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:11.646148 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:12.414076 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:13.182807 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:13.952756 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:14.722672 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:15.494603 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:16.263666 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:17.034300 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:17.804380 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:18.574447 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:19.345261 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:20.114689 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:20.884295 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:21.654281 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:22.424631 [3] Warning: no training nodes in this partition! Backward fake loss.
10:26:23.195255 [3] Warning: no training nodes in this partition! Backward fake loss.
10:27:50.514655 [3] proc begin: <DistEnv 3/4 nccl>
10:27:55.910805 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
10:27:55.926233 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:28:01.540148 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:04.195938 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:06.071828 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:07.949674 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:09.826262 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:11.698716 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:13.567240 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:15.434088 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:17.299712 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:19.165162 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:21.029929 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:22.897298 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:24.761462 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:26.626031 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:28.490603 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:30.354155 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:32.219354 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:34.082761 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:35.947048 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:37.811937 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:39.676629 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:41.538783 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:43.405530 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:45.269392 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:47.134924 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:48.999286 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:50.863073 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:52.729201 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:54.593530 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:56.462610 [3] Warning: no training nodes in this partition! Backward fake loss.
10:28:58.328044 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:00.194455 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:02.090606 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:03.987786 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:05.851528 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:07.719790 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:09.588715 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:11.455866 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:13.322682 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:15.186483 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:17.049909 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:18.913151 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:20.777184 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:22.642148 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:24.506821 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:26.371713 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:28.236562 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:30.100732 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:31.963500 [3] Warning: no training nodes in this partition! Backward fake loss.
10:29:33.828468 [3] Warning: no training nodes in this partition! Backward fake loss.
10:31:28.903927 [3] proc begin: <DistEnv 3/4 nccl>
10:31:33.587853 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
10:31:33.605282 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:31:40.351965 [3] Warning: no training nodes in this partition! Backward fake loss.
10:31:45.282539 [3] Warning: no training nodes in this partition! Backward fake loss.
10:31:49.354937 [3] Warning: no training nodes in this partition! Backward fake loss.
10:31:53.424745 [3] Warning: no training nodes in this partition! Backward fake loss.
10:31:57.492107 [3] Warning: no training nodes in this partition! Backward fake loss.
10:32:01.557266 [3] Warning: no training nodes in this partition! Backward fake loss.
10:32:05.692157 [3] Warning: no training nodes in this partition! Backward fake loss.
10:32:09.761642 [3] Warning: no training nodes in this partition! Backward fake loss.
10:32:13.830857 [3] Warning: no training nodes in this partition! Backward fake loss.
10:32:17.901465 [3] Warning: no training nodes in this partition! Backward fake loss.
10:32:21.967613 [3] Warning: no training nodes in this partition! Backward fake loss.
10:32:26.039444 [3] Warning: no training nodes in this partition! Backward fake loss.
10:32:30.103934 [3] Warning: no training nodes in this partition! Backward fake loss.
10:32:34.169613 [3] Warning: no training nodes in this partition! Backward fake loss.
10:32:38.235768 [3] Warning: no training nodes in this partition! Backward fake loss.
10:32:42.303682 [3] Warning: no training nodes in this partition! Backward fake loss.
10:32:46.370664 [3] Warning: no training nodes in this partition! Backward fake loss.
10:32:50.437344 [3] Warning: no training nodes in this partition! Backward fake loss.
10:32:54.503975 [3] Warning: no training nodes in this partition! Backward fake loss.
10:32:58.571256 [3] Warning: no training nodes in this partition! Backward fake loss.
10:33:02.694861 [3] Warning: no training nodes in this partition! Backward fake loss.
10:33:06.773221 [3] Warning: no training nodes in this partition! Backward fake loss.
10:33:10.844517 [3] Warning: no training nodes in this partition! Backward fake loss.
10:33:14.914051 [3] Warning: no training nodes in this partition! Backward fake loss.
10:33:18.982562 [3] Warning: no training nodes in this partition! Backward fake loss.
10:33:23.049094 [3] Warning: no training nodes in this partition! Backward fake loss.
10:33:27.116564 [3] Warning: no training nodes in this partition! Backward fake loss.
10:33:31.182408 [3] Warning: no training nodes in this partition! Backward fake loss.
10:33:35.247870 [3] Warning: no training nodes in this partition! Backward fake loss.
10:33:39.313385 [3] Warning: no training nodes in this partition! Backward fake loss.
10:33:43.380242 [3] Warning: no training nodes in this partition! Backward fake loss.
10:33:47.453053 [3] Warning: no training nodes in this partition! Backward fake loss.
10:33:51.520705 [3] Warning: no training nodes in this partition! Backward fake loss.
10:33:55.586093 [3] Warning: no training nodes in this partition! Backward fake loss.
10:33:59.655396 [3] Warning: no training nodes in this partition! Backward fake loss.
10:34:03.806555 [3] Warning: no training nodes in this partition! Backward fake loss.
10:34:07.882721 [3] Warning: no training nodes in this partition! Backward fake loss.
10:34:11.954704 [3] Warning: no training nodes in this partition! Backward fake loss.
10:34:16.024778 [3] Warning: no training nodes in this partition! Backward fake loss.
10:34:20.092375 [3] Warning: no training nodes in this partition! Backward fake loss.
10:34:24.160455 [3] Warning: no training nodes in this partition! Backward fake loss.
10:34:28.227594 [3] Warning: no training nodes in this partition! Backward fake loss.
10:34:32.295351 [3] Warning: no training nodes in this partition! Backward fake loss.
10:34:36.361331 [3] Warning: no training nodes in this partition! Backward fake loss.
10:34:40.429981 [3] Warning: no training nodes in this partition! Backward fake loss.
10:34:44.500956 [3] Warning: no training nodes in this partition! Backward fake loss.
10:34:48.566412 [3] Warning: no training nodes in this partition! Backward fake loss.
10:34:52.633227 [3] Warning: no training nodes in this partition! Backward fake loss.
10:34:56.698386 [3] Warning: no training nodes in this partition! Backward fake loss.
10:35:00.763776 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:42.885249 [3] proc begin: <DistEnv 3/4 nccl>
10:51:47.765930 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
10:51:47.782792 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:52:49.353872 [3] proc begin: <DistEnv 3/4 nccl>
10:52:54.361953 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
10:52:54.381400 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:53:50.131306 [3] proc begin: <DistEnv 3/4 nccl>
10:53:55.447030 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
10:53:55.469076 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:54:42.847148 [3] proc begin: <DistEnv 3/4 nccl>
10:54:47.898381 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
10:54:47.919804 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:56:02.112590 [3] proc begin: <DistEnv 3/4 nccl>
10:56:08.130061 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
10:56:08.148927 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:56:13.131110 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:14.579098 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:15.289085 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:15.994012 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:16.701299 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:17.409965 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:18.116620 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:18.825092 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:19.530883 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:20.239894 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:20.946084 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:21.653391 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:22.360946 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:23.069213 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:23.774865 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:24.480244 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:25.185150 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:25.890206 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:26.596675 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:27.301124 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:28.007223 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:28.715369 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:29.422436 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:30.129043 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:30.835404 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:31.540751 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:32.249167 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:32.955402 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:33.661180 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:34.367918 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:35.076752 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:35.783657 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:36.491020 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:37.198410 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:37.906143 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:38.613543 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:39.321843 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:40.029133 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:40.735085 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:41.441520 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:42.148290 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:42.854640 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:43.559845 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:44.265344 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:44.971441 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:45.676776 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:46.381543 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:47.089884 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:47.798109 [3] Warning: no training nodes in this partition! Backward fake loss.
10:56:48.503486 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:01.207416 [3] proc begin: <DistEnv 3/4 nccl>
10:58:06.466731 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
10:58:06.485495 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:00:00.467922 [3] proc begin: <DistEnv 3/4 nccl>
11:00:05.314395 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
11:00:05.331849 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:00:56.955296 [3] proc begin: <DistEnv 3/4 nccl>
11:01:02.103545 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
11:01:02.128966 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:02:09.698745 [3] proc begin: <DistEnv 3/4 nccl>
11:02:14.807190 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
11:02:14.823891 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:08:18.125895 [3] proc begin: <DistEnv 3/4 nccl>
11:08:23.110176 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
11:08:23.135214 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:09:49.679233 [3] proc begin: <DistEnv 3/4 nccl>
11:10:07.430410 [3] proc begin: <DistEnv 3/4 nccl>
11:10:11.711043 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
11:10:11.727359 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:20:41.997904 [3] proc begin: <DistEnv 3/4 nccl>
11:20:46.841484 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
11:20:46.862754 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:21:08.537334 [3] proc begin: <DistEnv 3/4 nccl>
11:21:13.487756 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
11:21:13.507782 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:22:42.309347 [3] proc begin: <DistEnv 3/4 nccl>
11:22:46.840870 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
11:22:46.865963 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:23:04.974735 [3] proc begin: <DistEnv 3/4 nccl>
11:23:10.362651 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
11:23:10.380295 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:23:40.339018 [3] proc begin: <DistEnv 3/4 nccl>
11:23:44.577724 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
11:23:44.598221 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:23:51.490930 [3] Warning: no training nodes in this partition! Backward fake loss.
11:23:54.064172 [3] Warning: no training nodes in this partition! Backward fake loss.
11:23:55.885382 [3] Warning: no training nodes in this partition! Backward fake loss.
11:23:57.708148 [3] Warning: no training nodes in this partition! Backward fake loss.
11:23:59.526697 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:01.343258 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:03.226326 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:05.045942 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:06.865271 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:08.690365 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:10.514469 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:12.337038 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:14.158320 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:15.978917 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:17.798122 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:19.616308 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:21.438154 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:23.256777 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:25.078500 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:26.899301 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:28.717998 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:30.535695 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:32.354698 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:34.175332 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:35.996050 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:37.632126 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:39.268710 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:40.900510 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:42.534194 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:44.170234 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:45.803539 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:47.436231 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:49.070660 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:50.706973 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:52.340940 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:53.975886 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:55.611331 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:57.246623 [3] Warning: no training nodes in this partition! Backward fake loss.
11:24:58.884059 [3] Warning: no training nodes in this partition! Backward fake loss.
11:25:00.518678 [3] Warning: no training nodes in this partition! Backward fake loss.
11:25:02.156965 [3] Warning: no training nodes in this partition! Backward fake loss.
11:25:03.850066 [3] Warning: no training nodes in this partition! Backward fake loss.
11:25:05.489871 [3] Warning: no training nodes in this partition! Backward fake loss.
11:25:07.130980 [3] Warning: no training nodes in this partition! Backward fake loss.
11:25:08.775220 [3] Warning: no training nodes in this partition! Backward fake loss.
11:25:10.417822 [3] Warning: no training nodes in this partition! Backward fake loss.
11:25:12.061438 [3] Warning: no training nodes in this partition! Backward fake loss.
11:25:13.702475 [3] Warning: no training nodes in this partition! Backward fake loss.
11:25:15.338219 [3] Warning: no training nodes in this partition! Backward fake loss.
11:25:16.972685 [3] Warning: no training nodes in this partition! Backward fake loss.
14:25:06.564357 [3] proc begin: <DistEnv 3/4 nccl>
14:25:11.023924 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:25:11.052498 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:25:18.047621 [3] Warning: no training nodes in this partition! Backward fake loss.
14:25:47.553852 [3] proc begin: <DistEnv 3/4 nccl>
14:25:52.565396 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:25:52.584133 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:25:59.466622 [3] Warning: no training nodes in this partition! Backward fake loss.
14:26:28.161331 [3] proc begin: <DistEnv 3/4 nccl>
14:26:33.958462 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:26:33.975179 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:26:38.147685 [3] Warning: no training nodes in this partition! Backward fake loss.
14:26:39.572393 [3] Warning: no training nodes in this partition! Backward fake loss.
14:26:40.278304 [3] Warning: no training nodes in this partition! Backward fake loss.
14:26:40.986101 [3] Warning: no training nodes in this partition! Backward fake loss.
14:26:50.010386 [3] proc begin: <DistEnv 3/4 nccl>
14:26:55.223186 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:26:55.240779 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:27:00.717600 [3] Warning: no training nodes in this partition! Backward fake loss.
14:27:02.287487 [3] Warning: no training nodes in this partition! Backward fake loss.
14:27:03.027870 [3] Warning: no training nodes in this partition! Backward fake loss.
14:27:03.740091 [3] Warning: no training nodes in this partition! Backward fake loss.
14:27:04.449783 [3] Warning: no training nodes in this partition! Backward fake loss.
14:27:05.159906 [3] Warning: no training nodes in this partition! Backward fake loss.
14:27:05.869405 [3] Warning: no training nodes in this partition! Backward fake loss.
14:27:06.581209 [3] Warning: no training nodes in this partition! Backward fake loss.
14:27:07.291652 [3] Warning: no training nodes in this partition! Backward fake loss.
14:27:08.002077 [3] Warning: no training nodes in this partition! Backward fake loss.
14:27:08.713244 [3] Warning: no training nodes in this partition! Backward fake loss.
14:27:09.425063 [3] Warning: no training nodes in this partition! Backward fake loss.
14:27:17.557571 [3] proc begin: <DistEnv 3/4 nccl>
14:27:24.508213 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
14:27:24.524996 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:06:35.899957 [3] proc begin: <DistEnv 3/4 nccl>
16:06:41.866903 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:06:41.883626 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:09:53.490451 [3] proc begin: <DistEnv 3/4 nccl>
16:09:58.456173 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:09:58.473852 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:11:41.223416 [3] proc begin: <DistEnv 3/4 nccl>
16:11:46.840157 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:11:46.856653 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:13:23.116136 [3] proc begin: <DistEnv 3/4 nccl>
16:13:23.194668 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
16:13:23.207698 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:13:24.483360 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.238808 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.252694 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.260523 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.269003 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.281805 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.291080 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.299409 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.314889 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.322481 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.332937 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.344380 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.354660 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.362464 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.377703 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.390615 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.398748 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.406386 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.415267 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.423326 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.431085 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.438978 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.447378 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.455520 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.465993 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.474098 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.487447 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.497188 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.505977 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.515065 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.524676 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.533810 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.542885 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.555157 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.564156 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.573383 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.582590 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.594485 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.603186 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.612441 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.623342 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.633001 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.641998 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.652690 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.666375 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.677671 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.687423 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.695322 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.703365 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.711252 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.718913 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.725659 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.733784 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.740556 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.749514 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.757851 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.770561 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.778229 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.787674 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.795723 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.805507 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.813213 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.823499 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.831404 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.841221 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.851362 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.862501 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.870480 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.879908 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.887561 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.896943 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.904569 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.914654 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.922077 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.931742 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.939593 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.948804 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.956743 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.966288 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.975826 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.987334 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.996625 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.008280 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.016174 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.026477 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.033916 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.051194 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.062745 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.072349 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.080421 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.089926 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.097454 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.114243 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.123057 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.142821 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.154652 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.166127 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.173729 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.183814 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.191515 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.200982 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.208715 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.218151 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.225605 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.238105 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.247166 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.257665 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.265096 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.275306 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.284847 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.299163 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.309749 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.322127 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.330392 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.341243 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.353981 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.363475 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.371582 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.381825 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.389533 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.399214 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.406934 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.416661 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.424648 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.437930 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.447190 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.458478 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.467165 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.481010 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.491309 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.503379 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.512592 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.524054 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.533308 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.557766 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.569648 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.583208 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.591348 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.604290 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.612043 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.624792 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.633366 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.642523 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.650421 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.660301 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.668297 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.678421 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.686584 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.697366 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.706256 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.719705 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.728001 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.740763 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.748255 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.758636 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.766516 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.780112 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.801527 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.812426 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.820597 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.834882 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.843685 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.854298 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.863330 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.873354 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.881157 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.894395 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.903839 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.913977 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.922102 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.932532 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.941334 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.951212 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.959749 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.974816 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.982769 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.994286 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.003225 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.015901 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.024251 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.034465 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.043900 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.057336 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.064862 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.075688 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.084504 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.095539 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.103470 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.118314 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.125428 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.139667 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.156467 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.167164 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.173734 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.182196 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.188732 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.196366 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.203039 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.211565 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.218350 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:41.625975 [3] proc begin: <DistEnv 3/4 nccl>
16:13:41.709738 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
16:13:41.723720 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:13:43.187874 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:43.975936 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:43.999827 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.022003 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.041845 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.067452 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.087229 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.106877 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.124227 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.146951 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.167916 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.190307 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.214482 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.234452 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.258230 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.277635 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.303007 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.326160 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.347105 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.371522 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.394496 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.417645 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.441360 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.466037 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.486968 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.508020 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.531368 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.554264 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.582213 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.603875 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.621773 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.646534 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.667480 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.693992 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.720437 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.745280 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.770922 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.794176 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.818604 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.839239 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.858668 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.879008 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.899156 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.915715 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.938091 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.970380 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.994952 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.017470 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.043230 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.067119 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.091079 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.117480 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.145439 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.170591 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.189983 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.211661 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.234457 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.255482 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.278657 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.295759 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.315834 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.334263 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.352381 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.375241 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.391497 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.414646 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.435250 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.453903 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.476988 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.502563 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.525265 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.545629 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.581705 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.597977 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.629013 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.652562 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.672305 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.694852 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.714355 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.741129 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.762766 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.788736 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.811534 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.830920 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.853762 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.872286 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.896363 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.918237 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.947046 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.970559 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.992251 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.014094 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.042120 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.062573 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.089615 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.110057 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.134133 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.158656 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.181959 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.205337 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.226825 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.252857 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.272494 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.296030 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.322496 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.340837 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.363321 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.385896 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.405357 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.430651 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.454384 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.481503 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.510705 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.538092 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.563993 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.590318 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.616525 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.644429 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.667642 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.688137 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.711599 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.737421 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.762185 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.783326 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.806433 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.827730 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.850536 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.868889 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.891727 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.912593 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.933677 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.958060 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.980844 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.005898 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.025766 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.049561 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.074966 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.098066 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.122525 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.145878 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.171424 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.197935 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.222966 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.246936 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.268746 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.289976 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.312581 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.338434 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.360857 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.378196 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.399854 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.422471 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.446558 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.470629 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.495245 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.518557 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.541066 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.562478 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.593053 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.621480 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.649805 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.679326 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.710179 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.734001 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.754930 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.776776 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.797646 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.817309 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.838933 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.862328 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.881827 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.905262 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.927708 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.949754 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.970597 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.990317 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.010081 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.032301 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.053496 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.075676 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.098885 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.116247 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.138649 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.158106 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.178048 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.200846 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.225307 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.238502 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.250837 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.264118 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.276636 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.293933 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.309983 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.327751 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.350304 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.374244 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.398819 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.422560 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.446831 [3] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.470996 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:06.393204 [3] proc begin: <DistEnv 3/4 nccl>
16:14:06.444713 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
16:14:06.457599 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:14:07.990112 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:08.860372 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:08.900902 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:08.933532 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:08.969940 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.006522 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.039623 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.072635 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.105071 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.132111 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.161071 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.192837 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.227377 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.260456 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.287794 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.319702 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.352508 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.386260 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.421752 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.456264 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.488251 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.523370 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.561364 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.591398 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.624204 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.659909 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.691436 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.724767 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.755044 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.787345 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.820928 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.854474 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.885375 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.920041 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.951065 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.982257 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.016188 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.047881 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.083055 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.120013 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.156350 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.193165 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.224988 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.264107 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.300442 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.337136 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.371927 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.419279 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.461838 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.507737 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.549929 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.585490 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.623129 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.656635 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.692631 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.727169 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.754459 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.782014 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.810886 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.843289 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.876073 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.910630 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.941717 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.968068 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.994707 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.024361 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.056836 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.091898 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.123596 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.155085 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.192816 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.226604 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.258990 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.287274 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.320764 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.352117 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.382159 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.420466 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.454773 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.488574 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.526686 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.570926 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.603875 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.635886 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.668308 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.699826 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.743079 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.774449 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.807794 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.838766 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.872084 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.898366 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.931482 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.958512 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.982550 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.015680 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.047357 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.079130 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.113323 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.148930 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.190272 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.223856 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.255997 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.287899 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.324101 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.356209 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.392131 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.430809 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.463213 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.500302 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.543128 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.588309 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.621535 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.655081 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.683565 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.716463 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.747627 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.783512 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.820738 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.856826 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.894815 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.924908 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.962469 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.995606 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.032866 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.070233 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.105053 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.135903 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.172623 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.206831 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.237772 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.270741 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.304293 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.334826 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.364947 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.397276 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.435200 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.463037 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.494844 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.537330 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.580080 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.611237 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.645354 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.672139 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.703413 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.735659 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.767692 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.795464 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.827346 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.861510 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.895468 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.927782 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.954071 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.986497 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.016211 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.047902 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.079945 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.115663 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.153407 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.189742 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.227007 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.264412 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.301737 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.337318 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.375192 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.407704 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.446847 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.478462 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.513453 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.550969 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.597032 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.628080 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.656314 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.689359 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.726747 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.762656 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.795802 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.831238 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.863275 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.897947 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.930801 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.962591 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.996830 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.030159 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.066366 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.103716 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.139179 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.173407 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.209492 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.248792 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.286570 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.323010 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.357699 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.394146 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.430202 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.476964 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.512334 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.557173 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.603623 [3] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.635627 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:25.135649 [3] proc begin: <DistEnv 3/4 nccl>
16:15:29.737504 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
16:15:29.759282 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:15:34.559180 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:36.059325 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:36.769109 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:37.478305 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:38.186202 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:38.894367 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:39.603177 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:40.313330 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:41.022696 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:41.731148 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:42.438681 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:43.146904 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:43.855457 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:44.563129 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:45.271316 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:45.981262 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:46.689305 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:47.397533 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:48.105866 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:48.814801 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:49.522720 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:50.231703 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:50.939369 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:51.647965 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:52.355698 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:53.063521 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:53.770179 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:54.477646 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:55.185846 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:55.895870 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:56.604188 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:57.312012 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:58.020071 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:58.729646 [3] Warning: no training nodes in this partition! Backward fake loss.
16:15:59.438373 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:00.145967 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:00.853437 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:01.575028 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:02.313840 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:03.038316 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:03.747819 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:04.454369 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:05.161494 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:05.871284 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:06.580042 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:07.287990 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:07.995260 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:08.702618 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:09.411766 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:10.119522 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:10.827620 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:11.296900 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:12.054573 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:12.523483 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:13.281404 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:13.748354 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:14.505213 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:14.972445 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:15.730834 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:16.197934 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:16.957186 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:17.425040 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:18.184467 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:18.652802 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:19.410925 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:19.878944 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:20.638242 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:21.104844 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:21.861934 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:22.328979 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:23.086584 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:23.554056 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:24.311170 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:24.779418 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:25.537367 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:26.004004 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:26.762129 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:27.229024 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:27.986384 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:28.453596 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:29.211347 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:29.678933 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:30.437053 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:30.904192 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:31.662751 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:32.130639 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:32.891071 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:33.359739 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:34.118964 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:34.587055 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:35.344718 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:35.812839 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:36.569936 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:37.038388 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:37.795306 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:38.263069 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:39.022466 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:39.491525 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:40.249316 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:40.718796 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:41.477506 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:41.946262 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:42.705413 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:43.174501 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:43.931966 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:44.401308 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:45.159447 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:45.628143 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:46.384082 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:46.852763 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:47.611248 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:48.079229 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:48.837506 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:49.304415 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:50.062649 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:50.530437 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:51.288768 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:51.756753 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:52.517841 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:52.989441 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:53.749236 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:54.217067 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:54.973953 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:55.441581 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:56.199253 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:56.667104 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:57.425568 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:57.892598 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:58.650103 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:59.118303 [3] Warning: no training nodes in this partition! Backward fake loss.
16:16:59.875672 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:00.344235 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:01.101846 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:01.570083 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:02.347955 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:02.835434 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:03.612332 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:04.081963 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:04.838662 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:05.307898 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:06.064707 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:06.533275 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:07.291389 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:07.759972 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:08.517131 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:08.985319 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:09.743148 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:10.211045 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:10.967429 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:11.433926 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:12.191867 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:12.658659 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:13.414279 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:13.881328 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:14.638118 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:15.105905 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:15.862229 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:16.329505 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:17.086497 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:17.553462 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:18.309653 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:18.776433 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:19.531832 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:20.000201 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:20.756367 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:21.223128 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:21.979647 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:22.446904 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:23.203698 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:23.671668 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:24.428305 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:24.895059 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:25.651894 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:26.119019 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:26.875052 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:27.342435 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:28.097671 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:28.564782 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:29.321553 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:29.788164 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:30.544809 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:31.011957 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:31.769292 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:32.238350 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:32.994531 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:33.461313 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:34.218185 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:34.684914 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:35.441509 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:35.909021 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:36.665097 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:37.133075 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:37.889990 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:38.357017 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:39.112403 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:39.579224 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:40.335886 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:40.802973 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:41.558871 [3] Warning: no training nodes in this partition! Backward fake loss.
16:17:42.025650 [3] Warning: no training nodes in this partition! Backward fake loss.
09:20:39.643069 [3] proc begin: <DistEnv 3/4 nccl>
09:20:53.270804 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:20:53.298712 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:22:47.595899 [3] proc begin: <DistEnv 3/4 nccl>
09:22:54.631615 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:22:54.658179 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:25:19.018745 [3] proc begin: <DistEnv 3/4 nccl>
09:25:26.112399 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:25:26.147008 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:26:05.170572 [3] proc begin: <DistEnv 3/4 nccl>
09:26:11.677428 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:26:11.718168 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:26:53.540475 [3] proc begin: <DistEnv 3/4 nccl>
09:27:00.698519 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:27:00.728378 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:27:26.497762 [3] proc begin: <DistEnv 3/4 nccl>
09:27:33.137708 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:27:33.178949 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:13:20.437744 [3] proc begin: <DistEnv 3/4 nccl>
14:13:36.762659 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:13:36.823157 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:13:49.354495 [3] Warning: no training nodes in this partition! Backward fake loss.
14:14:53.235792 [3] proc begin: <DistEnv 3/4 nccl>
14:15:01.006479 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:15:01.009578 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  467254 KB |  486390 KB |  524678 KB |   57424 KB |
|       from large pool |  467254 KB |  486388 KB |  524655 KB |   57400 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| Active memory         |  467254 KB |  486390 KB |  524678 KB |   57424 KB |
|       from large pool |  467254 KB |  486388 KB |  524655 KB |   57400 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  507904 KB |  507904 KB |  507904 KB |       0 B  |
|       from large pool |  505856 KB |  505856 KB |  505856 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18121 KB |   21515 KB |   59053 KB |   40932 KB |
|       from large pool |   18121 KB |   19468 KB |   36513 KB |   18391 KB |
|       from small pool |       0 KB |    2047 KB |   22540 KB |   22540 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      53    |      36    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      53    |      36    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      24    |      14    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |      11    |      11    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:15:08.055109 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:10.030420 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:11.891025 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:13.747473 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:15.599698 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:17.457904 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:19.305493 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:21.157756 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:23.008834 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:24.861333 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:26.708555 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:28.559205 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:30.413422 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:32.267686 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:34.127525 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:35.974060 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:37.829569 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:39.684773 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:41.536914 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:43.389790 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:45.242642 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:47.092095 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:48.942616 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:50.796482 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:52.645920 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:54.497311 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:56.351662 [3] Warning: no training nodes in this partition! Backward fake loss.
14:15:58.211028 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:00.069264 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:01.936576 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:03.847595 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:05.700696 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:07.557808 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:09.414348 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:11.272004 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:13.128594 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:14.980559 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:16.831232 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:18.680778 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:20.535886 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:22.388730 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:24.237421 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:26.090572 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:27.944611 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:29.797405 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:31.647926 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:33.502093 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:35.356244 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:37.210872 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:39.064680 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:40.922305 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:42.594433 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:44.451124 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:46.119162 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:47.975687 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:49.645246 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:51.503936 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:53.172435 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:55.023144 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:56.682984 [3] Warning: no training nodes in this partition! Backward fake loss.
14:16:58.532246 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:00.198339 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:02.083694 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:03.789845 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:05.647455 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:07.322247 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:09.179576 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:10.850757 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:12.713316 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:14.389343 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:16.248058 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:17.919814 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:19.784051 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:21.459514 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:23.313952 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:24.981120 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:26.839488 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:28.512554 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:30.370304 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:32.038837 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:33.896224 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:35.567879 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:37.431339 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:39.101893 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:40.958085 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:42.627964 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:44.485618 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:46.158920 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:48.013471 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:49.680784 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:51.534936 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:53.206348 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:55.058588 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:56.731742 [3] Warning: no training nodes in this partition! Backward fake loss.
14:17:58.590936 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:00.263080 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:02.128002 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:03.857717 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:05.721337 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:07.392321 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:09.252058 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:10.927508 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:12.786082 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:14.453788 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:16.312608 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:17.982646 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:19.839134 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:21.513079 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:23.371980 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:25.038825 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:26.895292 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:28.564207 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:30.421513 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:32.092043 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:33.946359 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:35.620057 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:37.477006 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:39.149253 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:41.007326 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:42.677684 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:44.536453 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:46.204331 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:48.065809 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:49.732745 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:51.586100 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:53.251191 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:55.108440 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:56.777860 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:58.641591 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:00.319743 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:02.202930 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:03.920269 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:05.778027 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:07.454833 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:09.314175 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:10.984188 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:12.839354 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:14.518031 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:16.378501 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:18.053399 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:19.910906 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:21.585611 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:23.443999 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:25.114258 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:26.972354 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:28.642816 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:30.496506 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:32.168375 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:34.025386 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:35.694465 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:37.550408 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:39.217379 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:41.073759 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:42.747047 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:44.609002 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:46.278526 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:48.143046 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:49.810330 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:51.664986 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:53.331611 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:55.185406 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:56.855271 [3] Warning: no training nodes in this partition! Backward fake loss.
14:19:58.710120 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:00.386080 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:02.290517 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:03.982393 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:05.843104 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:07.517380 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:09.377843 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:11.051290 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:12.907276 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:14.578590 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:16.437014 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:18.108421 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:19.964245 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:21.629541 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:23.481976 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:25.147510 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:27.003429 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:28.672055 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:30.526781 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:32.194074 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:34.051310 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:35.720187 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:37.581835 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:39.250870 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:41.105070 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:42.774829 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:44.630022 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:46.299897 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:48.154624 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:49.821754 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:51.674557 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:53.351545 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:55.210857 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:56.882054 [3] Warning: no training nodes in this partition! Backward fake loss.
14:20:58.741182 [3] Warning: no training nodes in this partition! Backward fake loss.
14:21:00.420185 [3] Warning: no training nodes in this partition! Backward fake loss.
14:21:02.298033 [3] Warning: no training nodes in this partition! Backward fake loss.
14:21:04.024279 [3] Warning: no training nodes in this partition! Backward fake loss.
14:22:14.883226 [3] proc begin: <DistEnv 3/4 nccl>
14:22:19.112871 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:22:19.130003 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:22:24.464540 [3] Warning: no training nodes in this partition! Backward fake loss.
14:22:27.008682 [3] Warning: no training nodes in this partition! Backward fake loss.
14:22:28.837679 [3] Warning: no training nodes in this partition! Backward fake loss.
14:22:30.671298 [3] Warning: no training nodes in this partition! Backward fake loss.
14:22:32.502574 [3] Warning: no training nodes in this partition! Backward fake loss.
14:25:50.280448 [3] proc begin: <DistEnv 3/4 nccl>
14:25:54.770657 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:25:54.796485 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:26:01.847637 [3] Warning: no training nodes in this partition! Backward fake loss.
14:26:25.642598 [3] proc begin: <DistEnv 3/4 nccl>
14:26:32.213647 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:26:32.217319 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  467254 KB |  486390 KB |  524678 KB |   57424 KB |
|       from large pool |  467254 KB |  486388 KB |  524655 KB |   57400 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| Active memory         |  467254 KB |  486390 KB |  524678 KB |   57424 KB |
|       from large pool |  467254 KB |  486388 KB |  524655 KB |   57400 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  507904 KB |  507904 KB |  507904 KB |       0 B  |
|       from large pool |  505856 KB |  505856 KB |  505856 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18121 KB |   21515 KB |   59053 KB |   40932 KB |
|       from large pool |   18121 KB |   19468 KB |   36513 KB |   18391 KB |
|       from small pool |       0 KB |    2047 KB |   22540 KB |   22540 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      53    |      36    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      53    |      36    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      24    |      14    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |      11    |      11    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:00:25.432951 [3] proc begin: <DistEnv 3/4 nccl>
15:00:25.612449 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
15:00:25.622013 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:00:26.818897 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.561144 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.572558 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.589423 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.600512 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.608995 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.616304 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.624214 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.631597 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.643373 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.651621 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.659226 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.666514 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.673975 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.681469 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.691073 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.699082 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.707189 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.714322 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.721302 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.730506 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.737888 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.747607 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.756174 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.766273 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.774527 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.783938 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.791281 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.798687 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.806295 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.814325 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.822420 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.831201 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.841556 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.850737 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.858544 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.870616 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.877978 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.885509 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.892426 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.900279 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.908288 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.915695 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.922658 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.930192 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.937852 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.946351 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.953875 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.961166 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.968033 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:56.945806 [3] proc begin: <DistEnv 3/4 nccl>
15:01:57.012244 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
15:01:57.021752 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:01:58.196901 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.925614 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.936121 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.943637 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.951483 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.959031 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.966244 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.973076 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.980416 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.988850 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.996341 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.004031 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.011709 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.019435 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.029886 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.038845 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.049091 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.058501 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.066214 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.079150 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.087091 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.094496 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.102077 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.109556 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.116406 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.125819 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.135639 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.146240 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.155946 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.163484 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.171232 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.178367 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.185379 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.192720 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.200212 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.207196 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.214220 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.221472 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.228613 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.237004 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.244299 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.251861 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.259010 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.266678 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.274283 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.280992 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.288367 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.296171 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.303388 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.310854 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:49.362794 [3] proc begin: <DistEnv 3/4 nccl>
15:02:49.429403 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
15:02:49.442258 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:02:50.738987 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.470418 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.494050 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.515515 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.537647 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.557389 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.583257 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.606948 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.625763 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.646889 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.665073 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.686414 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.708565 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.728555 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.748712 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.762420 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.781639 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.797918 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.819404 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.840415 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.858456 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.876178 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.898469 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.915360 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.930132 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.939996 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.951139 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.966885 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.984334 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.005984 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.024467 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.045847 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.065853 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.085583 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.105477 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.126248 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.148929 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.168302 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.182979 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.193337 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.204988 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.227205 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.243162 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.255453 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.266123 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.279163 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.290620 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.304595 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.316626 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.328153 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:09.384313 [3] proc begin: <DistEnv 3/4 nccl>
21:53:21.287564 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
21:53:21.347210 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:06:22.621336 [3] proc begin: <DistEnv 3/4 nccl>
22:06:45.561415 [3] proc begin: <DistEnv 3/4 nccl>
22:06:51.757766 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
22:06:51.774596 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:08:02.681806 [3] proc begin: <DistEnv 3/4 nccl>
22:08:08.689932 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
22:08:08.706819 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:09:31.149038 [3] proc begin: <DistEnv 3/4 nccl>
22:09:37.266890 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
22:09:37.289098 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:09:52.773512 [3] proc begin: <DistEnv 3/4 nccl>
22:09:58.227491 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
22:09:58.242722 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:10:56.375157 [3] proc begin: <DistEnv 3/4 nccl>
22:11:03.735702 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
22:11:03.749665 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:12:18.051150 [3] proc begin: <DistEnv 3/4 nccl>
22:12:22.579128 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
22:12:22.593206 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:12:59.117433 [3] proc begin: <DistEnv 3/4 nccl>
22:13:04.564733 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
22:13:04.586152 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:51:33.334954 [3] proc begin: <DistEnv 3/4 nccl>
14:51:33.465310 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
14:51:33.474584 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:52:07.198493 [3] proc begin: <DistEnv 3/4 nccl>
14:52:07.297255 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
14:52:07.306584 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:52:48.789510 [3] proc begin: <DistEnv 3/4 nccl>
14:52:48.844555 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
14:52:48.856776 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:53:06.944154 [3] proc begin: <DistEnv 3/4 nccl>
14:53:06.969900 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
14:53:06.979103 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:53:56.152701 [3] proc begin: <DistEnv 3/4 nccl>
14:53:56.206295 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
14:53:56.215804 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:55:22.111726 [3] proc begin: <DistEnv 3/4 nccl>
14:55:22.168066 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
14:55:22.177472 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:55:23.406284 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.254678 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.301048 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.359805 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.393100 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.403050 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.415201 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.423834 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.430739 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.438623 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.445605 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.451820 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.459734 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.467260 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.473905 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.481209 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.488493 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.495068 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.502705 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.511053 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.519065 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.527299 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.635092 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.716421 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.832450 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.912956 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.942061 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.987356 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.030115 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.047814 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.113617 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.155620 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.178152 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.239930 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.284737 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.304582 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.371459 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.414999 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.433553 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.498924 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.533284 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.560285 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.626139 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.667600 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.688254 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.751060 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.787704 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.816232 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.824998 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.836087 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.846110 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.854623 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.862321 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.868450 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.876347 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.886246 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.892390 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.899746 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.912196 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.919579 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.927418 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.934734 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.940896 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.952142 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.961708 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.969605 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.980604 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.991856 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.997934 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.006166 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.013400 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.019752 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.028165 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.035866 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.044712 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.052392 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.059616 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.065935 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.073504 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.148323 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.237025 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.417034 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.560748 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.660649 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.878156 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.061261 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.169737 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.392903 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.584751 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.747894 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.927839 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.086298 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.227218 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.391274 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.534281 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.668243 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.809172 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.940795 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.063250 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.195388 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.228704 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.260885 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.312796 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.373657 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.405625 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.470579 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.519147 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.551345 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.615923 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.647518 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.696788 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.762369 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.810409 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.842565 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.904917 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.952052 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.984807 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.047194 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.095522 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.127615 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.180374 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.240134 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.268121 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.332530 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.381031 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.411696 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.476234 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.523526 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.554956 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.620681 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.667787 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.697751 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.762836 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.811478 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.839486 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.905637 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.952680 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.980987 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.045618 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.095449 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.125519 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.188057 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.236683 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.266977 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.332178 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.366772 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.375975 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.394915 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.404517 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.410676 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.418509 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.425132 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.431263 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.438847 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.446833 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.453418 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.461342 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.469187 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.475650 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.483372 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.491112 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.500881 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.512776 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.522168 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.528407 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.536409 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.543936 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.551611 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.559403 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.567051 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.573500 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.583705 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.593457 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.601260 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.609384 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.617484 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.624825 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.632700 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.640512 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.647419 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.655144 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.662720 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.669776 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.677585 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.685393 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.692795 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.701906 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.709590 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.716411 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.726643 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.734840 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.741119 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.750450 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.760701 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.770927 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.828106 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.967833 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:32.044112 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:32.154372 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:32.198248 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:15.952463 [3] proc begin: <DistEnv 3/4 nccl>
21:50:40.459768 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
21:50:40.476653 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:50:51.658388 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:53.167378 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:53.638999 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:54.403744 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:55.115556 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:55.587133 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:56.349862 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:57.061246 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:57.535145 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:58.301189 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:59.015683 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:59.489845 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:00.254285 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:00.967148 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:01.442032 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:02.228299 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:02.970476 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:03.453058 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:04.219376 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:04.931733 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:05.406583 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:06.171281 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:06.884644 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:07.359190 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:08.123878 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:08.835407 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:09.307707 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:10.072883 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:10.785113 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:11.258643 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:12.024703 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:12.737211 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:13.210856 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:13.975596 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:14.689105 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:15.162609 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:15.926820 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:16.639554 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:17.112727 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:17.922840 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:18.705800 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:19.216692 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:20.041574 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:20.808706 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:21.281242 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:22.048320 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:22.937382 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:23.759519 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:25.047425 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:26.247239 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:26.969174 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:27.837565 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:28.594951 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:29.088986 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:29.912458 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:30.669563 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:31.167592 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:31.973532 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:32.737996 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:33.237397 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:34.039456 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:34.800596 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:35.300764 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:36.098012 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:36.854557 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:37.354520 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:38.151215 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:38.909170 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:39.409632 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:40.210294 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:40.965816 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:41.465922 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:42.253806 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:43.004093 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:43.504641 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:44.290214 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:45.038603 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:45.537327 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:46.323637 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:47.074454 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:47.574004 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:48.360172 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:49.108443 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:49.607134 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:50.393673 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:51.144136 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:51.643792 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:52.429530 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:53.180229 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:53.677864 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:54.458314 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:55.197780 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:55.702444 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:56.503177 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:57.231211 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:57.729893 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:58.530886 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:59.272029 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:59.761314 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:00.575839 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:01.332196 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:01.842451 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:02.688088 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:03.455345 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:03.957436 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:04.745334 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:05.497293 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:05.998353 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:06.792022 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:07.547813 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:08.047048 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:08.834772 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:09.586116 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:10.086568 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:10.875731 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:11.625961 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:12.126556 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:12.915748 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:13.667465 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:14.168506 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:14.954637 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:15.704202 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:16.203739 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:16.990728 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:17.741286 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:18.242861 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:19.029384 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:19.780598 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:20.282432 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:21.068470 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:21.820181 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:22.320800 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:23.105882 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:23.855804 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:24.355950 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:25.144402 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:25.894651 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:26.393989 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:27.180395 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:27.930471 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:28.431850 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:29.219301 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:29.971200 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:30.471050 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:31.256440 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:32.004429 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:32.504653 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:33.290721 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:34.040096 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:34.539264 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:35.320231 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:36.071711 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:36.572204 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:37.359026 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:38.108444 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:38.608934 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:39.395050 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:40.145921 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:40.644220 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:41.430950 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:42.184732 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:42.686627 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:43.469125 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:44.217855 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:44.715066 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:45.501076 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:46.251360 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:46.750579 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:47.536508 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:48.285392 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:48.785846 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:49.573048 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:50.323232 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:50.823186 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:51.610220 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:52.362793 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:52.865236 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:53.651574 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:54.402336 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:54.902792 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:55.693116 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:56.442193 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:56.939809 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:57.721919 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:58.473572 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:58.971188 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:59.756535 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:00.507625 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:01.006997 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:01.794496 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:02.564898 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:03.085269 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:03.900911 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:04.632532 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:05.138556 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:05.939377 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:06.671146 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:07.178106 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:07.978983 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:08.710587 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:59.113977 [3] proc begin: <DistEnv 3/4 nccl>
21:55:59.228292 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
21:55:59.240856 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:56:00.651867 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.413001 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.424780 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.435634 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.443891 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.450298 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.458070 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.467368 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.475702 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.483922 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.491563 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.497923 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.505834 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.514225 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.520430 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.528772 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.536500 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.543325 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.551894 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.559153 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.565478 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.573197 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.580676 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.587314 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.595364 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.602996 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.609599 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.617873 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.625236 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.631446 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.639896 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.647886 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.654542 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.665401 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.676908 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.685779 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.694681 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.703754 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.712723 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.720421 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.732194 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.739252 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.747513 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.761482 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.768255 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.779186 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.789738 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.798576 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.807374 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.815168 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.823615 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.831095 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.839841 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.846730 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.856940 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.868544 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.875879 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.885441 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.893680 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.901491 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.910366 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.918799 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.927455 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.937657 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.946329 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.954203 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.964738 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.974490 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.983654 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.995493 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.007581 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.015898 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.026608 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.034723 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.044925 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.053503 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.062325 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.070742 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.079143 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.091277 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.098668 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.110729 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.122008 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.130515 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.139548 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.147619 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.155405 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.166189 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.175497 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.182805 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.192694 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.203028 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.210310 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.219774 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.228020 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.236164 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.244427 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.252866 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.260108 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.269441 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.277361 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.286510 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.294195 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.302611 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.310434 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.320045 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.328646 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.336930 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.346215 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.355193 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.363683 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.371706 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.379698 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.386984 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.395543 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.404289 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.413456 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.422660 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.435799 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.444142 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.453913 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.462744 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.469586 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.479091 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.486532 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.493416 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.502870 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.511520 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.519737 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.528022 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.537837 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.545511 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.554620 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.562716 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.569942 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.579595 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.588222 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.596189 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.606750 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.614821 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.622992 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.633070 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.641787 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.652559 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.661395 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.669838 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.676495 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.685888 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.695101 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.702614 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.711417 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.719098 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.727183 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.735987 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.747155 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.754856 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.764630 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.774276 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.782358 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.790986 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.799240 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.807121 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.815315 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.823535 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.831827 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.841875 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.854264 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.862018 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.870381 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.880547 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.887986 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.896205 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.906596 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.914223 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.924875 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.934133 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.941454 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.950767 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.960017 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.967445 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.979889 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.988959 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.997549 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.006795 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.014043 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.021185 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.030979 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.040705 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.051607 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.061569 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.070154 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.076991 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.087007 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.095581 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.103904 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.115226 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.128067 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.135259 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.144139 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.151711 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:19.519487 [3] proc begin: <DistEnv 3/4 nccl>
21:58:34.199885 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
21:58:34.217949 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:49:07.081806 [3] proc begin: <DistEnv 3/4 nccl>
22:49:11.968982 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
22:49:11.981205 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3856 KB |    3878 KB |    3929 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      66 KB |      88 KB |     139 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3856 KB |    3878 KB |    3929 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      66 KB |      88 KB |     139 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18671 KB |   18707 KB |   18808 KB |  139776 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1981 KB |    2045 KB |    2118 KB |  139776 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:49:54.822566 [3] proc begin: <DistEnv 3/4 nccl>
22:49:56.297694 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
22:49:56.298755 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3856 KB |    3878 KB |    3929 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      66 KB |      88 KB |     139 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3856 KB |    3878 KB |    3929 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      66 KB |      88 KB |     139 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18671 KB |   18707 KB |   18808 KB |  139776 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1981 KB |    2045 KB |    2118 KB |  139776 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:50:02.039029 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.376120 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.410665 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.439541 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.470617 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.502596 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.534405 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.577850 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.607586 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.638160 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.666991 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.697321 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.731103 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.765279 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.802777 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.837279 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.871987 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.908907 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.942344 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.976808 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.011813 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.045231 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.076742 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.112079 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.143564 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.182024 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.217569 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.245692 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.283607 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.310707 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.345362 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.377559 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.414112 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.447095 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.479889 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.514411 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.546414 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.582910 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.620606 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.650237 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.681567 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.715555 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.747103 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.777836 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.816023 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.850529 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.888267 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.922789 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.952897 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.985485 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.018756 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.055358 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.085480 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.118091 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.152137 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.181633 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.212299 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.245134 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.277442 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.308094 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.343176 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.372674 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.405028 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.430922 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.465541 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.496361 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.531221 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.563223 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.596072 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.629032 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.661809 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.693399 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.727402 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.757790 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.787815 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.825538 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.856370 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.888892 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.920891 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.954289 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.984751 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.021421 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.053604 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.086162 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.119973 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.150469 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.181489 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.213633 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.249042 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.281946 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.315250 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.347560 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.377378 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.408982 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.442332 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.476714 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.508455 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.535460 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.569122 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.604456 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.634708 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.668437 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.698072 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.729312 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.760889 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.792762 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.826743 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.856556 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.893719 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.925220 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.960940 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.995993 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.032496 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.063099 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.095581 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.129563 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.162696 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.192347 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.229890 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.262291 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.293013 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.325491 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.355629 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.387703 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.417981 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.449481 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.482912 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.516026 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.544671 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.576894 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.609767 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.640155 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.669123 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.704998 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.737566 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.769051 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.804945 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.840097 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.873108 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.910328 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.940702 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.968308 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.005670 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.039378 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.066176 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.099109 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.131916 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.166472 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.199727 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.237514 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.268583 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.299699 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.329444 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.360907 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.390364 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.420534 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.453886 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.484437 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.514745 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.548814 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.582151 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.614956 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.648351 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.681311 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.712997 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.746080 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.776643 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.806693 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.839880 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.874584 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.908689 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.942132 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.978983 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.008274 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.042042 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.073130 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.105999 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.140032 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.172466 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.204185 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.235402 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.266679 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.297709 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.333103 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.368588 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.401252 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.430668 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.468710 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.500353 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.533323 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.565587 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.604318 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.637539 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.671202 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.702939 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.735990 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.766302 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.797890 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.830458 [3] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.862217 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:24.931452 [3] proc begin: <DistEnv 3/4 nccl>
10:38:26.650596 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
10:38:26.651806 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3856 KB |    3878 KB |    3929 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      66 KB |      88 KB |     139 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3856 KB |    3878 KB |    3929 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      66 KB |      88 KB |     139 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18671 KB |   18707 KB |   18808 KB |  139776 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1981 KB |    2045 KB |    2118 KB |  139776 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:38:28.162667 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.298852 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.331650 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.368183 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.398474 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.433084 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.464477 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.491841 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.523305 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.558788 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.587657 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.620348 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.653668 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.697122 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.730872 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.760572 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.804468 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.836701 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.871807 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.911821 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.941994 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.976739 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.022731 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.138467 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.175391 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.211286 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.245929 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.284237 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.320394 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.355780 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.390895 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.425877 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.461749 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.499760 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.537527 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.952812 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.984949 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.017330 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.055041 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.089403 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.122566 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.164119 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.197032 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.226402 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.258448 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.296879 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.331140 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.361647 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.391956 [3] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.428300 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:23.786223 [3] proc begin: <DistEnv 3/4 nccl>
10:48:23.896732 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
10:48:23.906266 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:48:25.218677 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.887778 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.899384 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.906333 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.913068 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.919432 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.926820 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.935863 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.944495 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.953056 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.961226 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.969596 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.979852 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.988303 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.997193 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.005793 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.013069 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.023920 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.037332 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.049653 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.061526 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.068811 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.079946 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.088115 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.098279 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.105759 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.112276 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.119506 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.126961 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.135372 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.144848 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.154059 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.160909 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.168231 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.175511 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.182625 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.189875 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.197826 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.204842 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.211751 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.219824 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.228701 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.235371 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.242881 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.249780 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.256053 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.264076 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.273755 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.282774 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.290933 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.301040 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.310016 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.317311 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.324336 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.332104 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.342065 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.348967 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.358636 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.366054 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.372643 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.379634 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.389604 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.397537 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.404975 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.414083 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.421246 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.428429 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.435820 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.442951 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.453221 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.459975 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.471951 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.485246 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.496275 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.506087 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.517846 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.528922 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.537047 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.545749 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.552710 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.560067 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.567593 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.575430 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.586763 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.595142 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.602758 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.610134 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.617429 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.624602 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.634291 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.641576 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.649088 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.656550 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.663628 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.675633 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.684848 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.692041 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.701598 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.708506 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.715512 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.723097 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.731574 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.741518 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.748953 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.755265 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.762819 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.778719 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.788104 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.804562 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.811860 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.819262 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.828082 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.846520 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.856052 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.866239 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.873450 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.884347 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.894960 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.905969 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.915818 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.924991 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.931625 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.938470 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.946735 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.955477 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.962874 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.969354 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.976073 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.983089 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.990303 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.003733 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.014306 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.023299 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.030771 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.037868 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.044395 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.052158 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.061292 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.071451 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.078025 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.085592 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.099082 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.107759 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.116939 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.125706 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.133575 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.141626 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.149214 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.159547 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.168136 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.179934 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.189293 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.207891 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.220700 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.228356 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.235417 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.242512 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.252730 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.260296 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.267538 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.275265 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.282715 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.290445 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.297732 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.304992 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.312518 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.320432 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.328517 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.335731 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.346518 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.357377 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.364586 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.375094 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.385097 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.394210 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.401125 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.410692 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.417732 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.424967 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.433273 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.442416 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.450557 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.457457 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.466674 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.474033 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.481181 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.489285 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.496579 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.506243 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.513179 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.520132 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.528154 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.537409 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.544539 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.551919 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.558939 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.572273 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.584427 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.590956 [3] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.598191 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:08.189186 [3] proc begin: <DistEnv 3/4 nccl>
10:51:08.223536 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
10:51:08.235826 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:51:10.700228 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.412479 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.435049 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.455135 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.472923 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.494306 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.512513 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.533562 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.557120 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.578528 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.599506 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.616793 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.631013 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.641774 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.657990 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.672515 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.690912 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.709204 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.718694 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.729635 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.740353 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.753001 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.763383 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.775703 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.786259 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.802110 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.822738 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.839559 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.860251 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.878090 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.897879 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.913746 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.932734 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.950893 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.962422 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.977315 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.991026 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.000724 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.015192 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.025958 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.035712 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.046579 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.057196 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.075977 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.089278 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.099872 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.109992 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.124569 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.142027 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.162219 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.180636 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.207378 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.227231 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.242724 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.258763 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.272115 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.290382 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.308359 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.329997 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.346741 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.366486 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.384294 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.403329 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.419487 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.432312 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.446603 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.462054 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.477013 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.497058 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.514419 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.534880 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.552303 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.573048 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.592522 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.611175 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.626623 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.638292 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.649718 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.659560 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.676653 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.698529 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.715732 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.737841 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.755173 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.768518 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.789345 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.808911 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.827611 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.847351 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.864592 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.885552 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.914492 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.932493 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.953122 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.972683 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.991037 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.007724 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.020640 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.030514 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.042701 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.053065 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.064194 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.096036 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.112050 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.126756 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.136854 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.163143 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.179551 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.204514 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.219064 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.229471 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.241184 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.251633 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.262636 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.272778 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.285581 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.303684 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.322649 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.342406 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.359704 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.381698 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.402020 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.423426 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.438511 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.458471 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.475452 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.494641 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.512923 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.533651 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.553921 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.572395 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.593499 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.610674 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.630649 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.650561 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.670582 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.688071 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.706889 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.725065 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.746000 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.764616 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.783743 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.798653 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.809442 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.820133 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.832541 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.846250 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.854900 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.864848 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.878237 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.891585 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.912036 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.934480 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.954814 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.972359 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.992518 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.011291 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.031538 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.048289 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.070239 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.093744 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.108992 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.118588 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.129092 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.141274 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.162169 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.180637 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.211872 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.228185 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.240312 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.250245 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.259869 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.270589 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.279971 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.289509 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.302771 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.317077 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.328071 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.338736 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.347876 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.357219 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.366254 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.386190 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.404593 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.425573 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.445122 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.465648 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.486020 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.504247 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.525623 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.544086 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.564897 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.574576 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.589654 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.604346 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.616512 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.626814 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.637677 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.648005 [3] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.658031 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:32.942039 [3] proc begin: <DistEnv 3/4 nccl>
10:52:33.014259 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
10:52:33.023465 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:52:34.284242 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.165126 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.194867 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.220936 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.258061 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.286771 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.318234 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.334993 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.363412 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.390974 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.417435 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.445214 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.472420 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.493705 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.513409 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.541360 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.560660 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.584215 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.608937 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.638615 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.666679 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.693922 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.723839 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.750524 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.778791 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.805651 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.833967 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.862313 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.889261 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.917262 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.948391 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.981674 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.012331 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.039831 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.067021 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.092661 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.117079 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.145456 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.171641 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.197406 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.223505 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.248894 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.272157 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.297732 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.321726 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.349099 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.375527 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.405942 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.432469 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.458301 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.480542 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.506123 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.529637 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.556339 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.582972 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.613009 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.640650 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.666546 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.691886 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.722297 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.750621 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.776718 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.805244 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.832809 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.861208 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.889759 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.923091 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.940267 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.981204 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.999818 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.022142 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.048780 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.073212 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.102691 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.128288 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.149825 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.171753 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.196863 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.218616 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.244964 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.271187 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.297947 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.325964 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.353842 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.378625 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.410196 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.432894 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.462040 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.489858 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.514115 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.541465 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.566918 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.593851 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.617826 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.645572 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.673053 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.693726 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.724313 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.750021 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.775801 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.802147 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.830863 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.858236 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.886367 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.912522 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.942886 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.987258 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.011430 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.038892 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.068791 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.098121 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.124934 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.154972 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.178196 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.204611 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.226928 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.252827 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.280620 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.306587 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.333149 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.361536 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.389816 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.416442 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.439535 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.466977 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.492934 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.518028 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.544116 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.569120 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.598823 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.626238 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.653949 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.679714 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.706071 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.734538 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.758718 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.786802 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.812507 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.838995 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.854047 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.872600 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.889057 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.910297 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.943153 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.983551 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.002244 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.022322 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.042981 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.068564 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.094656 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.120371 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.148562 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.172230 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.196571 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.222315 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.249205 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.273584 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.301070 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.328916 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.356939 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.385069 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.414897 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.438998 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.469389 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.495439 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.521202 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.548137 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.571184 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.598049 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.625377 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.655785 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.683366 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.711566 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.736818 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.762850 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.789024 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.821526 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.842423 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.864276 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.893673 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.923513 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.947821 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.990479 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.009377 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.030785 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.056014 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.083426 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.112750 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.141441 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.169516 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.196852 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.224654 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.245766 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.270048 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.302864 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.320233 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.341100 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.368700 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.392194 [3] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.417099 [3] Warning: no training nodes in this partition! Backward fake loss.
10:53:37.240618 [3] proc begin: <DistEnv 3/4 nccl>
10:53:54.193407 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
10:53:54.209964 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:55:30.944116 [3] proc begin: <DistEnv 3/4 nccl>
10:55:37.517501 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
10:55:37.536855 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:56:23.460179 [3] proc begin: <DistEnv 3/4 nccl>
10:56:28.009773 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
10:56:28.027188 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:58:16.886090 [3] proc begin: <DistEnv 3/4 nccl>
10:58:37.117923 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
10:58:37.138862 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:58:40.275907 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:41.212054 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:41.490178 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:41.766551 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:42.044012 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:42.319743 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:42.595564 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:42.871862 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:43.149480 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:43.426861 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:43.702887 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:43.978671 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:44.256941 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:44.533263 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:44.813087 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:45.088488 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:45.364268 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:45.640002 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:45.915904 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:46.192698 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:46.469209 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:46.746063 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:47.023177 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:47.300533 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:47.578790 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:47.855347 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:48.132415 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:48.409535 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:48.687535 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:48.965185 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:49.241646 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:49.519911 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:49.797941 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:50.078023 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:50.357553 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:50.634811 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:50.911795 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:51.188413 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:51.464802 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:51.741846 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:52.017700 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:52.294124 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:52.570060 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:52.846171 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:53.122295 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:53.399206 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:53.674783 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:53.951587 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:54.228245 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:54.505124 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:54.781375 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:55.058721 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:55.335392 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:55.612475 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:55.889298 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:56.166544 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:56.444551 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:56.719946 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:56.997490 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:57.273430 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:57.549723 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:57.825876 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:58.103651 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:58.380402 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:58.658084 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:58.934990 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:59.210868 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:59.488575 [3] Warning: no training nodes in this partition! Backward fake loss.
10:58:59.764597 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:00.040696 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:00.317584 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:00.595270 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:00.871844 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:01.148094 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:01.424445 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:01.700441 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:01.978178 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:02.265938 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:02.555113 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:02.842938 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:03.131194 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:03.420927 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:03.703853 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:03.980972 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:04.257520 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:04.534008 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:04.810827 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:05.087916 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:05.365723 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:05.643592 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:05.920322 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:06.197443 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:06.474155 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:06.750670 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:07.028671 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:07.305972 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:07.582427 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:07.859311 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:08.136373 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:08.413032 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:08.690166 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:08.967922 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:09.244148 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:09.520928 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:09.798971 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:10.075746 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:10.352427 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:10.628853 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:10.906371 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:11.182607 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:11.459337 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:11.735965 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:12.012551 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:12.289785 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:12.566177 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:12.842225 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:13.119993 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:13.397012 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:13.673404 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:13.950496 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:14.226798 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:14.503763 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:14.781528 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:15.059283 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:15.335328 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:15.611987 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:15.888784 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:16.165005 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:16.441616 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:16.717997 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:16.994585 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:17.271208 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:17.548683 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:17.824673 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:18.100985 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:18.377411 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:18.653440 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:18.929367 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:19.205937 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:19.482513 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:19.759466 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:20.035370 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:20.312827 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:20.589499 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:20.866101 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:21.142488 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:21.418410 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:21.694426 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:21.970846 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:22.246609 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:22.523964 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:22.800269 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:23.076187 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:23.353003 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:23.628762 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:23.905179 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:24.181640 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:24.457740 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:24.733764 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:25.009799 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:25.286050 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:25.562008 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:25.838343 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:26.115157 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:26.391885 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:26.667673 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:26.943865 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:27.220209 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:27.496645 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:27.773344 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:28.050210 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:28.328105 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:28.605671 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:28.882464 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:29.158779 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:29.436328 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:29.713224 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:29.988717 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:30.264860 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:30.542139 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:30.818265 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:31.095528 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:31.371360 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:31.648875 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:31.925910 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:32.202200 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:32.477989 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:32.755114 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:33.031899 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:33.308230 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:33.585082 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:33.862126 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:34.138000 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:34.414772 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:34.692376 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:34.968363 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:35.243470 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:35.519414 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:35.795714 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:36.072475 [3] Warning: no training nodes in this partition! Backward fake loss.
10:59:57.111806 [3] proc begin: <DistEnv 3/4 nccl>
11:00:02.820312 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
11:00:02.840667 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:00:07.350460 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:08.501669 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:08.982373 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:09.461269 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:09.942507 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:10.423177 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:10.902722 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:11.381858 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:11.861126 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:12.339885 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:12.818299 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:13.299272 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:13.778061 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:14.256949 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:14.735997 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:15.216802 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:15.696178 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:16.175917 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:16.655103 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:17.134376 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:17.613870 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:18.092731 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:18.572605 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:19.053036 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:19.531937 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:20.011085 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:20.497177 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:20.976608 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:21.455627 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:21.934142 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:22.414763 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:22.892874 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:23.372991 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:23.853469 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:24.331773 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:24.811361 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:25.291515 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:25.771339 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:26.250002 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:26.729168 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:27.208944 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:27.687225 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:28.166971 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:28.648665 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:29.128682 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:29.607580 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:30.086835 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:30.567154 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:31.046387 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:31.524630 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:32.003160 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:32.484513 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:32.962898 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:33.441822 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:33.921893 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:34.400887 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:34.879961 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:35.359091 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:35.839089 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:36.316896 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:36.796682 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:37.276330 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:37.755308 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:38.234622 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:38.713567 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:39.192916 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:39.672931 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:40.151734 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:40.631738 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:41.110244 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:41.588582 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:42.067648 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:42.546543 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:43.024988 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:43.503456 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:43.981581 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:44.459476 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:44.937608 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:45.415258 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:45.894108 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:46.372967 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:46.851262 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:47.329157 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:47.807338 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:48.285506 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:48.763730 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:49.242172 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:49.720451 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:50.198496 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:50.685674 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:51.165660 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:51.644254 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:52.123301 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:52.603777 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:53.082920 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:53.561271 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:54.040919 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:54.520412 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:55.000039 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:55.480575 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:55.962732 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:56.441978 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:56.921672 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:57.400661 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:57.880476 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:58.359913 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:58.840589 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:59.319396 [3] Warning: no training nodes in this partition! Backward fake loss.
11:00:59.799174 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:00.278466 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:00.757481 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:01.236819 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:01.728123 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:02.229221 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:02.728121 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:03.218938 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:03.699111 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:04.181322 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:04.660446 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:05.141318 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:05.621077 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:06.100835 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:06.580251 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:07.059236 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:07.538387 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:08.017731 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:08.496449 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:08.974759 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:09.454081 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:09.933466 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:10.414594 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:10.895093 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:11.375809 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:11.855733 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:12.336585 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:12.816890 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:13.295461 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:13.774648 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:14.253802 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:14.733272 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:15.213270 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:15.691711 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:16.171980 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:16.652024 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:17.130512 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:17.609302 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:18.089150 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:18.567768 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:19.047227 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:19.527593 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:20.006659 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:20.486579 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:20.966660 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:21.447087 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:21.926381 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:22.405243 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:22.884181 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:23.362837 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:23.841988 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:24.320241 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:24.799657 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:25.279316 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:25.757627 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:26.236625 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:26.716036 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:27.196365 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:27.675862 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:28.155204 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:28.635882 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:29.114986 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:29.593789 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:30.072395 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:30.551228 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:31.030083 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:31.509160 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:31.987672 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:32.466286 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:32.944766 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:33.424252 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:33.904070 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:34.382583 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:34.862067 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:35.340094 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:35.818942 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:36.297763 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:36.776434 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:37.255230 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:37.733898 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:38.212848 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:38.691882 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:39.171108 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:39.649711 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:40.130146 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:40.608825 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:41.086878 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:41.566191 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:42.044652 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:42.522960 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:43.002071 [3] Warning: no training nodes in this partition! Backward fake loss.
11:01:43.481707 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:00.458668 [3] proc begin: <DistEnv 3/4 nccl>
11:02:06.158638 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
11:02:06.179961 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:02:11.306346 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:12.987730 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:13.865542 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:14.744686 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:15.625200 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:16.503615 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:17.382381 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:18.262356 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:19.141454 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:20.019640 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:20.901336 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:21.779362 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:22.658278 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:23.536359 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:24.414228 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:25.291159 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:26.170892 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:27.048318 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:27.926811 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:28.805570 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:29.683023 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:30.560213 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:31.440365 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:32.317812 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:33.196293 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:34.074686 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:34.953618 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:35.832015 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:36.710966 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:37.590463 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:38.468963 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:39.347681 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:40.226309 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:41.106621 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:41.989809 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:42.870541 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:43.749653 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:44.628236 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:45.506319 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:46.384573 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:47.263582 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:48.142079 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:49.020671 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:49.899021 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:50.779825 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:51.657974 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:52.537116 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:53.415616 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:54.292972 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:55.171152 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:56.050196 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:56.928853 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:57.807293 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:58.686051 [3] Warning: no training nodes in this partition! Backward fake loss.
11:02:59.565641 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:00.443451 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:01.322125 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:02.205740 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:03.127532 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:04.029155 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:04.911510 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:05.791469 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:06.672336 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:07.553052 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:08.432241 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:09.312048 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:10.191155 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:11.070606 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:11.950116 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:12.829841 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:13.708873 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:14.587751 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:15.467794 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:16.346536 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:17.226245 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:18.105627 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:18.985372 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:19.864215 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:20.744675 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:21.625011 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:22.505161 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:23.384141 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:24.262869 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:25.142319 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:26.021702 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:26.900309 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:27.779279 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:28.659388 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:29.538602 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:30.416745 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:31.296707 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:32.176422 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:33.055323 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:33.933924 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:34.813058 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:35.692293 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:36.570709 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:37.450965 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:38.330252 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:39.209886 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:40.089082 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:40.969331 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:41.848681 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:42.726712 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:43.605273 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:44.483030 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:45.362348 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:46.240525 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:47.118921 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:47.996736 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:48.875581 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:49.753254 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:50.631456 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:51.510786 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:52.388979 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:53.265388 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:54.142335 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:55.020156 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:55.898953 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:56.776541 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:57.655027 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:58.532559 [3] Warning: no training nodes in this partition! Backward fake loss.
11:03:59.411030 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:00.290107 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:01.170603 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:02.068750 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:02.989116 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:03.889561 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:04.758644 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:05.638358 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:06.517356 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:07.397665 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:08.276117 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:09.155009 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:10.034537 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:10.915103 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:11.794208 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:12.675392 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:13.553303 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:14.433265 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:15.311648 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:16.192769 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:17.071493 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:17.951727 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:18.830867 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:19.711338 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:20.590986 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:21.472464 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:22.351937 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:23.229927 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:24.107997 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:24.985936 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:25.866213 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:26.747607 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:27.630120 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:28.514650 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:29.401339 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:30.280229 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:31.161554 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:32.039895 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:32.918721 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:33.796762 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:34.677067 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:35.556025 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:36.435097 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:37.313901 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:38.191934 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:39.069568 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:39.953783 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:40.837227 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:41.719250 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:42.600321 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:43.484771 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:44.369064 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:45.254096 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:46.141967 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:47.035611 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:47.928919 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:48.822270 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:49.712445 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:50.604415 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:51.495759 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:52.386990 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:53.273871 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:54.164030 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:55.049999 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:55.936940 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:56.823043 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:57.709390 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:58.598026 [3] Warning: no training nodes in this partition! Backward fake loss.
11:04:59.481524 [3] Warning: no training nodes in this partition! Backward fake loss.
11:05:00.365596 [3] Warning: no training nodes in this partition! Backward fake loss.
11:05:01.245097 [3] Warning: no training nodes in this partition! Backward fake loss.
11:05:02.168098 [3] Warning: no training nodes in this partition! Backward fake loss.
11:05:03.079956 [3] Warning: no training nodes in this partition! Backward fake loss.
11:05:03.962065 [3] Warning: no training nodes in this partition! Backward fake loss.
11:05:04.844719 [3] Warning: no training nodes in this partition! Backward fake loss.
11:05:05.727769 [3] Warning: no training nodes in this partition! Backward fake loss.
11:05:06.608720 [3] Warning: no training nodes in this partition! Backward fake loss.
11:05:07.492039 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:38.308692 [3] proc begin: <DistEnv 3/4 nccl>
14:44:38.447192 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
14:44:38.456495 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:44:40.211569 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.357173 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.368282 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.378745 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.388131 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.399845 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.411116 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.422002 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.431268 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.446856 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.457790 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.468182 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.478609 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.488908 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.499780 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.509778 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.520339 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.530015 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.541330 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.552625 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.563000 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.572355 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.583053 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.591464 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.602998 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.613536 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.623743 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.633004 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.643192 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.651604 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.660800 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.671671 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.680163 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.689359 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.699482 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.708012 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.717854 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.728364 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.737075 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.746837 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.755766 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.763868 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.773585 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.782808 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.790931 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.801590 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.810989 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.818828 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.828867 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.840555 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.850055 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.861094 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.871111 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.879524 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.899930 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.914450 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.923506 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.934243 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.946529 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.955379 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.964814 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.977576 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.985863 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.998722 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.008002 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.017054 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.027779 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.037632 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.045554 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.055993 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.066222 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.075583 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.084234 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.093229 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.102263 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.111962 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.121477 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.129718 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.139361 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.149732 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.159068 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.170292 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.179906 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.187940 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.197979 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.206471 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.214222 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.238943 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.249772 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.256740 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.266619 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.275631 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.282948 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.294282 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.303491 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.315391 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.328279 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.337873 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.353170 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.365921 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.374772 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.383150 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.392863 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.409208 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.418315 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.428301 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.436617 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.444453 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.454121 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.462414 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.470332 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.486276 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.495576 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.505619 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.517387 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.527495 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.535966 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.549031 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.559568 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.568465 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.581261 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.592130 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.600441 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.611493 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.621967 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.630423 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.640880 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.650142 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.658698 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.670185 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.680334 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.691182 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.702956 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.716258 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.723752 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.736585 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.747361 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.756587 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.767903 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.777590 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.786473 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.797401 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.805187 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.812137 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.820210 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.827374 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.834271 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.847388 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.856048 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.866675 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.877771 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.888832 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.900920 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.917640 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.929066 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.939127 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.952738 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.964277 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.973921 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.996398 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.013715 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.028853 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.039186 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.051947 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.060036 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.073331 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.082406 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.090527 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.100370 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.110346 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.117234 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.126645 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.135788 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.142912 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.152042 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.161260 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.168804 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.179951 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.188574 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.196949 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.205473 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.214240 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.222418 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.231325 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.242800 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.250429 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.260193 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.269168 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.276417 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.289503 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.300412 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.317744 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.328739 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.338827 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.353663 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.369441 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.380062 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.389895 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.398887 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.412838 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:13.681115 [3] proc begin: <DistEnv 3/4 nccl>
14:45:13.742409 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
14:45:13.754324 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:45:15.236284 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:46.800429 [3] proc begin: <DistEnv 3/4 nccl>
14:45:46.879298 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
14:45:46.891904 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:45:49.092906 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.881345 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.893235 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.904514 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.914579 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.924225 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.934338 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.945876 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.955584 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.965668 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.976373 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.985709 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.995417 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.005530 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.014383 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.025093 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.034739 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.043617 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.054529 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.064166 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.074001 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.084034 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.094031 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.103474 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.113607 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.124380 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.132960 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.143468 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.154275 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.163143 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.173926 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.183871 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.196751 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.208048 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.218532 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.228023 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.237718 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.247524 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.257554 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.267982 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.278939 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.287915 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.300384 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.310844 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.321196 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.333226 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.346262 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.355712 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.365675 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.374745 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.384169 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.396073 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.407611 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.418278 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.431483 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.442812 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.453213 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.466401 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.477551 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.488113 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.500429 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.512923 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.522410 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.541421 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.558809 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.569376 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.578829 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.589262 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.598605 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.608286 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.619009 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.628278 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.638893 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.652846 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.669702 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.677950 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.685362 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.691618 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.702682 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.709534 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.720151 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.730339 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.745457 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.755938 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.767013 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.780212 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.791502 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.813015 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.823863 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.835755 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.857630 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.869809 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.880086 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.888878 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.898245 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.907013 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.916967 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.926162 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.935371 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.946946 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.955937 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.964376 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.974077 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.986627 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.996896 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.006508 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.016120 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.025126 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.035487 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.045509 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.054004 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.063830 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.083771 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.093344 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.102808 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.112259 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.121036 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.131595 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.140795 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.149726 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.160141 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.170331 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.178751 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.189546 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.200182 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.210361 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.220601 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.230249 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.239178 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.250227 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.259623 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.268050 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.278176 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.287427 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.296046 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.305383 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.314643 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.323582 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.333074 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.342479 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.351271 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.361024 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.370146 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.378620 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.389204 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.399348 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.407931 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.417597 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.427366 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.435591 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.445178 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.456181 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.464822 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.475466 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.486234 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.494755 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.506378 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.516302 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.524839 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.535664 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.545595 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.554056 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.566330 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.576368 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.585677 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.595751 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.605804 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.620674 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.637500 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.647810 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.661202 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.673301 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.682780 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.691030 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.700408 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.710524 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.718567 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.727998 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.738218 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.746344 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.756788 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.765777 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.773970 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.783983 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.792905 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.800784 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.816615 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.825922 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.835822 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.845172 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.859628 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.867714 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.877479 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.888149 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.896440 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.906942 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.915738 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.923695 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.933688 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.942052 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:39.843451 [3] proc begin: <DistEnv 3/4 nccl>
14:51:39.892740 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
14:51:39.904883 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:51:42.294063 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.294670 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.307215 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.318978 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.327754 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.336138 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.348266 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.358754 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.369166 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.396907 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.414959 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.423518 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.434012 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.445045 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.454564 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.465262 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.475634 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.483995 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.494176 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.503636 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.514106 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.524968 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.535333 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.542620 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.551929 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.560791 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.568973 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.581658 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.592971 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.601999 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.614595 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.625074 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.637899 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.648783 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.659708 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.668254 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.677206 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.688139 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.696519 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.708286 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.721194 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.729944 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.741596 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.751035 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.759262 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.770099 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.779425 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.787524 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.798768 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.807616 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.815215 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.825382 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.834089 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.842659 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.852838 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.863334 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.871181 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.882118 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.891363 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.899396 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.909800 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.918938 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.927032 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.937415 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.946434 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.954349 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.971039 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.983443 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.992434 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.001609 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.010529 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.019451 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.027907 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.038370 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.047488 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.056813 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.066688 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.075794 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.087325 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.098721 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.107940 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.118964 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.129244 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.135929 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.145113 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.155174 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.162016 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.175591 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.185255 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.192803 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.207625 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.220254 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.229879 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.240090 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.251934 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.261153 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.271987 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.282101 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.291160 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.300885 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.310676 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.319699 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.330417 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.340381 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.349545 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.363500 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.373969 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.384190 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.406462 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.418482 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.434418 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.444016 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.457526 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.464312 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.474856 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.484844 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.493000 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.503269 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.514164 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.522370 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.534408 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.543750 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.552768 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.564187 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.574054 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.582194 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.592661 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.616169 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.625195 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.635960 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.647379 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.661515 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.673280 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.683294 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.694127 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.703035 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.713107 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.723020 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.732635 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.742597 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.752856 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.763229 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.774498 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.782461 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.792604 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.802928 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.811922 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.822685 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.833662 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.841335 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.855650 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.867294 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.874424 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.885761 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.894602 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.902623 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.913166 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.921988 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.929862 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.941203 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.950051 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.958233 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.970011 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.979583 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.988730 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.000334 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.007311 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.014421 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.021817 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.028761 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.036794 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.050530 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.068027 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.078716 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.090784 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.100509 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.109037 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.123296 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.133393 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.142259 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.154940 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.165452 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.174181 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.186846 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.198192 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.207468 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.219926 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.230354 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.239142 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.250536 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.260593 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.270233 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.282066 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.292334 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.301520 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.311855 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.323631 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.334920 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.346250 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.356148 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:00.100499 [3] proc begin: <DistEnv 3/4 nccl>
14:55:00.193120 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
14:55:00.202338 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:55:01.907557 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:02.947012 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:02.959683 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:02.971156 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:02.982212 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:02.992761 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.003624 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.021676 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.036275 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.050154 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.063462 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.075252 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.087629 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.100518 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.111088 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.120722 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.131761 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.145865 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.155716 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.164108 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.178753 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.187838 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.199944 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.212974 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.221909 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.237629 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.247861 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.259621 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.274456 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.285274 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.300339 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.309280 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.322980 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.332854 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.347570 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.356532 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.374738 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.384949 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.396833 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.408971 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.420814 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.434140 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.441946 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.450584 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.462227 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.471350 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.480015 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.489921 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.498056 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.506703 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.517601 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.524776 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.538888 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.547903 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.556659 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.563374 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.575796 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.582760 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.592171 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.602908 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.615670 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.624340 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.638371 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.647184 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.661347 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.668951 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.682123 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.691500 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.703364 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.713072 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.726590 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.734357 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.747539 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.760022 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.770270 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.778448 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.792953 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.801220 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.815254 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.825459 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.835327 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.843709 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.857487 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.865160 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.878660 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.888369 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.907439 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.920347 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.930276 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.942536 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.952216 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.960777 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.978436 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.986444 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.001287 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.008454 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.021112 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.031344 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.043591 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.051398 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.074417 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.088880 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.106387 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.115637 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.128176 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.136709 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.149421 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.159159 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.173526 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.181758 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.194032 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.203737 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.215720 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.224734 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.238674 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.254472 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.270586 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.280569 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.293625 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.301939 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.314831 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.323345 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.337507 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.347425 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.364877 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.375201 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.386827 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.398404 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.409688 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.422196 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.433368 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.443399 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.455840 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.464539 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.477255 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.485423 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.498724 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.508273 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.522144 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.533091 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.542819 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.551506 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.566710 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.574392 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.587318 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.597642 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.610956 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.623031 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.646280 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.658896 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.668006 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.675630 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.689703 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.700635 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.719946 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.730358 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.743893 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.755278 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.773828 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.784606 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.804114 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.816785 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.826970 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.835593 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.847169 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.855615 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.865687 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.873316 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.886806 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.893874 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.904202 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.912599 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.925844 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.937726 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.947882 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.956207 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.968078 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.976752 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.987584 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.995239 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.005650 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.012948 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.023174 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.030885 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.043369 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.053580 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.063926 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.071652 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.083447 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.091850 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.105275 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.113800 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.126160 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.134903 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.146044 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.154192 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.165946 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.174147 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.185895 [3] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.194241 [3] Warning: no training nodes in this partition! Backward fake loss.
14:58:30.467223 [3] proc begin: <DistEnv 3/4 nccl>
14:58:44.142652 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
14:58:44.159478 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:00:07.866752 [3] proc begin: <DistEnv 3/4 nccl>
15:00:07.919506 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
15:00:07.929404 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:00:09.246438 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:09.991506 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.007948 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.020700 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.028417 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.035862 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.043261 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.054437 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.071427 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.079636 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.087706 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.094849 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.103725 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.110619 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.119724 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.127788 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.135168 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.143130 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.150995 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.158610 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.165623 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.172479 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.180187 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.188901 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.197162 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.205183 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.213050 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.220376 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.227852 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.235472 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.243019 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.250901 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.258219 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.265941 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.273723 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.281374 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.289057 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.299861 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.308432 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.316536 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.326032 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.335014 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.345077 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.354710 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.362100 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.369881 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.377050 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.384396 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.392379 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.399984 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.407327 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.413981 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.421979 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.428067 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.436602 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.443235 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.451166 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.457411 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.467677 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.474963 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.482919 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.489800 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.498096 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.504580 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.511867 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.519276 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.530819 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.537495 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.545854 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.551895 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.560195 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.566884 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.576393 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.583258 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.593295 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.601829 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.610841 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.621176 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.630153 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.637118 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.645986 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.652076 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.660322 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.670312 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.677936 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.683875 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.693657 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.701206 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.709305 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.715613 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.724168 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.730928 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.739291 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.745955 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.758123 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.767557 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.774815 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.780707 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.788193 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.794933 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.802880 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.809006 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.816596 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.823265 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.830924 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.836854 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.845162 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.852592 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.859681 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.866908 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.874825 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.882270 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.899631 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.906318 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.914953 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.928371 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.938027 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.944587 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.952289 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.959190 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.967149 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.973825 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.982113 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.988284 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.000001 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.011262 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.019639 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.026032 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.034206 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.040055 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.051477 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.059016 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.070751 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.078724 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.087170 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.093462 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.105331 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.114646 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.122800 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.132115 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.144777 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.156913 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.167160 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.175500 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.183590 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.190132 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.202021 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.209926 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.219010 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.225837 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.233387 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.239869 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.247863 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.257948 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.269808 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.276381 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.284909 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.291119 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.302642 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.309690 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.317899 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.323750 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.332084 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.338305 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.346089 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.352652 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.360103 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.367668 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.376212 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.383500 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.391775 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.402781 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.410916 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.418411 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.426810 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.433982 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.442316 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.450387 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.458912 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.466769 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.475172 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.482091 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.490455 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.497610 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.505818 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.513463 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.522036 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.538376 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.548962 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.556171 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.564668 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.572008 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.580036 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.587851 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.596296 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.604283 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.612391 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.619833 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.628417 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.635987 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:18.533037 [3] proc begin: <DistEnv 3/4 nccl>
15:18:18.596082 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
15:18:18.605603 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:18:19.802603 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.463043 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.474924 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.483083 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.491120 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.498719 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.509352 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.519619 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.528029 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.538034 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.545659 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.554572 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.562036 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.570328 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.577518 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.585001 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.592591 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.600395 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.608115 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.615578 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.623023 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.630187 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.638289 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.645884 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.653378 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.660433 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.668912 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.678689 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.686936 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.694903 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.702531 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.713060 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.722746 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.731392 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.743156 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.750768 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.758153 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.767777 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.775608 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.782873 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.789968 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.798295 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.805604 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.813026 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.820910 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.828498 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.835813 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.843039 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.850703 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.858520 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.866514 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.872940 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.880433 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.886819 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.894835 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.901246 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.921353 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.929438 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.938060 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.944478 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.952558 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.958852 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.967414 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.973820 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.981892 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.987669 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.995826 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.002193 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.010287 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.016266 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.026626 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.032702 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.041220 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.047618 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.056162 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.063178 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.070806 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.077472 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.085320 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.091906 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.100430 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.106910 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.114680 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.120701 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.128923 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.135317 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.145521 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.152389 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.162887 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.170122 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.178947 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.184965 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.196498 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.204163 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.212971 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.222378 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.230276 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.237237 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.245277 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.251159 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.259470 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.266087 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.279349 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.285805 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.293520 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.300089 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.307690 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.314414 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.322630 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.328812 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.336173 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.342639 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.355094 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.364391 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.375014 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.384751 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.393199 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.399845 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.410362 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.427230 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.442039 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.448554 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.461351 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.470718 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.480155 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.486399 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.494142 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.500862 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.518190 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.528225 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.544374 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.551728 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.561068 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.567418 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.575330 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.581962 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.589715 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.598239 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.606577 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.612573 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.620420 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.627098 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.635764 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.642218 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.650531 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.656535 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.665046 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.671621 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.679719 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.685938 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.693693 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.699610 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.707582 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.714200 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.722162 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.728579 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.738356 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.755190 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.763930 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.770556 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.778300 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.785267 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.792815 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.801946 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.810285 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.817021 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.824157 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.830449 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.841510 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.850539 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.858532 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.870523 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.878068 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.884701 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.895248 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.901845 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.909419 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.915684 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.923960 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.930547 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.939010 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.945290 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.965223 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.977584 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.989645 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.998601 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.008759 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.017344 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.025711 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.031861 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.039728 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.046519 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.054640 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.060861 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.069580 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.076409 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.083974 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.090687 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.100688 [3] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.107149 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:57.754236 [3] proc begin: <DistEnv 3/4 nccl>
08:58:57.852964 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
08:58:57.864444 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

08:58:59.120546 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.817894 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.830486 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.838039 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.845929 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.853988 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.860955 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.868987 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.877165 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.885336 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.896493 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.908272 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.917699 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.924782 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.932403 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.939159 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.946990 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.954319 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.962761 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.969692 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.977221 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.984616 [3] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.994413 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.004878 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.014538 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.026470 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.035719 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.043875 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.055437 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.065195 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.074309 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.082239 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.089652 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.096287 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.104242 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.111387 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.118563 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.125899 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.132835 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.140984 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.148388 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.155766 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.164738 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.174651 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.182397 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.192415 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.200753 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.208638 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.216696 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.224777 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.232815 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.241245 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.249390 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.259027 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.267692 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.276581 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.284413 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.292748 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.303642 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.314420 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.324502 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.334664 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.346028 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.356422 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.363771 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.371353 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.378832 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.386177 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.393426 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.401000 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.408199 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.415228 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.434043 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.447142 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.455150 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.462733 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.470531 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.477398 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.484752 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.492470 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.499896 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.507300 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.516536 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.525090 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.533103 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.540794 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.547846 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.555076 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.563958 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.571642 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.579235 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.586550 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.593933 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.611819 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.622702 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.630394 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.637118 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.643815 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.651424 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.658733 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.666694 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.673455 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.680875 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.688587 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.695978 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.702905 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.714417 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.736883 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.749167 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.759419 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.776059 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.787461 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.796224 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.809382 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.818887 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.825992 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.833490 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.840601 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.848069 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.857880 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.871320 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.881282 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.890552 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.898795 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.907129 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.915405 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.923292 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.932116 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.940293 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.948887 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.957274 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.965266 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.973370 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.981592 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.990128 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.998480 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.006409 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.014251 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.021846 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.030420 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.038449 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.046712 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.054532 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.062457 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.070315 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.078190 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.086354 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.094689 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.102707 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.110299 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.118143 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.126680 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.134403 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.142597 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.152847 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.163958 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.174743 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.185367 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.193694 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.202082 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.209867 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.218819 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.226540 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.233973 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.241023 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.250697 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.258196 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.265343 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.272235 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.279932 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.287188 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.298654 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.309047 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.316597 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.323008 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.329428 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.338494 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.345753 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.353514 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.360289 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.367941 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.376197 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.383686 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.391031 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.398612 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.406160 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.413043 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.422593 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.432728 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.441945 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.456209 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.467497 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.474693 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.482468 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.490351 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.501082 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.508553 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.515791 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.523091 [3] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.532645 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:05.615841 [3] proc begin: <DistEnv 3/4 nccl>
09:00:05.712763 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
09:00:05.723428 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:00:07.165795 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:07.893290 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:07.915155 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:07.938790 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:07.962169 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:07.982545 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.002304 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.026326 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.043071 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.058656 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.077843 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.097976 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.124061 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.146542 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.165420 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.187812 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.209684 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.230849 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.253981 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.272639 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.294483 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.312399 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.328277 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.350432 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.369501 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.390613 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.413696 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.434519 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.457051 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.477366 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.503606 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.520343 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.542702 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.570696 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.594250 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.618270 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.643472 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.666504 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.686659 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.709948 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.731274 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.754803 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.774728 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.796375 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.817832 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.838737 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.862195 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.883538 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.903752 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.923256 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.944087 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.958363 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.969814 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.981933 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.002074 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.026455 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.045575 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.066914 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.086428 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.106141 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.129513 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.149315 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.170672 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.193614 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.215155 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.234579 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.255024 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.273185 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.288210 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.298548 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.311048 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.324458 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.335675 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.346528 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.357305 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.372849 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.386908 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.398518 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.414241 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.435212 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.457676 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.478982 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.501520 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.521496 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.541575 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.565963 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.585358 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.606251 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.633464 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.653378 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.666395 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.680117 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.691461 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.702869 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.714218 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.724965 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.736159 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.747796 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.759367 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.770025 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.781059 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.794004 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.818067 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.841898 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.860254 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.882509 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.898413 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.913539 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.934774 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.955373 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.972871 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.994406 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.014588 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.037441 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.060184 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.080864 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.096583 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.116303 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.132864 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.155797 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.178458 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.198763 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.219124 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.235939 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.258531 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.276793 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.297933 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.322263 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.342889 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.361893 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.380134 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.402547 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.422555 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.446587 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.467601 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.489796 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.509355 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.530935 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.553547 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.586188 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.605058 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.638708 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.655035 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.681452 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.702580 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.726191 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.749295 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.770778 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.793994 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.814983 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.834023 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.850957 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.873863 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.893106 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.914639 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.937098 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.959898 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.981804 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.000959 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.022388 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.041453 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.062629 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.085830 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.106848 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.129413 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.149366 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.174714 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.197960 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.217747 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.238669 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.258543 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.276001 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.298088 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.316926 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.336809 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.353538 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.368787 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.390818 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.414013 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.435226 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.454281 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.472654 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.494119 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.514821 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.538095 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.559737 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.591132 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.614206 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.645891 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.665701 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.686831 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.706566 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.724774 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.747225 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.770109 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.789089 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.810356 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.833900 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.854930 [3] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.875326 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:09.891183 [3] proc begin: <DistEnv 3/4 nccl>
09:01:09.979798 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
09:01:09.992757 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:01:11.507427 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.218320 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.248662 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.279436 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.309676 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.344957 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.374052 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.405281 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.438335 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.469580 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.504120 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.542688 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.571137 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.599580 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.627990 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.661402 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.692529 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.718960 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.745510 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.777584 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.808715 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.832664 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.862233 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.894521 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.926430 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.958776 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.986937 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.015769 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.041218 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.070881 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.096753 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.125600 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.154505 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.181496 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.209335 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.243779 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.271775 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.298935 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.328532 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.354616 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.380627 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.405323 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.434296 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.461847 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.490540 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.517296 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.546631 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.582929 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.611390 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.642930 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.674691 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.704809 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.735138 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.763628 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.792785 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.828215 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.856635 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.889184 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.922726 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.955393 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.984986 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.025306 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.062081 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.093733 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.124779 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.150151 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.181017 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.211278 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.241868 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.270558 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.298329 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.328619 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.357050 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.388192 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.420816 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.450089 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.477169 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.505211 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.535578 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.564984 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.593114 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.622350 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.651510 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.677800 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.708067 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.733894 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.764106 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.791544 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.820703 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.852086 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.881390 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.914905 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.944699 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.979020 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.004960 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.035262 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.069133 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.099360 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.129689 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.161274 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.189761 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.218265 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.248194 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.274308 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.299636 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.329227 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.358116 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.388177 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.417606 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.448730 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.482256 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.512414 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.541108 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.572597 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.601513 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.630345 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.657725 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.687151 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.714269 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.743879 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.773450 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.801937 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.833376 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.869813 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.901363 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.936146 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.970768 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.999998 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.027391 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.078264 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.105010 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.133668 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.161541 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.189834 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.218641 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.247128 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.274370 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.299310 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.325902 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.351683 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.381168 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.411426 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.438994 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.468817 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.496792 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.527382 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.557028 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.586117 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.616984 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.647291 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.679466 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.705629 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.733097 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.763106 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.789572 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.817280 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.845774 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.873555 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.903632 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.933173 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.956201 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.990529 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.021006 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.056257 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.095823 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.125759 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.154407 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.182735 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.203004 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.233085 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.261996 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.291987 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.320959 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.350032 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.377931 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.406873 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.434717 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.462403 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.492478 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.522104 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.554231 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.583704 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.610046 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.638909 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.666150 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.693855 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.724093 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.753749 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.783460 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.813276 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.836081 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.860989 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.885925 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.917106 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.950441 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.993549 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:18.030364 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:18.066822 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:18.107004 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:18.135118 [3] Warning: no training nodes in this partition! Backward fake loss.
09:01:42.465946 [3] proc begin: <DistEnv 3/4 nccl>
09:01:48.733032 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:01:48.749534 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:02:59.845262 [3] proc begin: <DistEnv 3/4 nccl>
09:03:05.507792 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:03:05.521926 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:04:05.227814 [3] proc begin: <DistEnv 3/4 nccl>
09:04:11.404741 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:04:11.422637 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:05:07.852490 [3] proc begin: <DistEnv 3/4 nccl>
09:05:26.729389 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
09:05:26.743887 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:05:34.317666 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:35.278570 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:35.559211 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:35.839446 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:36.118590 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:36.399200 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:36.680009 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:36.959863 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:37.240388 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:37.521012 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:37.801172 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:38.081051 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:38.363034 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:38.642769 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:38.922985 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:39.209955 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:39.489755 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:39.769434 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:40.050078 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:40.330619 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:40.613162 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:40.895935 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:41.176235 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:41.456940 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:41.737418 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:42.017854 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:42.298244 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:42.578615 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:42.859041 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:43.140042 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:43.420341 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:43.702594 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:43.983373 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:44.264802 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:44.546223 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:44.827180 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:45.107523 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:45.388006 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:45.668480 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:45.948687 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:46.228625 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:46.510742 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:46.791758 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:47.071663 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:47.352804 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:47.633025 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:47.913187 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:48.193736 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:48.476503 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:48.756242 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:49.037278 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:49.318955 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:49.599513 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:49.880327 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:50.160991 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:50.440901 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:50.721076 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:51.001460 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:51.280273 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:51.561807 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:51.842141 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:52.123557 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:52.403683 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:52.685116 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:52.965924 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:53.246428 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:53.527147 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:53.807593 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:54.088347 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:54.368346 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:54.648804 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:54.928413 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:55.208841 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:55.490997 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:55.772659 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:56.053637 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:56.334051 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:56.614084 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:56.895229 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:57.174937 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:57.455056 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:57.735038 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:58.015010 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:58.295255 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:58.575007 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:58.855307 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:59.135863 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:59.416692 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:59.696091 [3] Warning: no training nodes in this partition! Backward fake loss.
09:05:59.977160 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:00.258214 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:00.539861 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:00.819882 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:01.099908 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:01.381468 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:01.661377 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:01.945099 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:02.236035 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:02.526209 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:02.817306 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:03.108792 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:03.398962 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:03.679616 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:03.960866 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:04.241393 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:04.523013 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:04.803433 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:05.084360 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:05.364970 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:05.646601 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:05.927116 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:06.209129 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:06.491508 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:06.773098 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:07.053316 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:07.334836 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:07.615614 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:07.896551 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:08.176813 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:08.457002 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:08.737439 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:09.017047 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:09.298478 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:09.579407 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:09.860494 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:10.141172 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:10.421073 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:10.700960 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:10.981447 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:11.262685 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:11.544557 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:11.824814 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:12.104940 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:12.385254 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:12.665710 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:12.946163 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:13.226889 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:13.506931 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:13.786955 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:14.066777 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:14.347369 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:14.627858 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:14.908364 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:15.188744 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:15.469550 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:15.749755 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:16.030278 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:16.311140 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:16.591030 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:16.871557 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:17.152507 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:17.432681 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:17.712224 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:17.993048 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:18.274380 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:18.554750 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:18.837452 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:19.117321 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:19.398246 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:19.679231 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:19.960665 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:20.240960 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:20.521731 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:20.802108 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:21.083832 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:21.364261 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:21.645327 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:21.925395 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:22.206190 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:22.486410 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:22.766911 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:23.048243 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:23.328468 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:23.610057 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:23.890651 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:24.171256 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:24.451666 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:24.732194 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:25.013552 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:25.294509 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:25.574255 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:25.853981 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:26.134447 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:26.415205 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:26.694929 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:26.974503 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:27.254905 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:27.535996 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:27.817190 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:28.097613 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:28.377247 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:28.657026 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:28.937248 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:29.217432 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:29.497660 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:29.778229 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:30.058958 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:30.339728 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:30.621079 [3] Warning: no training nodes in this partition! Backward fake loss.
09:06:30.901575 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:38.037358 [3] proc begin: <DistEnv 3/4 nccl>
09:07:42.871298 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
09:07:42.897077 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:07:47.896462 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:49.093640 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:49.589136 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:50.085709 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:50.581975 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:51.077106 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:51.572582 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:52.068732 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:52.563627 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:53.059665 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:53.554912 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:54.048870 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:54.544508 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:55.040421 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:55.536227 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:56.032765 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:56.528921 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:57.023682 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:57.517868 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:58.012565 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:58.507448 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:59.001688 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:59.496100 [3] Warning: no training nodes in this partition! Backward fake loss.
09:07:59.991947 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:00.486634 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:00.982459 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:01.478828 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:01.975329 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:02.486388 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:02.998876 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:03.512611 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:04.011219 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:04.507130 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:05.003264 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:05.499500 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:05.994894 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:06.490556 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:06.987723 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:07.484407 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:07.979032 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:08.475320 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:08.972380 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:09.469117 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:09.966268 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:10.463461 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:10.959021 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:11.455237 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:11.950967 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:12.448018 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:12.943370 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:13.440605 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:13.936078 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:14.432331 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:14.928716 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:15.426651 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:15.923729 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:16.419553 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:16.916259 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:17.412083 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:17.907695 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:18.404273 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:18.899530 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:19.394652 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:19.892006 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:20.387692 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:20.884257 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:21.379770 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:21.874742 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:22.369729 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:22.864806 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:23.359815 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:23.855190 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:24.349751 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:24.844746 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:25.341288 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:25.837170 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:26.332147 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:26.827622 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:27.324265 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:27.819068 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:28.313645 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:28.809387 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:29.305630 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:29.801627 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:30.298167 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:30.795615 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:31.292241 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:31.786700 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:32.283019 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:32.778164 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:33.275354 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:33.771523 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:34.266037 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:34.761073 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:35.257785 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:35.753733 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:36.249071 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:36.744029 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:37.240721 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:37.736223 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:38.231331 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:38.726266 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:39.222484 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:39.718512 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:40.214057 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:40.709364 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:41.204627 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:41.700686 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:42.196850 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:42.692687 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:43.187814 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:43.684194 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:44.179715 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:44.675636 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:45.172298 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:45.667954 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:46.162732 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:46.658844 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:47.155089 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:47.650922 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:48.146416 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:48.641335 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:49.135871 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:49.631650 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:50.127361 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:50.622090 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:51.117291 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:51.613216 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:52.108188 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:52.603403 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:53.099258 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:53.594532 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:54.089667 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:54.584448 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:55.079314 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:55.574325 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:56.069970 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:56.565012 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:57.060246 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:57.555622 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:58.050404 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:58.545762 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:59.040972 [3] Warning: no training nodes in this partition! Backward fake loss.
09:08:59.537091 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:00.033425 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:00.528833 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:01.025527 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:01.521005 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:02.026344 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:02.538673 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:03.053650 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:03.556678 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:04.053008 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:04.548616 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:05.043954 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:05.539923 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:06.036139 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:06.531822 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:07.029275 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:07.525247 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:08.020939 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:08.516763 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:09.012462 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:09.509084 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:10.005929 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:10.501936 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:10.999673 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:11.497004 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:11.992498 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:12.488707 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:12.985334 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:13.481288 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:13.976269 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:14.471412 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:14.967261 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:15.462515 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:15.958119 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:16.453727 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:16.949417 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:17.445170 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:17.941225 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:18.436618 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:18.932441 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:19.427604 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:19.926355 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:20.421517 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:20.917883 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:21.413352 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:21.908470 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:22.404295 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:22.899514 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:23.394652 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:23.890446 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:24.386095 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:24.881879 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:25.376832 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:25.874255 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:26.369933 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:26.865446 [3] Warning: no training nodes in this partition! Backward fake loss.
09:09:27.361320 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:03.433786 [3] proc begin: <DistEnv 3/4 nccl>
09:10:07.830572 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
09:10:07.847388 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:10:12.677904 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:14.235598 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:15.130181 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:16.022682 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:16.919651 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:17.811193 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:18.705721 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:19.603107 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:20.498557 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:21.395370 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:22.296481 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:23.194456 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:24.090408 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:24.988153 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:25.885279 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:26.781475 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:27.675242 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:28.572996 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:29.468507 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:30.365708 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:31.263839 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:32.161366 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:33.058209 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:33.962715 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:34.861285 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:35.758034 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:36.655754 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:37.552061 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:38.448935 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:39.345949 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:40.244916 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:41.140791 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:42.035169 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:42.931392 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:43.827183 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:44.722538 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:45.616722 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:46.510829 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:47.402737 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:48.294199 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:49.185171 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:50.076919 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:50.967406 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:51.857077 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:52.745652 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:53.635163 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:54.525667 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:55.415980 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:56.305218 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:57.197295 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:58.087591 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:58.978214 [3] Warning: no training nodes in this partition! Backward fake loss.
09:10:59.868727 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:00.760091 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:01.674702 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:02.606304 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:03.504607 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:04.400727 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:05.295885 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:06.191951 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:07.089513 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:07.984182 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:08.879317 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:09.774353 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:10.671619 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:11.569572 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:12.466968 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:13.362484 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:14.258325 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:15.154298 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:16.049032 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:16.945352 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:17.834760 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:18.726912 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:19.617562 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:20.509384 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:21.400732 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:22.292456 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:23.183427 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:24.076644 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:24.967066 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:25.863149 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:26.756402 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:27.649666 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:28.544580 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:29.439896 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:30.334718 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:31.232813 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:32.127693 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:33.022131 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:33.915965 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:34.806971 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:35.701101 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:36.595660 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:37.493702 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:38.388076 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:39.283558 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:40.180502 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:41.076846 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:41.971173 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:42.868152 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:43.763115 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:44.658831 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:45.553003 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:46.447200 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:47.342660 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:48.237674 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:49.133053 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:50.028073 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:50.923975 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:51.818568 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:52.715815 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:53.610507 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:54.506286 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:55.399809 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:56.293836 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:57.189119 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:58.086196 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:58.982325 [3] Warning: no training nodes in this partition! Backward fake loss.
09:11:59.876415 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:00.771638 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:01.667672 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:02.595353 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:03.516966 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:04.413048 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:05.308416 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:06.204535 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:07.097343 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:07.996052 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:08.891552 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:09.790224 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:10.685133 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:11.581923 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:12.477026 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:13.370802 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:14.266463 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:15.162678 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:16.057232 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:16.949639 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:17.841626 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:18.733195 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:19.625178 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:20.515442 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:21.407334 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:22.298894 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:23.190607 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:24.081957 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:24.973037 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:25.865593 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:26.756799 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:27.648190 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:28.539768 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:29.431331 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:30.321940 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:31.214422 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:32.105202 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:32.996693 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:33.887373 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:34.778329 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:35.670057 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:36.561255 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:37.453964 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:38.346864 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:39.237532 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:40.129530 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:41.025159 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:41.924110 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:42.819571 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:43.711578 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:44.602582 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:45.497844 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:46.395670 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:47.291628 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:48.185523 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:49.080697 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:49.975173 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:50.870013 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:51.766368 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:52.660802 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:53.555094 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:54.450405 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:55.346345 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:56.239964 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:57.133642 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:58.029243 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:58.924462 [3] Warning: no training nodes in this partition! Backward fake loss.
09:12:59.818719 [3] Warning: no training nodes in this partition! Backward fake loss.
09:13:00.713470 [3] Warning: no training nodes in this partition! Backward fake loss.
09:13:01.617726 [3] Warning: no training nodes in this partition! Backward fake loss.
09:13:02.541749 [3] Warning: no training nodes in this partition! Backward fake loss.
09:13:03.460262 [3] Warning: no training nodes in this partition! Backward fake loss.
09:13:04.354287 [3] Warning: no training nodes in this partition! Backward fake loss.
09:13:05.250706 [3] Warning: no training nodes in this partition! Backward fake loss.
09:13:06.147557 [3] Warning: no training nodes in this partition! Backward fake loss.
09:13:07.043449 [3] Warning: no training nodes in this partition! Backward fake loss.
09:13:07.939887 [3] Warning: no training nodes in this partition! Backward fake loss.
09:13:08.835650 [3] Warning: no training nodes in this partition! Backward fake loss.
09:13:09.732304 [3] Warning: no training nodes in this partition! Backward fake loss.
09:13:10.628188 [3] Warning: no training nodes in this partition! Backward fake loss.
09:13:11.524802 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:04.757270 [3] proc begin: <DistEnv 3/4 nccl>
14:50:29.513056 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:50:29.527678 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:50:30.856617 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:32.295814 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:33.011410 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:33.725351 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:34.437640 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:35.149704 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:35.864899 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:36.579017 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:37.295135 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:38.008555 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:38.723355 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:39.437422 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:40.149525 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:40.862200 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:41.575841 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:42.289620 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:43.002632 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:43.715451 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:44.430539 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:45.143823 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:45.857252 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:46.569415 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:47.282584 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:47.996105 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:48.710874 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:49.426601 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:50.140952 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:50.855284 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:51.567686 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:52.281649 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:52.995553 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:53.710895 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:54.426750 [3] Warning: no training nodes in this partition! Backward fake loss.
14:50:55.140921 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:10.259220 [3] proc begin: <DistEnv 3/4 nccl>
14:51:10.422663 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
14:51:10.432619 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:51:11.645101 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.359045 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.372155 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.380719 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.392166 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.399991 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.407215 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.415038 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.422398 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.431744 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.440883 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.449917 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.456661 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.464745 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.472017 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.479634 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.490852 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.498874 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.506092 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.513694 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.521192 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.528532 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.536701 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.543971 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.551452 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.559066 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.566963 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.573913 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.581330 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.589367 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.598097 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.606100 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.613306 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.620678 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.628140 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.635364 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.643294 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.650802 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.658617 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.666282 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.679585 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.688294 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.695753 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.704952 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.714326 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.723007 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.731876 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.742667 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.750358 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.758151 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.765638 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.772155 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.779837 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.787965 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.795580 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.802175 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.810695 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.817019 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.825026 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.831184 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.839939 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.845920 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.853677 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.860079 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.868088 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.875965 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.886446 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.892954 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.900127 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.906277 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.915541 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.922047 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.929862 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.936106 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.943465 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.950258 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.958408 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.964298 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.972211 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.979735 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.992191 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.000192 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.010319 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.017191 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.024968 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.031600 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.043206 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.055753 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.063664 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.070477 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.084945 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.092026 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.105636 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.114703 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.123841 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.130805 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.142201 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.148476 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.156220 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.162447 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.171332 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.177637 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.185589 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.191443 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.199667 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.209322 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.217310 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.228448 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.239185 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.249090 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.268679 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.277114 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.286113 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.291975 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.299549 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.305612 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.314277 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.320804 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.332578 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.341088 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.351187 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.357168 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.364675 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.374241 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.383389 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.389772 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.397744 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.405737 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.413658 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.422791 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.430044 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.436486 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.444782 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.450513 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.458609 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.465536 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.479731 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.491130 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.500532 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.507058 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.526802 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.538896 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.547126 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.556316 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.565008 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.570523 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.579222 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.585209 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.593037 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.599154 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.608844 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.615138 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.622928 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.629226 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.637065 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.646040 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.655264 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.661559 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.669335 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.674917 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.684560 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.692189 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.700028 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.705694 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.713754 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.720245 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.730614 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.737402 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.744826 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.752175 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.763975 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.772752 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.781009 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.787186 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.795943 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.802149 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.812527 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.819877 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.829858 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.840057 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.847642 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.854603 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.863011 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.868837 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.876409 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.882499 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.890435 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.899836 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.907363 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.913839 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.921955 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.928061 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.935437 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.941910 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.949655 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.955725 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.963712 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.969680 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.977315 [3] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.983456 [3] Warning: no training nodes in this partition! Backward fake loss.
14:54:37.185226 [3] proc begin: <DistEnv 3/4 nccl>
14:54:46.745506 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
14:54:46.759180 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:47:44.555794 [3] proc begin: <DistEnv 3/4 nccl>
20:47:53.356223 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:47:53.374277 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:48:16.925330 [3] proc begin: <DistEnv 3/4 nccl>
20:48:23.552060 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:48:23.575731 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:49:01.114921 [3] proc begin: <DistEnv 3/4 nccl>
20:49:06.281946 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:49:06.300425 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:52:06.359040 [3] proc begin: <DistEnv 3/4 nccl>
20:52:11.566975 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:52:11.594559 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:52:51.680996 [3] proc begin: <DistEnv 3/4 nccl>
20:52:51.815302 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
20:52:51.824782 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:52:53.087909 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.814109 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.829192 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.840773 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.850053 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.862153 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.872901 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.885937 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.896148 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.905488 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.914658 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.924051 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.935590 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.950065 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.959601 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.968827 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.978489 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.987673 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.996929 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.006146 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.015997 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.025172 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.034422 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.047075 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.057892 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.068289 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.079873 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.089467 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.098580 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.108740 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.118688 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.127856 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.136982 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.153230 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.166567 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.175687 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.184733 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.194236 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.207123 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.218767 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.229168 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.242055 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.252348 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.264828 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.275195 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.285646 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.296438 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.308194 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.320371 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.331502 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.344502 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.353972 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.364656 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.376861 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.386377 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.400115 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.414900 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.425998 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.435297 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.446607 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.456015 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.465685 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.474958 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.484268 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.493477 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.502850 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.514101 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.526848 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.536099 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.548003 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.568046 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.578588 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.587535 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.597009 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.608645 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.620406 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.630781 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.639939 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.652121 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.661295 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.672188 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.692547 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.706923 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.717424 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.727830 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.738070 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.749215 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.761168 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.771128 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.785060 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.796813 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.809716 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.820420 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.836280 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.854149 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.870006 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.880923 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.891426 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.903855 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.915749 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.926286 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.937367 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.946546 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.956124 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.965129 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.974294 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.982913 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.995443 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.007548 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.017080 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.030390 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.040341 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.052923 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.063738 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.073075 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.083790 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.097841 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.109241 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.118244 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.128576 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.138815 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.150576 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.163354 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.173827 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.182743 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.191325 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.200455 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.209379 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.218533 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.227355 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.236773 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.245864 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.254574 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.264687 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.275442 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.286400 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.297148 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.307267 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.320391 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.329921 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.339405 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.351760 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.360877 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.372633 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.381912 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.393411 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.406392 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.416220 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.428803 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.438210 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.450214 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.463220 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.476737 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.487769 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.497553 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.520942 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.533555 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.543068 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.553103 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.573024 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.585972 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.595407 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.604549 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.614618 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.625035 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.635758 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.647412 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.657646 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.668057 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.678690 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.699400 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.711769 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.722215 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.733628 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.744360 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.754700 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.766487 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.777784 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.788583 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.801057 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.812351 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.821922 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.833388 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.843647 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.852858 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.868346 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.877964 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.886995 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.899948 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.908680 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.920295 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.932028 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.945839 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.955108 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.964467 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.973522 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.982497 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.991233 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:56.000171 [3] Warning: no training nodes in this partition! Backward fake loss.
20:52:56.012220 [3] Warning: no training nodes in this partition! Backward fake loss.
20:53:18.807525 [3] proc begin: <DistEnv 3/4 nccl>
20:53:24.305164 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:53:24.322438 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:54:39.256417 [3] proc begin: <DistEnv 3/4 nccl>
20:54:39.313654 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
20:54:39.323943 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:54:40.672268 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.410484 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.426307 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.439771 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.454263 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.471527 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.481766 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.494777 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.509369 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.519208 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.529995 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.543083 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.552897 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.562562 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.572219 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.586204 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.595877 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.605978 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.615478 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.625126 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.635220 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.649174 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.661111 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.678247 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.694991 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.706698 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.719959 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.729397 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.740248 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.750129 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.759431 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.772951 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.782339 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.792239 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.804413 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.814524 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.824555 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.839957 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.855080 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.870737 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.882135 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.892457 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.905126 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.915801 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.931639 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.945278 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.956902 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.968503 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.981658 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.993604 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.004684 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.017442 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.031975 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.044780 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.055556 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.066472 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.079530 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.090041 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.102815 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.112874 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.126877 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.143542 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.157054 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.168931 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.179496 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.190227 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.200835 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.215195 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.226670 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.239685 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.251196 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.266016 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.283251 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.299712 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.312327 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.325433 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.339875 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.358334 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.374962 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.391856 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.405616 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.416497 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.428967 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.441870 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.457057 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.471405 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.483155 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.492533 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.503645 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.519341 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.531666 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.547222 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.557207 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.567780 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.578319 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.588206 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.604761 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.622997 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.636241 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.648020 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.661486 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.673409 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.692841 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.711525 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.724855 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.737508 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.750569 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.764135 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.776055 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.787837 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.800218 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.811824 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.823640 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.836322 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.848947 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.860610 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.872020 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.885968 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.897200 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.910181 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.925412 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.939721 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.950751 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.963086 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.973429 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.989505 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.000043 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.010720 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.021214 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.034189 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.047142 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.056682 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.067457 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.078486 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.093290 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.107043 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.118259 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.132355 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.141841 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.154544 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.165765 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.179792 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.189718 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.199131 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.208574 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.218199 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.227543 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.237347 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.246683 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.263544 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.274467 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.290350 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.300501 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.313787 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.323654 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.333035 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.342313 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.351523 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.360619 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.369476 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.378620 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.387727 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.397584 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.414542 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.424259 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.433502 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.443818 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.458555 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.474882 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.490566 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.499903 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.509513 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.520893 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.535064 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.545446 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.555154 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.565659 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.575582 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.585204 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.596665 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.606109 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.619261 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.631225 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.646517 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.656597 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.670166 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.681352 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.693021 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.705000 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.716816 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.729418 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.740593 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.752046 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.762844 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.774918 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.790021 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.801471 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.813976 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.826284 [3] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.843123 [3] Warning: no training nodes in this partition! Backward fake loss.
20:55:42.779926 [3] proc begin: <DistEnv 3/4 nccl>
20:55:47.996350 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:55:48.014489 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:57:56.487774 [3] proc begin: <DistEnv 3/4 nccl>
20:57:56.580344 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
20:57:56.593441 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:57:57.680624 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.408376 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.422653 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.435771 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.448115 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.457499 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.468801 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.480660 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.491569 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.501166 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.510330 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.520167 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.529678 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.538670 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.547901 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.557392 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.566563 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.581177 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.591949 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.601486 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.610767 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.619864 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.629372 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.638736 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.647745 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.657506 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.670262 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.679764 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.690653 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.700423 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.709928 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.719417 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.728698 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.738102 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.747094 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.757276 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.768453 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.778822 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.788203 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.797889 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.809962 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.824128 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.835971 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.846074 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.855519 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.865246 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.874693 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.884125 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.893346 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.902679 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.914029 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.923225 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.932749 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.942965 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.952786 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.963128 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.972890 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.984361 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.994400 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.003643 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.013083 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.022549 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.032205 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.041918 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.051053 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.061029 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.073643 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.086141 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.095400 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.105016 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.114452 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.124759 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.135910 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.145619 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.155029 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.168244 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.178320 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.187656 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.197066 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.206984 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.216411 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.226483 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.238702 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.247833 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.259512 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.269326 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.279105 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.292223 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.304526 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.317283 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.326008 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.335790 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.345008 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.354112 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.366866 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.376045 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.385598 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.394697 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.408035 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.419269 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.428811 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.439806 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.451700 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.463552 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.473833 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.501111 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.515979 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.529179 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.539418 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.548928 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.558587 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.568289 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.579006 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.590274 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.602220 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.614272 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.628627 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.640602 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.649778 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.659026 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.670504 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.679401 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.688986 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.700610 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.713743 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.725113 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.734522 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.745836 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.755087 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.764713 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.779055 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.790581 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.801287 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.811410 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.827190 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.837003 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.850960 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.860926 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.870173 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.879216 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.888787 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.901151 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.910638 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.921421 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.933462 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.946230 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.954973 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.967597 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.980846 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.989731 [3] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.998762 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.008108 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.027432 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.041638 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.051423 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.060863 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.070314 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.079506 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.088758 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.099916 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.109360 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.118617 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.127762 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.136812 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.145958 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.161986 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.172093 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.182762 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.194410 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.203213 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.212760 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.224048 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.233784 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.243240 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.252690 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.265523 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.275381 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.284365 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.299981 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.310465 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.322611 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.331852 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.341088 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.352936 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.365100 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.377420 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.386428 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.395857 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.405497 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.415140 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.424713 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.437596 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.446930 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.459209 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.470592 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.480895 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.506120 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.519114 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.528702 [3] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.548004 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:48.252741 [3] proc begin: <DistEnv 3/4 nccl>
21:00:48.307885 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
21:00:48.321096 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:00:49.815979 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.559422 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.573549 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.586662 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.596963 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.606516 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.620500 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.632598 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.641968 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.651448 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.660707 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.673534 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.685648 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.696584 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.706183 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.715408 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.725202 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.734760 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.743998 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.753993 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.763198 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.772611 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.783832 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.793067 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.805342 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.814867 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.824353 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.833392 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.842687 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.851811 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.861393 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.874433 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.885235 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.897328 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.906892 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.916243 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.925917 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.937784 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.949118 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.958688 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.968184 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.977744 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.987239 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.996560 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.005965 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.015181 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.024663 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.034435 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.043536 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.052722 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.064285 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.074150 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.083465 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.093018 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.104668 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.113794 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.122876 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.135667 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.152885 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.167946 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.177674 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.187828 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.197411 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.207145 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.216475 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.226968 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.236420 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.245861 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.260844 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.271301 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.287628 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.303229 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.314155 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.323425 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.332642 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.341674 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.351085 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.360121 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.369214 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.378426 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.387747 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.397116 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.406243 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.417463 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.430230 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.441949 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.452942 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.462683 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.472035 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.490554 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.504689 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.515795 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.527806 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.537037 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.546719 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.557222 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.568416 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.577666 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.586923 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.596114 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.605766 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.614897 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.624098 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.633620 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.642824 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.651863 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.661268 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.677125 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.691261 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.703220 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.718827 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.738426 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.754028 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.764667 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.774824 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.783923 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.793149 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.802255 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.812398 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.823211 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.832206 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.841897 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.852074 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.861430 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.873722 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.885001 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.893969 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.903312 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.913883 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.925445 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.937100 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.948015 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.957177 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.966795 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.976134 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.985927 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.997440 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.006491 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.018751 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.030889 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.040197 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.049562 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.058483 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.072287 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.085976 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.095836 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.105750 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.119864 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.129188 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.138486 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.159102 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.172334 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.182912 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.192691 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.201609 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.214069 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.225190 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.233944 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.243085 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.262506 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.276003 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.304579 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.319626 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.329494 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.345383 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.355832 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.364868 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.375871 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.384734 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.394045 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.402993 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.411908 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.421375 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.433547 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.442956 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.452150 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.461808 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.473248 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.482425 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.491363 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.502992 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.514616 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.524209 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.535230 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.544522 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.553571 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.562374 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.571500 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.580571 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.589881 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.599194 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.608212 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.617300 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.626501 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.638294 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.649925 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.659220 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.672130 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.682261 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.692656 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:25.264901 [3] proc begin: <DistEnv 3/4 nccl>
21:07:25.322878 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
21:07:25.332381 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:07:26.517751 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.238706 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.253931 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.266438 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.279398 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.289988 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.299511 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.308950 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.320710 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.334007 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.343199 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.352605 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.362773 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.373891 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.383051 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.394970 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.403776 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.413228 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.423010 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.436624 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.446146 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.461110 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.476293 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.486494 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.496894 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.507762 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.519180 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.528438 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.540267 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.549477 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.558947 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.568308 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.577973 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.586958 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.599335 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.610645 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.619868 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.632570 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.646145 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.655697 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.665621 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.675376 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.684768 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.694640 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.704191 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.715667 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.728988 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.741157 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.753846 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.767403 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.776546 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.786279 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.795553 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.806811 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.816458 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.825666 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.834852 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.844157 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.855305 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.866039 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.882469 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.895942 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.907051 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.916117 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.927077 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.941944 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.951070 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.961122 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.972336 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.985828 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.998858 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.008295 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.020990 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.031877 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.042067 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.053472 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.067936 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.077967 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.090760 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.102220 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.115584 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.129853 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.142954 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.156058 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.165600 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.178108 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.190560 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.204559 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.214405 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.233032 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.245371 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.261317 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.283911 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.296022 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.308616 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.317435 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.328461 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.340070 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.349052 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.372236 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.386740 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.395395 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.407166 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.419738 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.432720 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.443665 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.452907 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.461740 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.474125 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.486298 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.496119 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.504991 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.513705 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.522301 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.531037 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.539803 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.548858 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.557760 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.566359 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.575042 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.583998 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.592631 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.605248 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.619449 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.631920 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.641771 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.650843 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.663189 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.673978 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.690196 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.700828 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.714434 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.728647 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.738935 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.748047 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.757115 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.765861 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.774684 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.783454 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.792182 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.801013 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.810365 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.821970 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.832639 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.842024 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.850844 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.860674 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.870202 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.878709 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.890677 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.899931 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.910360 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.923628 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.935302 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.946518 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.957846 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.969555 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.982372 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.992044 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.000908 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.009743 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.020235 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.030807 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.039375 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.048523 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.057231 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.066469 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.075279 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.084044 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.092807 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.104872 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.116013 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.129452 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.141805 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.154017 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.163061 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.172018 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.186292 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.201515 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.212519 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.222350 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.231209 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.240369 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.251837 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.270817 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.290155 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.305742 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.317503 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.326990 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.336203 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.345282 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.358049 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.381953 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.394469 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.404305 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.413215 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.421746 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.433164 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.446497 [3] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.456908 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:31.451615 [3] proc begin: <DistEnv 3/4 nccl>
21:09:31.560565 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
21:09:31.570196 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:09:32.780657 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.512195 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.527836 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.538676 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.547876 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.557518 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.570204 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.582035 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.591645 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.602786 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.614009 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.623225 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.636597 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.651389 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.662494 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.673571 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.684306 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.694335 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.704606 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.717201 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.729655 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.740870 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.752797 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.762855 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.774790 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.784835 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.796314 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.806019 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.815195 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.830050 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.844679 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.856388 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.869842 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.880064 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.889904 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.901695 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.912839 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.922200 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.934732 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.946500 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.956148 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.969829 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.980597 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.992328 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.005541 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.018116 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.030122 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.039448 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.048490 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.057834 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.067380 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.078328 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.089486 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.098674 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.111491 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.121824 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.132563 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.142572 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.154609 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.163544 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.172752 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.184147 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.193515 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.202854 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.212598 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.222264 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.232503 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.243319 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.254651 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.264658 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.275566 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.285494 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.295156 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.304780 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.322672 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.334997 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.344406 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.356127 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.367025 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.376955 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.390341 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.401511 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.412573 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.424020 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.435460 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.444920 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.466366 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.493996 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.508593 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.525241 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.535197 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.545571 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.558227 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.569584 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.579293 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.593080 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.605626 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.617974 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.634537 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.644907 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.657154 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.671979 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.681106 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.691419 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.701530 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.711271 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.720053 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.729366 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.739200 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.750212 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.760509 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.771462 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.781040 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.790990 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.804697 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.815102 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.825101 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.834519 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.843890 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.854945 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.866185 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.875218 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.884435 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.895691 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.906327 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.918284 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.926897 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.938661 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.947868 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.957206 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.966235 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.977796 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.988730 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.998525 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.007500 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.017294 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.026474 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.038614 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.048169 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.057749 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.070498 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.083459 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.092682 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.102371 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.111445 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.120451 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.130123 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.141799 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.153774 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.164293 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.181330 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.193739 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.206431 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.216244 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.228361 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.240444 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.249669 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.261946 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.277079 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.289707 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.301025 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.310186 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.321250 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.330578 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.340632 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.349717 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.359322 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.370527 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.382635 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.391743 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.401422 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.410611 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.419662 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.429230 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.438656 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.451379 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.473824 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.497183 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.507636 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.517292 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.528457 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.541799 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.551609 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.562327 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.575776 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.585376 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.594527 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.606797 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.616641 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.627256 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.641214 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.653400 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.666329 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.675658 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.684892 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.695315 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.704570 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.719521 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.730180 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.739802 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:50.944576 [3] proc begin: <DistEnv 3/4 nccl>
21:09:50.996742 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
21:09:51.009039 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:09:52.434375 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.178173 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.191679 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.204388 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.217311 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.228367 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.238266 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.247868 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.257315 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.269613 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.278931 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.288311 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.297764 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.307041 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.322372 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.335407 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.345938 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.356714 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.369411 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.381786 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.391258 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.400890 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.410515 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.420129 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.429456 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.439061 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.448428 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.458015 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.470293 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.479846 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.489605 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.500657 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.512899 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.528791 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.542004 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.554778 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.563865 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.572911 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.581744 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.591677 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.610296 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.624163 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.632945 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.641955 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.652637 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.664901 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.673829 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.683660 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.692825 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.702242 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.711083 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.726672 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.738668 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.752228 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.763821 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.773581 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.782569 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.791720 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.800931 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.814359 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.824167 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.840669 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.852107 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.862326 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.875016 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.886886 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.895814 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.908724 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.919444 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.929259 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.938994 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.960125 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.970786 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.980123 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.989427 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.000660 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.009881 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.018852 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.027645 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.036651 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.046680 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.055360 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.064603 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.073723 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.086253 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.096947 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.105908 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.115477 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.128451 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.141816 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.151168 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.162694 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.173642 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.185503 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.195407 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.207533 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.218073 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.227277 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.236222 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.245368 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.254113 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.263285 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.272169 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.281539 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.290415 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.299588 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.308457 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.317441 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.326162 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.335146 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.344028 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.353729 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.366105 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.378765 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.391092 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.403093 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.412038 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.421159 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.429964 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.439066 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.447910 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.458015 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.467721 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.479562 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.492789 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.504224 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.512983 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.521994 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.530673 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.539429 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.552194 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.560686 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.574681 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.586925 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.595760 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.605341 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.614331 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.623255 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.632186 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.641609 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.650351 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.659017 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.668873 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.681197 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.693130 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.702150 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.714775 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.723942 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.733545 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.747219 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.760450 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.773818 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.783405 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.792382 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.803033 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.814699 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.823476 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.834475 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.850985 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.862683 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.871917 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.880808 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.889603 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.898843 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.907576 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.916551 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.925346 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.934306 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.943268 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.968183 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.980585 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.990814 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.999980 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.009517 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.018549 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.027508 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.036394 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.045479 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.054109 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.063062 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.071946 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.080909 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.089708 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.098515 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.107495 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.116578 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.127179 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.136829 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.150004 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.159647 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.168526 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.177568 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.186941 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.199221 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.208061 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.216833 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.225871 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.234644 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.243679 [3] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.255138 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:37.575144 [3] proc begin: <DistEnv 3/4 nccl>
21:12:37.667580 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
21:12:37.677061 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:12:39.042747 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.739988 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.754022 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.763651 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.773082 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.782487 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.792088 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.801845 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.812658 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.825378 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.844084 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.857117 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.866487 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.878035 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.887188 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.896606 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.907080 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.919091 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.928732 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.942448 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.951668 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.963516 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.977393 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.989209 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.998515 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.007986 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.018182 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.027831 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.039970 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.052268 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.066304 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.075509 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.085728 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.095136 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.104439 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.113829 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.123652 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.134970 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.144700 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.154024 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.163404 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.179971 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.194384 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.205952 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.215560 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.226392 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.239221 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.252114 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.261753 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.277025 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.287694 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.297083 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.306128 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.315159 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.324316 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.335942 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.346672 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.356970 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.367290 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.379279 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.392979 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.402044 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.411160 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.420184 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.429516 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.439226 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.450618 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.462977 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.474769 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.485393 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.494916 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.504674 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.517560 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.534668 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.547004 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.558896 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.568109 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.579058 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.587987 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.602010 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.611906 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.623066 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.631953 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.641083 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.650618 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.663383 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.675539 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.685107 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.694623 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.712250 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.723483 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.755605 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.767268 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.776209 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.786039 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.798619 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.807587 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.820093 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.829125 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.838668 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.847961 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.859685 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.868857 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.878831 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.888202 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.897110 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.909256 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.920057 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.929129 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.937943 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.949037 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.957896 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.966619 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.975485 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.987771 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.996529 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.006180 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.016566 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.026097 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.035073 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.045101 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.054799 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.068377 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.078475 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.087268 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.097727 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.106758 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.115394 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.125056 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.137010 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.149371 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.160654 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.170696 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.180304 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.189724 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.199120 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.208274 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.217415 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.230090 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.245044 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.258879 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.268222 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.277626 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.286722 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.295850 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.304869 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.313912 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.326990 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.336663 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.349706 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.361692 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.370673 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.379701 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.388782 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.397743 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.406637 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.416935 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.429707 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.438491 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.447516 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.456313 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.465427 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.477766 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.487569 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.498135 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.508373 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.517607 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.531580 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.540165 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.549270 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.559402 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.568228 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.577584 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.586348 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.595347 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.610800 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.622672 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.631278 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.640423 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.652921 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.663409 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.672236 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.684191 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.695497 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.707266 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.716251 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.727130 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.741918 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.764852 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.778025 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.787376 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.798332 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.808264 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.820871 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.836831 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.847910 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.856966 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.869931 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.881459 [3] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.893060 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:12.613200 [3] proc begin: <DistEnv 3/4 nccl>
21:16:12.651200 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
21:16:12.660750 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:16:13.983181 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.698841 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.713652 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.723012 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.732598 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.744528 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.756693 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.769918 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.779049 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.787723 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.796718 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.807799 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.817044 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.826373 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.835570 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.844480 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.854227 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.863231 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.876147 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.885266 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.897541 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.916190 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.928561 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.937570 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.946929 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.955571 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.964766 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.976124 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.985814 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.999556 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.009765 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.022533 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.033109 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.043796 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.054943 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.065346 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.078448 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.089064 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.099331 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.110803 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.120408 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.132368 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.145258 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.160668 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.173431 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.182992 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.196509 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.205797 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.216668 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.227868 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.242030 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.253450 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.262699 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.272666 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.281858 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.291149 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.300362 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.317946 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.333562 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.342887 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.355998 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.366294 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.375623 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.386088 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.398772 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.410714 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.420127 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.429649 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.438663 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.447676 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.457102 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.466450 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.478694 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.491793 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.502968 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.520743 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.542080 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.552796 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.562251 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.571309 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.580209 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.590875 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.601190 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.612228 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.624660 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.638453 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.651475 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.663335 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.674031 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.698525 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.714881 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.740141 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.753920 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.764383 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.776415 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.786047 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.797022 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.806571 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.819138 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.828789 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.838399 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.847653 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.856853 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.868926 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.881299 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.891334 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.900940 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.912037 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.924043 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.934998 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.944170 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.953725 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.967034 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.981136 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.990688 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.999803 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.009445 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.018484 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.029447 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.039403 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.048419 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.057876 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.069811 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.080534 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.089965 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.101834 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.112960 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.122102 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.131434 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.140426 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.149813 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.165437 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.174942 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.184363 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.194081 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.203105 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.215511 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.224544 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.233740 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.242856 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.252160 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.261341 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.271896 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.281042 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.293500 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.305022 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.316394 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.328422 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.338626 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.347754 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.357282 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.370493 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.380806 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.390111 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.400222 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.412110 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.423646 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.432590 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.442381 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.451309 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.460662 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.470804 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.480099 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.489980 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.499295 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.508400 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.517762 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.526736 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.535825 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.544977 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.554345 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.566093 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.576044 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.585393 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.597651 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.607099 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.616492 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.627034 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.642981 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.653440 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.663410 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.672720 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.682299 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.703004 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.716716 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.726912 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.736234 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.745697 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.755008 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.764635 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.774217 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.783588 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.793396 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.802912 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.812902 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.822384 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.831487 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.840823 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.852324 [3] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.860966 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:42.328614 [3] proc begin: <DistEnv 3/4 nccl>
21:20:42.355773 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
21:20:42.365151 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:20:44.457517 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.144525 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.159292 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.169945 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.181262 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.192274 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.202716 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.216149 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.225305 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.238608 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.251703 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.263103 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.272419 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.284404 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.295417 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.309007 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.321257 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.334454 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.349681 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.359414 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.369240 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.379943 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.389673 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.398721 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.408413 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.421231 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.430511 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.439913 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.449013 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.458182 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.469161 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.478785 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.488339 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.500217 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.510727 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.520256 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.529758 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.538880 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.548329 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.560606 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.577864 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.590888 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.600345 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.612338 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.622847 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.632790 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.642005 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.651432 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.661041 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.670726 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.681827 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.692245 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.702537 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.713151 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.723935 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.736397 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.746260 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.755376 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.764989 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.778112 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.788976 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.799122 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.809368 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.819410 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.830234 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.840506 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.855408 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.871672 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.884211 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.895813 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.908785 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.920074 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.931169 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.941704 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.952188 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.965283 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.976095 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.995606 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.009191 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.022508 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.033408 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.045044 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.063373 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.078530 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.090319 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.101614 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.112684 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.124301 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.137228 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.157432 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.171323 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.183870 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.194168 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.211805 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.230031 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.239769 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.248403 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.259639 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.272297 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.283815 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.302898 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.314044 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.333022 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.345685 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.365477 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.378301 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.387626 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.396214 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.407835 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.417057 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.426479 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.435259 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.445489 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.454491 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.464179 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.473224 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.482256 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.493718 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.508196 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.520339 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.530447 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.539293 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.548295 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.560889 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.572202 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.581398 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.593749 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.603754 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.615728 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.625361 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.634180 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.643007 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.651773 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.663191 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.673324 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.683087 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.691668 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.700978 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.710181 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.718821 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.727980 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.737351 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.749503 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.761330 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.770426 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.779221 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.788122 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.797270 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.809619 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.818596 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.827557 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.842659 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.854726 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.864033 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.873014 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.881809 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.891253 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.900315 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.909424 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.921479 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.930577 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.939709 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.948878 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.960701 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.969682 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.978513 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.988078 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.999432 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.010595 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.019846 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.029551 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.041772 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.051095 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.063186 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.078034 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.086498 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.095369 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.104210 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.112995 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.121750 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.130325 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.139143 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.148076 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.157115 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.165837 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.174655 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.183221 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.192313 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.201464 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.210496 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.233371 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.243679 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.252892 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.262080 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.272795 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.281281 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.293379 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.306134 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.314586 [3] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.323584 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:33.343924 [3] proc begin: <DistEnv 3/4 nccl>
20:31:45.819423 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:31:45.835941 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:33:07.294560 [3] proc begin: <DistEnv 3/4 nccl>
20:33:12.972124 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:33:12.990564 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:59:42.849738 [3] proc begin: <DistEnv 3/4 nccl>
21:00:01.568356 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
21:00:01.583749 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:00:09.423342 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:10.974939 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:11.746759 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:12.520260 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:13.293005 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:14.065443 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:14.840552 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:15.611260 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:16.382090 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:17.154535 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:17.924985 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:18.698297 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:19.470639 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:20.240591 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:21.010946 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:21.783540 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:22.553547 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:23.322826 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:24.091922 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:24.859921 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:25.628474 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:26.395785 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:27.166761 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:27.934064 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:28.702972 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:29.471740 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:30.240764 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:31.008770 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:31.779233 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:32.587061 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:33.359715 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:34.129262 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:34.895547 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:35.664391 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:36.432054 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:37.196949 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:37.961729 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:38.727769 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:39.494741 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:40.263220 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:41.033863 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:41.805056 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:42.577275 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:43.347811 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:44.119056 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:44.889888 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:45.661406 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:46.431004 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:47.202740 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:47.974647 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:48.747136 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:49.517371 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.287366 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.056952 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.826866 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.597122 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:53.367487 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:54.141010 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:54.912108 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:55.684344 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:56.455203 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:57.224583 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:57.995335 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:58.766878 [3] Warning: no training nodes in this partition! Backward fake loss.
21:00:59.537291 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:00.308460 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:01.078765 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:01.851900 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:02.657285 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:03.459058 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:04.233369 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:05.003142 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:05.771542 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:06.539305 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:07.307852 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:08.076521 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:08.844020 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:09.716627 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:10.489041 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:11.261705 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:12.028400 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:12.795681 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:13.561486 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:14.327763 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:15.093162 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:15.862874 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:16.629548 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:17.402480 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:18.173597 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:18.951361 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:19.719261 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:20.491591 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:21.264044 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:22.036260 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:22.808909 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:23.581675 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:24.353561 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:25.125959 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:25.899331 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:26.672000 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:27.444080 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:28.215959 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:28.989091 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:29.759760 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:30.531137 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:31.302101 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:32.073520 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:32.842465 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:33.611951 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:34.382609 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:35.152477 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:35.922414 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:36.692843 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:37.464191 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:38.234537 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:39.004326 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:39.779109 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:40.548252 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:41.316919 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:42.085783 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:42.855500 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:43.624845 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:44.395752 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:45.429463 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:46.200960 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:46.971559 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:47.739280 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:48.506972 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:49.273476 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:50.037912 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:50.802009 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:51.565424 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:52.332133 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:53.096826 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:53.865982 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:54.635222 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:55.404060 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:56.174359 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:56.944247 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:57.714554 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:58.484076 [3] Warning: no training nodes in this partition! Backward fake loss.
21:01:59.254596 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:00.025391 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:00.796469 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:01.572432 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:02.379563 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:03.173660 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:03.944971 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:04.716193 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:05.488390 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:06.259141 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:07.031631 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:07.802835 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:08.574168 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:09.345136 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:10.113899 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:10.883745 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:11.654701 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:12.426494 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:13.197448 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:13.968244 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:14.739273 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:15.507817 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:16.275386 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:17.047818 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:17.819943 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:18.588429 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:19.357092 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:20.126642 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:20.897381 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:21.666390 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:22.644425 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:23.798924 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:24.569520 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:25.339724 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:26.106073 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:26.872553 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:27.637801 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:28.402678 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:29.168356 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:29.933495 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:30.699987 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:31.466459 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:32.236024 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:33.006702 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:33.778909 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:34.549465 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:35.320094 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:36.090975 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:36.862278 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:37.633495 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:38.405025 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:39.177128 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:39.948664 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:40.720691 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:41.492351 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:42.264634 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:43.037144 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:43.809131 [3] Warning: no training nodes in this partition! Backward fake loss.
21:02:44.580794 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:05.818400 [3] proc begin: <DistEnv 3/4 nccl>
14:32:23.236271 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:32:23.253379 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:32:32.107164 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:33.726672 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:34.494065 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:35.261797 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:36.030113 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:36.797346 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:37.565764 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:38.332828 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:39.102771 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:39.869296 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:40.636998 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:41.405802 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:42.174794 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:42.943858 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:43.712525 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:44.481788 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:45.249581 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:46.017763 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:46.785764 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:47.554653 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:48.322507 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:49.090159 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:49.860490 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:50.630171 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:51.399591 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:52.167122 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:52.935717 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:53.704248 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:54.471787 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:55.239988 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:56.007520 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:56.775230 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:57.543725 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:58.311747 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:59.080589 [3] Warning: no training nodes in this partition! Backward fake loss.
14:32:59.849083 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:00.616185 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:01.384441 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:02.155090 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:02.955526 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:03.748043 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:04.515597 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:05.282968 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:06.049234 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:06.817053 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:07.585934 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:08.353612 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:09.121672 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:09.890704 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:10.659050 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:11.427198 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:12.195684 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:12.965186 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:13.733806 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:14.503367 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:15.274338 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:16.044408 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:16.814397 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:17.586162 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:18.358458 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:19.127363 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:19.897988 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:20.669754 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:21.439268 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:22.207858 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:22.976736 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:23.746517 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:24.515535 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:25.280338 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:26.045775 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:26.810868 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:27.576215 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:28.341695 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:29.106506 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:29.871339 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:30.638175 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:31.402732 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:32.167812 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:32.934156 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:33.701672 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:34.468622 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:35.235949 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:36.002725 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:36.769377 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:37.535442 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:38.302009 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:39.068622 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:39.834878 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:40.610086 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:41.449501 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:42.468054 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:43.718967 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:45.019066 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:46.342018 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:47.670606 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:48.998527 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:50.323376 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:51.650791 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:52.972308 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:54.293782 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:55.621005 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:56.951482 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:58.281851 [3] Warning: no training nodes in this partition! Backward fake loss.
14:33:59.616861 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:00.952384 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:01.793438 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:02.595633 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:03.386306 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:04.153192 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:04.920721 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:05.686536 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:06.452492 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:07.218987 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:07.985318 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:08.751935 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:09.518920 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:10.285276 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:11.051745 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:11.817935 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:12.585215 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:13.352374 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:14.119243 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:14.886353 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:15.654549 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:16.421437 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:17.189320 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:17.957668 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:18.725768 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:19.494218 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:20.263980 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:21.031229 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:21.798935 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:22.566251 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:23.333626 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:24.100216 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:24.866952 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:25.633191 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:26.399346 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:27.163940 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:27.929842 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:28.695501 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:29.461629 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:30.228076 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:30.993982 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:31.761626 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:32.528132 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:33.293910 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:34.061036 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:34.829605 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:35.597399 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:36.365111 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:37.133562 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:37.901941 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:38.670668 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:39.439409 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:40.208253 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:40.976111 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:41.744754 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:42.513707 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:43.281220 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:44.049718 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:44.817314 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:45.586593 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:46.353720 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:47.121565 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:47.888868 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:48.656814 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:49.424000 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:50.192631 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:50.962789 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:51.731810 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:52.501761 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:53.271440 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:54.043119 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:54.812239 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:55.579528 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:56.346572 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:57.113714 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:57.880261 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:58.647016 [3] Warning: no training nodes in this partition! Backward fake loss.
14:34:59.413893 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:00.179883 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:00.946782 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:01.730935 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:02.534295 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:03.314841 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:04.083318 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:04.852277 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:05.622047 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:06.390678 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:07.160184 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:07.929026 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:08.698917 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:09.467439 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:10.236061 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:11.004874 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:11.773958 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:12.543280 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:13.309650 [3] Warning: no training nodes in this partition! Backward fake loss.
14:35:14.077448 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:04.091272 [3] proc begin: <DistEnv 3/4 nccl>
14:40:09.099525 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:40:09.117018 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:40:14.495123 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:16.129762 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:17.000737 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:17.868379 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:18.738091 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:19.608362 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:20.481925 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:21.350135 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:22.221971 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:23.093529 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:23.962978 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:24.831846 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:25.702532 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:26.573970 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:27.445052 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:28.316120 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:29.186909 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:30.055933 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:30.923547 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:31.792721 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:32.661065 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:33.530619 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:34.399177 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:35.267861 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:36.135652 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:37.002604 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:37.871125 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:38.739209 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:39.605400 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:40.471730 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:41.338521 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:42.205614 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:43.074564 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:43.945237 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:44.811928 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:45.679474 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:46.549096 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:47.416886 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:48.285522 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:49.155875 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:50.025120 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:50.901761 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:51.771639 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:52.639808 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:53.513362 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:54.386422 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:55.258808 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:56.131877 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:57.006210 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:57.876230 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:58.751040 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:59.625742 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:00.498312 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:01.372154 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:02.281281 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:03.181453 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:04.052516 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:04.924334 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:05.795560 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:06.665866 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:07.537871 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:08.410449 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:09.282832 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:10.155550 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:11.027537 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:11.900335 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:12.772543 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:13.646392 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:14.517468 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:15.391479 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:16.264404 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:17.136158 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:18.008967 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:18.880892 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:19.752710 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:20.625075 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:21.494622 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:22.363377 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:23.234159 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:24.103082 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:24.973690 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:25.843832 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:26.712555 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:27.578297 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:28.446203 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:29.314674 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:30.185474 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:31.056177 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:31.927922 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:32.800264 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:33.672113 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:34.543400 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:35.416769 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:36.289962 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:37.163565 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:38.036875 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:38.911068 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:39.784067 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:40.656738 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:41.529805 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:42.400595 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:43.274703 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:44.147692 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:45.021292 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:45.895026 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:46.766176 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:47.638605 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:48.511692 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:49.378593 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:50.244837 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:51.111281 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:51.976443 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:52.842146 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:53.710940 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:54.580534 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:55.449742 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:56.321278 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:57.191110 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:58.059044 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:58.927430 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:59.795171 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:00.664834 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:01.532449 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:02.423344 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:03.335207 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:04.210690 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:05.082901 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:05.955749 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:06.827873 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:07.702971 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:08.574928 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:09.444817 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:10.312893 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:11.180355 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:12.047903 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:12.913232 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:13.777799 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:14.643136 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:15.508319 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:16.373298 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:17.239491 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:18.105082 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:18.970098 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:19.835292 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:20.701368 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:21.568387 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:22.437470 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:23.304720 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:24.171929 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:25.039197 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:25.905914 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:26.772908 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:27.640019 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:28.506933 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:29.374146 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:30.241320 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:31.108776 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:31.976145 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:32.842244 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:33.709775 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:34.577518 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:35.442504 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:36.308501 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:37.174106 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:38.039258 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:38.904418 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:39.769121 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:40.634866 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:41.503224 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:42.373757 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:43.241967 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:44.110535 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:44.979455 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:45.847434 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:46.716080 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:47.584679 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:48.456064 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:49.329993 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:50.202645 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:51.075989 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:51.949607 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:52.821745 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:53.694655 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:54.568740 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:55.438994 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:56.311525 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:57.186309 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:58.059633 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:58.932483 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:59.807949 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:00.679302 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:01.553127 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:02.465729 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:03.364938 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:04.237947 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:05.112852 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:05.986283 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:06.859941 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:07.732475 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:08.603978 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:54.985741 [3] proc begin: <DistEnv 3/4 nccl>
14:44:59.442731 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:44:59.465552 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:45:05.622071 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:07.206877 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:08.082247 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:08.957447 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:09.833064 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:10.709203 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:11.585732 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:12.461120 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:13.335643 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:14.212333 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:15.084061 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:15.956123 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:16.828536 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:17.698035 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:18.568201 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:19.441266 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:20.314892 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:21.186618 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:22.057232 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:22.928180 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:23.800708 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:24.671269 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:25.541754 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:26.411662 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:27.282619 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:28.155220 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:29.028033 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:29.900311 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:30.773094 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:31.647982 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:32.521499 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:33.394496 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:34.267962 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:35.141520 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:36.014085 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:36.887508 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:37.762093 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:38.632065 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:39.504027 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:40.377438 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:41.249268 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:42.124385 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:42.997269 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:43.869227 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:44.739834 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:45.614074 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:46.487767 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:47.362149 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:48.234911 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.108287 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.978891 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.853245 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.727263 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:52.599619 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:53.473604 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:54.344887 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:55.216513 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:56.089336 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:56.960085 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:57.832375 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:58.704378 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:59.578200 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:00.452728 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:01.328033 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:02.240664 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:03.134356 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:04.012173 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:04.889172 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:05.764190 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:06.636702 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:07.511688 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:08.385580 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:09.259763 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:10.135002 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:11.010302 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:11.886392 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:12.762499 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:13.635929 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:14.513991 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:15.389262 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:16.261864 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:17.132217 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:18.002101 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:18.874171 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:19.747227 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:20.622077 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:21.496054 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:22.372080 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:23.248900 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:24.127042 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:25.002874 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:25.878768 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:26.750338 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:27.619540 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:28.487113 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:29.354244 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:30.221477 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:31.090486 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:31.961251 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:32.830951 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:33.699667 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:34.568460 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:35.439782 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:36.310189 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:37.179381 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:38.049477 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:38.918283 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:39.786770 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:40.655917 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:41.525013 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:42.396301 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:43.268704 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:44.138558 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:45.007772 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:45.878591 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:46.747687 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:47.616776 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:48.485961 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:49.355938 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:50.225590 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:51.095037 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:51.963832 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:52.831515 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:53.699944 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:54.568772 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:55.437006 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:56.305161 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:57.175836 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:58.046436 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:58.919843 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:59.792676 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:00.663511 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:01.535887 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:02.433746 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:03.350716 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:04.225954 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:05.098347 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:05.970540 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:06.844672 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:07.716840 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:08.587480 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:09.457874 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:10.326727 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:11.195978 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:12.065849 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:12.935091 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:13.803687 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:14.672623 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:15.541438 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:16.411534 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:17.281011 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:18.150252 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:19.018656 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:19.886853 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:20.755244 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:21.626455 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:22.496365 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:23.364314 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:24.231871 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:25.099639 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:25.969547 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:26.841606 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:27.712423 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:28.582737 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:29.454058 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:30.325745 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:31.197722 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:32.070563 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:32.941954 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:33.814134 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:34.686060 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:35.556925 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:36.426111 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:37.302037 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:38.172318 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:39.042742 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:39.914160 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:40.784681 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:41.654385 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:42.524848 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:43.393603 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:44.263467 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:45.133891 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:46.003305 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:46.877870 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:47.748542 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:48.617827 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:49.488642 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:50.360872 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:51.233798 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:52.105972 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:52.978049 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:53.847975 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:54.718162 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:55.590114 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:56.462694 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:57.337254 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:58.210984 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:59.084422 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:59.955251 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:13.700677 [3] proc begin: <DistEnv 3/4 nccl>
14:59:18.227477 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:59:18.254855 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:59:23.872618 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:25.792786 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:27.108915 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:28.429455 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:29.749714 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:31.064975 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:32.373142 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:33.683918 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:34.993640 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:36.309851 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:37.620365 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:38.930509 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:40.238150 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:41.542426 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:42.847193 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:44.151067 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:45.456261 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:46.760982 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:48.064093 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:49.369793 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:50.674292 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:51.979455 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:53.284684 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:54.589599 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:55.894236 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:57.197434 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:58.503901 [3] Warning: no training nodes in this partition! Backward fake loss.
14:59:59.809772 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:01.115323 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:02.463111 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:03.780148 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:05.085445 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:06.389946 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:07.695731 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:08.999471 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.303413 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.614269 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:12.390881 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:13.153199 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:13.915014 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:14.676816 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:15.438664 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:16.200719 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:16.961839 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:17.721860 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:18.482965 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:19.244275 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:20.005032 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:20.765591 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:21.526234 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:22.286298 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:23.048019 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:23.807782 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:24.568300 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:25.328201 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:26.088468 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:26.848234 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.609062 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:28.368946 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:29.129081 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:29.888415 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:30.650587 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:31.409930 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:32.169576 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:32.928742 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:33.687786 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:34.446379 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:35.205401 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:35.964688 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:36.724996 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:37.484251 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:38.243001 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:39.002206 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:39.762533 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:40.523071 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:41.283461 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:42.044347 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:42.804143 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:43.564609 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:44.324967 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:45.085570 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:45.846941 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:46.607172 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:47.367119 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:48.127617 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:48.887836 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:49.648023 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:50.408454 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:51.168630 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:51.928543 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:52.692347 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:53.452878 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:54.215792 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:54.979104 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:55.740663 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:56.503476 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:57.263861 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:58.033002 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:58.795140 [3] Warning: no training nodes in this partition! Backward fake loss.
15:00:59.556479 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:00.318358 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:01.080336 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:01.843043 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:02.633524 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:03.428549 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:04.189620 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:04.950896 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:05.712805 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:06.486498 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:07.261863 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:08.038642 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:08.814524 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:09.592345 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:10.368614 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:11.144619 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:11.922209 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:12.697917 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:13.474449 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:14.250761 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:15.026829 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:15.802200 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:16.578403 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:17.355630 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:18.130198 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:18.906240 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:19.681251 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:20.456740 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:21.230554 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:22.006037 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:22.779559 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:23.555287 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:24.330272 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:25.105348 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:25.879880 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:26.653939 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:27.428965 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:28.202648 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:28.976929 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:29.752037 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:30.526871 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:31.301772 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:32.075890 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:32.851078 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:33.626630 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:34.402997 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:35.178003 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:35.953970 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:36.730079 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:37.505700 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:38.281157 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:39.055702 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:39.831111 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:40.606659 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:41.382412 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:42.158193 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:42.933717 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:43.709043 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:44.485783 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:45.261672 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:46.037921 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:46.814007 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:47.589829 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:48.365210 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:49.140678 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:49.917519 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:50.692673 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:51.469090 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:52.244764 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:53.021164 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:53.798958 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:54.575382 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:55.351048 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:56.126878 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:56.901965 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:57.678150 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.453839 [3] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.229365 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:00.005664 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:00.782459 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:01.558251 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:02.364943 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:03.168496 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:03.944371 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:04.720978 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:05.497002 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:06.273299 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:07.049837 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:07.825501 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:08.601273 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:09.378060 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:10.155197 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:10.931878 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:11.708109 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:12.483432 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:13.259016 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:14.034790 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:14.811217 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:15.587846 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:16.363606 [3] Warning: no training nodes in this partition! Backward fake loss.
15:02:17.139821 [3] Warning: no training nodes in this partition! Backward fake loss.
14:03:56.425074 [3] proc begin: <DistEnv 3/4 nccl>
14:04:12.621113 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:04:12.651518 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:04:24.052639 [3] Warning: no training nodes in this partition! Backward fake loss.
14:04:25.685073 [3] Warning: no training nodes in this partition! Backward fake loss.
14:04:26.446390 [3] Warning: no training nodes in this partition! Backward fake loss.
14:04:27.208328 [3] Warning: no training nodes in this partition! Backward fake loss.
14:04:27.970620 [3] Warning: no training nodes in this partition! Backward fake loss.
14:04:28.733261 [3] Warning: no training nodes in this partition! Backward fake loss.
14:04:29.494447 [3] Warning: no training nodes in this partition! Backward fake loss.
14:04:30.256532 [3] Warning: no training nodes in this partition! Backward fake loss.
14:04:31.017965 [3] Warning: no training nodes in this partition! Backward fake loss.
14:04:31.779479 [3] Warning: no training nodes in this partition! Backward fake loss.
14:04:32.541400 [3] Warning: no training nodes in this partition! Backward fake loss.
14:04:33.302022 [3] Warning: no training nodes in this partition! Backward fake loss.
14:04:34.064547 [3] Warning: no training nodes in this partition! Backward fake loss.
14:04:34.825600 [3] Warning: no training nodes in this partition! Backward fake loss.
14:04:35.588410 [3] Warning: no training nodes in this partition! Backward fake loss.
14:04:36.352751 [3] Warning: no training nodes in this partition! Backward fake loss.
14:04:37.116630 [3] Warning: no training nodes in this partition! Backward fake loss.
14:04:37.880453 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:19.470285 [3] proc begin: <DistEnv 3/4 nccl>
14:09:24.513838 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:09:24.533083 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:09:29.917361 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:31.800655 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:32.565113 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:33.327584 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:34.090687 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:34.854678 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:35.617410 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:36.379745 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:37.143093 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:37.906315 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:38.669658 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:39.431731 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:40.194139 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:40.956044 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:41.720714 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:42.483855 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:43.246904 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:44.010061 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:44.774345 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:45.537392 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:46.300277 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:47.063231 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:47.825756 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:48.587286 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:49.349716 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:50.114071 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:16.792902 [3] proc begin: <DistEnv 3/4 nccl>
14:10:22.347785 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:10:22.367653 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:10:28.616231 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:30.536920 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:31.302945 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:32.068649 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:32.835165 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:33.602710 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:34.368504 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:35.134557 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:35.900733 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:36.664565 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:37.428713 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:38.194019 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:38.958780 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:39.724095 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:40.489485 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:41.254125 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:42.018684 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:42.784541 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:43.549263 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:44.314915 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:45.080124 [3] Warning: no training nodes in this partition! Backward fake loss.
14:18:18.324433 [3] proc begin: <DistEnv 3/4 nccl>
14:18:52.370246 [3] proc begin: <DistEnv 3/4 nccl>
14:18:56.817372 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
14:18:56.851234 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:34:48.552046 [3] proc begin: <DistEnv 3/4 nccl>
14:34:54.313347 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
14:34:54.332176 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:43:14.487535 [3] proc begin: <DistEnv 3/4 nccl>
14:43:18.988781 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:43:19.002874 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:43:23.925518 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:26.622602 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:27.954696 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:29.360211 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:30.690988 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:32.033960 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:33.304072 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:34.464561 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:35.856022 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:37.181317 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:38.332249 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:39.696429 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:41.086287 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:42.423142 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:43.692147 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:45.036170 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:46.374570 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:47.711592 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:49.049630 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:50.388090 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:51.719848 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:53.055084 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:54.387857 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:55.800583 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:57.133251 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:58.468286 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:59.807623 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:01.141421 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:02.501580 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:03.707414 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:05.075255 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:06.407259 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:07.739839 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:08.913847 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:10.188974 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:11.567796 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:12.909057 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:14.250396 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:15.595899 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:16.933134 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:18.267255 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:19.603205 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:20.942465 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:22.273318 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:23.609518 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:24.947933 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:26.291126 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:27.621985 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:28.952197 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:30.291736 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:31.623155 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:32.955991 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:34.287911 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:35.626733 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:36.780103 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:38.123152 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:39.468322 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:40.811028 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.000235 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.401372 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:44.707747 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:46.055575 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:47.397617 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:48.701740 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:49.894410 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:51.236236 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:52.579023 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:53.799059 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:55.148686 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:56.495356 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:57.843389 [3] Warning: no training nodes in this partition! Backward fake loss.
14:44:59.191665 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:00.533841 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:01.874140 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:03.310853 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:04.643188 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:05.981145 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:07.319973 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:08.656498 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:09.991466 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:11.322512 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:12.586363 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:13.814065 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:15.173098 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:16.520184 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:17.716547 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:19.092630 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:20.438233 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:21.658084 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:23.044409 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:24.340611 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:25.683729 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:27.027731 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:28.373351 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:29.722281 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:31.064481 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:32.404468 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:33.742521 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:35.079728 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:36.412335 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:37.642363 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:39.025865 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:40.299750 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:41.513091 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:42.875760 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:44.219479 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:45.566062 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:46.910867 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:48.255409 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.601442 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.939945 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:52.272224 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:53.612698 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:54.953801 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:56.295166 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:57.629039 [3] Warning: no training nodes in this partition! Backward fake loss.
14:45:58.889451 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:00.105820 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:01.483711 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:02.727842 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:04.154052 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:05.424319 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:06.771566 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:08.118627 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:09.282792 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:10.679693 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:12.007433 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:13.169822 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:14.576367 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:15.839777 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:17.188135 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:18.448311 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:19.731893 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:21.102853 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:22.450139 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:23.613397 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:25.009719 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:26.353270 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:27.623409 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:28.968144 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:30.308108 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:31.607437 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:32.797619 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:34.137136 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:35.384277 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:36.725807 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:38.060478 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:39.404773 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:40.749192 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:42.092209 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:43.249449 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:44.583900 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:45.927658 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:47.174586 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:48.441779 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:49.841812 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:51.183289 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:52.526911 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:53.867481 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:55.205205 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:56.538319 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:57.872896 [3] Warning: no training nodes in this partition! Backward fake loss.
14:46:59.206472 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:00.545751 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:01.982004 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:03.320409 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:04.630064 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:05.965346 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:07.301722 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:08.635219 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:09.968878 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:11.304258 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:12.638547 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:13.973177 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:15.304759 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:16.642946 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:17.981615 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:19.324944 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:20.642855 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:21.956092 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:23.312602 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:24.641540 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:25.911978 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:27.272502 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:28.630103 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:29.956340 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:31.268953 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:32.584632 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:33.811625 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:35.213688 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:36.516605 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:37.862243 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:39.208681 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:40.554732 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:41.898396 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:43.242103 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:44.586521 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:45.926821 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:47.137933 [3] Warning: no training nodes in this partition! Backward fake loss.
14:47:48.526817 [3] Warning: no training nodes in this partition! Backward fake loss.
16:31:49.855372 [3] proc begin: <DistEnv 3/4 nccl>
16:31:53.983054 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
16:31:53.999646 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:31:59.444345 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:01.607416 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:03.034771 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:04.368292 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:05.731584 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:07.001751 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:08.385283 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:09.630046 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:11.064757 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:12.408588 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:13.748384 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:15.088616 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:16.438319 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:17.778296 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:19.117939 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:20.458468 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:21.860437 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:23.186080 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:24.511888 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:25.837514 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:27.053803 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:28.425395 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:29.639068 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:31.031978 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:32.336745 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:33.677477 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:35.019782 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:36.361231 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:37.704115 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:38.877325 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:40.263422 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:41.590305 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:42.806897 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:44.176410 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:45.517800 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:46.858122 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:48.207373 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:49.548085 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:50.893621 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:52.316960 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:53.642659 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:54.943281 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:56.151891 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:57.537716 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:58.730633 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:00.138144 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:01.462114 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:02.830351 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:04.164819 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:05.502654 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:06.846178 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:08.181378 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:09.510608 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:10.837955 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:12.174584 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:13.506705 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:14.834245 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:16.175093 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:17.508444 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:18.835167 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:19.978703 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:21.385763 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:22.748659 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:23.920681 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:25.342624 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:26.681793 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:28.025476 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:29.198235 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:30.600326 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:31.929210 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:33.118177 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:34.530915 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:35.873900 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:37.219640 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:38.564580 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:39.902768 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:41.245009 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:42.595356 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:43.934042 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:45.270984 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:46.682575 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:48.009925 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:49.337009 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:50.668824 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:52.001146 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:53.189959 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:54.586802 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:55.918568 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:57.267086 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:58.661736 [3] Warning: no training nodes in this partition! Backward fake loss.
16:33:59.953960 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:01.294325 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:02.727477 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:04.083394 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:05.274952 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:06.710599 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:08.049367 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:09.344976 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:10.686147 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:12.027125 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:13.375037 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:14.718626 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:16.060771 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:17.404482 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:18.638820 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:20.051528 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:21.392561 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:22.593490 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:24.030903 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:25.311334 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:26.657291 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:27.870983 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:29.268154 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:30.582706 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:31.778819 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:33.218472 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:34.553488 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:35.896776 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:37.038350 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:38.461179 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:39.792219 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:41.039736 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:42.422733 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:43.765623 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:44.986780 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:46.381168 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:47.688146 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:48.929747 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:50.331387 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:51.657404 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:52.927417 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:54.212817 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:55.593012 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:56.753711 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:58.144091 [3] Warning: no training nodes in this partition! Backward fake loss.
16:34:59.447429 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:00.699265 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:02.083504 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:03.511998 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:04.728011 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:06.110266 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:07.452254 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:08.804096 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:10.043198 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:11.444482 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:12.770458 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:14.109431 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:15.447428 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:16.785144 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:18.139265 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:19.478365 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:20.817958 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:22.150079 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:23.475748 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:24.802975 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:26.129435 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:27.297243 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:28.681906 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:29.984382 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:31.256871 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:32.650929 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:33.989504 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:35.179175 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:36.574859 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:37.755817 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:39.160876 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:40.501096 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:41.843540 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:43.185683 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:44.529357 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:45.783195 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:47.117805 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:48.527979 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:49.709687 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:51.028590 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:52.442802 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:53.769872 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:55.112038 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:56.430300 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:57.645267 [3] Warning: no training nodes in this partition! Backward fake loss.
16:35:59.057721 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:00.243136 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:01.689729 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:03.072449 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:04.414467 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:05.752555 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:07.094954 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:08.435809 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:09.772236 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:11.100291 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:12.427089 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:13.541208 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:14.956210 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:16.281035 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:17.626661 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:18.833783 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:20.292610 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:21.637393 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:22.978593 [3] Warning: no training nodes in this partition! Backward fake loss.
16:36:24.317843 [3] Warning: no training nodes in this partition! Backward fake loss.
19:15:53.726028 [3] proc begin: <DistEnv 3/4 nccl>
19:15:59.005498 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
19:15:59.031713 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:16:55.770356 [3] proc begin: <DistEnv 3/4 nccl>
19:17:01.364194 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
19:17:01.387722 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:30:46.359786 [3] proc begin: <DistEnv 3/4 nccl>
19:30:52.082378 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
19:30:52.100721 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:32:54.876458 [3] proc begin: <DistEnv 3/4 nccl>
19:32:59.865117 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
19:32:59.887282 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:33:51.378571 [3] proc begin: <DistEnv 3/4 nccl>
19:33:56.920840 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
19:33:56.934619 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:35:02.074027 [3] proc begin: <DistEnv 3/4 nccl>
19:35:08.556424 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
19:35:08.576414 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:46:47.248238 [3] proc begin: <DistEnv 3/4 nccl>
22:46:50.941299 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
22:46:50.958771 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

08:22:44.452131 [3] proc begin: <DistEnv 3/4 nccl>
08:22:49.090153 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
08:22:49.432642 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

08:22:55.826819 [3] Warning: no training nodes in this partition! Backward fake loss.
08:22:57.645313 [3] Warning: no training nodes in this partition! Backward fake loss.
08:22:58.353659 [3] Warning: no training nodes in this partition! Backward fake loss.
08:22:59.059488 [3] Warning: no training nodes in this partition! Backward fake loss.
08:22:59.766814 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:00.474555 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:01.184982 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:01.924816 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:02.662117 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:03.375493 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:04.086252 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:04.794252 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:05.502806 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:06.210739 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:06.917955 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:07.625918 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:08.333497 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:09.041410 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:09.748823 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:10.453606 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:11.160112 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:11.865630 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:12.571987 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:13.278654 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:13.983667 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:14.688185 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:15.395896 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:16.101046 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:16.806963 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:17.511990 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:18.218693 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:18.925353 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:19.631791 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:20.336890 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:21.042458 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:21.747818 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:22.453777 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:23.159265 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:23.863684 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:24.570002 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:25.276930 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:25.981911 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:26.686857 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:27.391903 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:28.096893 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:28.802404 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:29.507001 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:30.212929 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:30.918210 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:31.622881 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:32.329683 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:32.798699 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:33.554725 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:34.023513 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:34.781804 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:35.249709 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:36.005909 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:36.473894 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:37.231112 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:37.698778 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:38.455204 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:38.922848 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:39.677755 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:40.144925 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:40.900119 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:41.368461 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:42.123007 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:42.590592 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:43.346698 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:43.814298 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:44.570004 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:45.036832 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:45.792203 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:46.259256 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:47.014461 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:47.481730 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:48.237293 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:48.704798 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:49.461134 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:49.928154 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:50.683712 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:51.151212 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:51.907268 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:52.374049 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:53.128798 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:53.596223 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:54.352593 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:54.819540 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:55.576830 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:56.043640 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:56.798961 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:57.266645 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:58.022095 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:58.488273 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:59.242901 [3] Warning: no training nodes in this partition! Backward fake loss.
08:23:59.709325 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:00.463848 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:00.929650 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:01.685413 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:02.159416 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:02.942976 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:03.425467 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:04.186106 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:04.655597 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:05.416890 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:05.886431 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:06.646282 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:07.115720 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:07.875781 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:08.344537 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:09.105013 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:09.574285 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:10.333750 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:10.804136 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:11.564898 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:12.034456 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:12.795814 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:13.265886 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:14.024687 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:14.492261 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:15.248955 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:15.716218 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:16.474019 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:16.942930 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:17.700152 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:18.167698 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:18.926579 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:19.393788 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:20.149835 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:20.616338 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:21.373659 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:21.840711 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:22.596791 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:23.063574 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:23.819788 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:24.285932 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:25.040717 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:25.506334 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:26.260095 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:26.725799 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:27.480753 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:27.948003 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:28.702010 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:29.168476 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:29.922855 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:30.388967 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:31.144141 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:31.609965 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:32.363771 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:32.829838 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:33.585092 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:34.051009 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:34.805830 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:35.271505 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:36.026176 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:36.493245 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:37.248107 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:37.713973 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:38.470039 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:38.936006 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:39.690109 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:40.156276 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:40.911005 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:41.376941 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:42.132029 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:42.598806 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:43.354605 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:43.820714 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:44.575967 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:45.042170 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:45.796713 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:46.262911 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:47.017574 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:47.483370 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:48.238605 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:48.704734 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:49.461155 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:49.928017 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:50.682136 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:51.147801 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:51.902288 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:52.368424 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:53.123667 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:53.589608 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:54.344686 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:54.810586 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:55.566008 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:56.031777 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:56.786378 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:57.252051 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:58.007560 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:58.473284 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:59.227752 [3] Warning: no training nodes in this partition! Backward fake loss.
08:24:59.693663 [3] Warning: no training nodes in this partition! Backward fake loss.
08:25:00.449096 [3] Warning: no training nodes in this partition! Backward fake loss.
08:25:00.914883 [3] Warning: no training nodes in this partition! Backward fake loss.
08:25:01.670515 [3] Warning: no training nodes in this partition! Backward fake loss.
08:25:02.157570 [3] Warning: no training nodes in this partition! Backward fake loss.
08:25:02.942405 [3] Warning: no training nodes in this partition! Backward fake loss.
08:25:03.415778 [3] Warning: no training nodes in this partition! Backward fake loss.
08:26:31.299322 [3] proc begin: <DistEnv 3/4 nccl>
08:26:37.260664 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
08:26:37.286686 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

08:28:24.941763 [3] proc begin: <DistEnv 3/4 nccl>
08:28:31.014461 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
08:28:31.038800 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:38:03.500140 [3] proc begin: <DistEnv 3/4 nccl>
10:38:09.541098 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
10:38:09.560963 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:40:00.825590 [3] proc begin: <DistEnv 3/4 nccl>
10:40:07.917633 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
10:40:07.934311 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:42:05.224465 [3] proc begin: <DistEnv 3/4 nccl>
10:42:11.277182 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
10:42:11.297728 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:43:37.872552 [3] proc begin: <DistEnv 3/4 nccl>
10:43:43.522624 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
10:43:43.540893 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:43:47.779690 [3] Warning: no training nodes in this partition! Backward fake loss.
10:43:49.028245 [3] Warning: no training nodes in this partition! Backward fake loss.
10:43:49.733379 [3] Warning: no training nodes in this partition! Backward fake loss.
10:43:50.438350 [3] Warning: no training nodes in this partition! Backward fake loss.
10:43:51.143764 [3] Warning: no training nodes in this partition! Backward fake loss.
10:43:51.849353 [3] Warning: no training nodes in this partition! Backward fake loss.
10:43:52.556519 [3] Warning: no training nodes in this partition! Backward fake loss.
10:43:53.261266 [3] Warning: no training nodes in this partition! Backward fake loss.
10:43:53.968678 [3] Warning: no training nodes in this partition! Backward fake loss.
10:43:54.673683 [3] Warning: no training nodes in this partition! Backward fake loss.
10:43:55.378280 [3] Warning: no training nodes in this partition! Backward fake loss.
10:43:56.082644 [3] Warning: no training nodes in this partition! Backward fake loss.
10:43:56.787384 [3] Warning: no training nodes in this partition! Backward fake loss.
10:43:57.492092 [3] Warning: no training nodes in this partition! Backward fake loss.
10:43:58.197092 [3] Warning: no training nodes in this partition! Backward fake loss.
10:43:58.902061 [3] Warning: no training nodes in this partition! Backward fake loss.
10:43:59.607697 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:00.311951 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:01.016397 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:01.735421 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:02.467564 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:03.190952 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:03.896366 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:04.603904 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:05.310510 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:06.017778 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:06.723916 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:07.430070 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:08.135610 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:08.841667 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:09.550374 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:10.256456 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:10.961646 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:11.668086 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:12.371933 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:13.076670 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:13.782829 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:14.489442 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:15.195510 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:15.900985 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:16.606297 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:17.311731 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:18.017891 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:18.724875 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:19.431832 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:20.136928 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:20.841614 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:21.546238 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:22.250930 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:22.955009 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:23.660141 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:24.128076 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:24.884351 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:25.351393 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:26.107120 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:26.575575 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:27.331540 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:27.799172 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:28.555655 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:29.023343 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:29.780532 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:30.248007 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:31.005069 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:31.472747 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:32.228710 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:32.696175 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:33.454137 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:33.922254 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:34.680249 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:35.147322 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:35.904788 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:36.372738 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:37.129560 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:37.597115 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:38.354521 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:38.822394 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:39.582611 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:40.050162 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:40.807443 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:41.274849 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:42.033568 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:42.501036 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:43.261416 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:43.728575 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:44.486142 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:44.953142 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:45.709226 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:46.177528 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:46.932579 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:47.399716 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:48.155114 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:48.622286 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:49.377770 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:49.846061 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:50.601405 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:51.068766 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:51.823830 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:52.291377 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:53.047276 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:53.514788 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:54.270676 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:54.738465 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:55.493985 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:55.960972 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:56.716278 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:57.183843 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:57.938611 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:58.404937 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:59.159475 [3] Warning: no training nodes in this partition! Backward fake loss.
10:44:59.625723 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:00.380506 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:00.848429 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:01.604733 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:02.073139 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:02.852535 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:03.337729 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:04.108306 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:04.576660 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:05.333220 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:05.802029 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:06.558527 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:07.027804 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:07.784366 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:08.253258 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:09.009954 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:09.480174 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:10.238561 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:10.706828 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:11.463275 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:11.932083 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:12.691254 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:13.159759 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:13.919148 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:14.386772 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:15.143878 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:15.612535 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:16.369741 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:16.837529 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:17.593261 [3] Warning: no training nodes in this partition! Backward fake loss.
10:45:18.060801 [3] Warning: no training nodes in this partition! Backward fake loss.
10:49:48.241618 [3] proc begin: <DistEnv 3/4 nccl>
10:49:52.388778 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
10:49:52.407170 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:49:58.517846 [3] Warning: no training nodes in this partition! Backward fake loss.
10:50:01.447169 [3] Warning: no training nodes in this partition! Backward fake loss.
10:50:03.325543 [3] Warning: no training nodes in this partition! Backward fake loss.
10:50:05.152154 [3] Warning: no training nodes in this partition! Backward fake loss.
10:50:06.979779 [3] Warning: no training nodes in this partition! Backward fake loss.
10:50:08.809740 [3] Warning: no training nodes in this partition! Backward fake loss.
10:50:10.640221 [3] Warning: no training nodes in this partition! Backward fake loss.
10:50:12.465628 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:14.546697 [3] proc begin: <DistEnv 3/4 nccl>
14:00:20.372117 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:00:20.392594 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:00:24.908487 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:26.377199 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:26.807023 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:27.237390 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:27.666925 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:28.097355 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:28.528045 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:28.963323 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:29.394629 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:29.825100 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:30.255681 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:30.685863 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:31.116175 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:31.546431 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:31.977285 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:32.408150 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:32.839100 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:33.270165 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:33.701491 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:34.131870 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:34.561559 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:34.992033 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:35.422433 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:35.853240 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:36.283972 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:36.715057 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:37.145796 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:37.576165 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:38.007269 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:38.436697 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:38.867206 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:39.296228 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:39.726785 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:40.156782 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:40.587408 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:41.018124 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:41.447596 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:41.878065 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:42.307990 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:42.738193 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:43.167602 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:43.597389 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:44.027257 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:44.456665 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:44.885839 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:45.314809 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:45.743983 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:46.173878 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:46.602975 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:47.033167 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:47.463407 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:47.763935 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:48.232358 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:48.532264 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:49.000563 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:49.300890 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:49.768006 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:50.068107 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:50.536639 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:50.836975 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:51.303587 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:51.603547 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:52.071145 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:52.371302 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:52.838907 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:53.139037 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:53.606953 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:53.907026 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:54.374457 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:54.674891 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:55.143104 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:55.443419 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:55.911109 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:56.211362 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:56.679680 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:56.979387 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:57.448648 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:57.748798 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:58.216747 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:58.516994 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:58.985621 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:59.286320 [3] Warning: no training nodes in this partition! Backward fake loss.
14:00:59.756251 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:00.057175 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:00.525563 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:00.825707 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:01.294785 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:01.596268 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:02.063863 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:02.376461 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:02.863079 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:03.174333 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:03.655707 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:03.956735 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:04.427744 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:04.728029 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:05.198547 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:05.500175 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:05.970006 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:06.270988 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:06.740493 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:07.040787 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:07.512123 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:07.814104 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:08.284339 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:08.586720 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:09.056438 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:09.357492 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:09.828318 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:10.129814 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:10.601578 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:10.903570 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:11.373962 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:11.676067 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:12.146449 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:12.446578 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:12.916416 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:13.217509 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:13.687560 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:13.988395 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:14.458180 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:14.759229 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:15.229599 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:15.531218 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:16.000822 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:16.301899 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:16.771668 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:17.070981 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:17.539554 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:17.839699 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:18.309230 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:18.609626 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:19.079249 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:19.379382 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:19.848353 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:20.148051 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:20.617715 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:20.918262 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:21.385743 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:21.684978 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:22.152010 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:22.451955 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:22.920355 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:23.219949 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:23.689109 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:23.987807 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:24.455001 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:24.754889 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:25.222638 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:25.522916 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:25.990369 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:26.289939 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:26.758517 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:27.058121 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:27.526123 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:27.825709 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:28.293644 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:28.594678 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:29.061806 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:29.362381 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:29.830547 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:30.130960 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:30.599413 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:30.900001 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:31.367905 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:31.668068 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:32.136647 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:32.436717 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:32.905791 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:33.207871 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:33.676261 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:33.976157 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:34.444399 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:34.745327 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:35.214101 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:35.514380 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:35.983264 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:36.284021 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:36.752203 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:37.052203 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:37.521447 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:37.824437 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:38.293953 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:38.594225 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:39.062640 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:39.363256 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:39.831536 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:40.131929 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:40.600678 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:40.902500 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:41.370518 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:41.671141 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:42.139472 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:42.440262 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:42.908948 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:43.209637 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:43.678460 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:43.979488 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:44.449093 [3] Warning: no training nodes in this partition! Backward fake loss.
14:01:44.750418 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:13.794983 [3] proc begin: <DistEnv 3/4 nccl>
16:28:18.918511 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
16:28:18.943790 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:28:24.571289 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:26.479989 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:27.188175 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:27.897583 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:28.607968 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:29.317123 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:30.026124 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:30.734789 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:31.444028 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:32.151110 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:32.859109 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:33.568023 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:34.276474 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:34.983598 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:35.692615 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:36.403517 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:37.112791 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:37.820920 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:38.529739 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:39.237450 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:39.945813 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:40.652853 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:41.360541 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:42.067817 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:42.775819 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:43.482483 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:44.189745 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:44.897886 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:45.605530 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:46.312837 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:47.020834 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:47.727449 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:48.435467 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:49.143214 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:49.851836 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:50.558865 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:51.266983 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:51.973832 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:52.681971 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:53.388671 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:54.097308 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:54.804062 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:55.511878 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:56.218991 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:56.926477 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:57.635245 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:58.342025 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:59.051049 [3] Warning: no training nodes in this partition! Backward fake loss.
16:28:59.759636 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:00.467286 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:01.174876 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:01.642204 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:02.432432 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:02.916269 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:03.694862 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:04.162371 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:04.926956 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:05.394707 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:06.159517 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:06.626804 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:07.390925 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:07.858215 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:08.623081 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:09.090591 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:09.854745 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:10.321610 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:11.084259 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:11.551016 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:12.314242 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:12.781944 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:13.543193 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:14.010521 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:14.772399 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:15.239535 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:16.003007 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:16.469905 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:17.232322 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:17.699688 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:18.463398 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:18.929610 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:19.692359 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:20.159326 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:20.922208 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:21.388414 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:22.151270 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:22.617443 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:23.380340 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:23.847245 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:24.610391 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:25.076871 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:25.838858 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:26.305916 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:27.067871 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:27.534769 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:28.297639 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:28.766106 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:29.529986 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:29.996190 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:30.758792 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:31.225104 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:31.988023 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:32.454792 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:33.220336 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:33.687245 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:34.450278 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:34.916758 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:35.680918 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:36.146985 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:36.910964 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:37.377696 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:38.139738 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:38.606126 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:39.368460 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:39.835131 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:40.598413 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:41.066524 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:41.829724 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:42.297007 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:43.062387 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:43.529536 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:44.294985 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:44.761707 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:45.525735 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:45.994238 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:46.758848 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:47.226857 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:47.991297 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:48.458801 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:49.222987 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:49.689949 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:50.454136 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:50.919802 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:51.683131 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:52.148950 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:52.913484 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:53.379487 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:54.144080 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:54.610121 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:55.372655 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:55.838820 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:56.599799 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:57.066332 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:57.828070 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:58.294175 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:59.058174 [3] Warning: no training nodes in this partition! Backward fake loss.
16:29:59.523794 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:00.286159 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:00.751264 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:01.519971 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:02.002137 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:02.794265 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:03.263653 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:04.026276 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:04.493836 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:05.256943 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:05.724523 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:06.487986 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:06.955366 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:07.717862 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:08.185149 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:08.947217 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:09.414024 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:10.178332 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:10.645446 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:11.409949 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:11.876428 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:12.639904 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:13.106668 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:13.869892 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:14.337170 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:15.099051 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:15.565509 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:16.329371 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:16.795314 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:17.558502 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:18.025447 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:18.789318 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:19.255651 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:20.017943 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:20.484919 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:21.246448 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:21.712551 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:22.473763 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:22.939465 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:23.700762 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:24.167129 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:24.929872 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:25.396050 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:26.156823 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:26.623293 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:27.385148 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:27.850698 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:28.613799 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:29.079913 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:29.842022 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:30.307555 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:31.068949 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:31.534660 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:32.296656 [3] Warning: no training nodes in this partition! Backward fake loss.
16:30:32.762521 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:04.747760 [3] proc begin: <DistEnv 3/4 nccl>
16:32:10.250933 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
16:32:10.270535 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:32:15.082632 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:16.875503 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:17.584806 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:18.295765 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:19.005752 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:19.713483 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:20.421468 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:21.130661 [3] Warning: no training nodes in this partition! Backward fake loss.
16:32:42.020013 [3] proc begin: <DistEnv 3/4 nccl>
16:32:47.615234 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:32:47.641578 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:40:52.904720 [3] proc begin: <DistEnv 3/4 nccl>
16:40:58.307295 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:40:58.325151 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:47:03.691398 [3] proc begin: <DistEnv 3/4 nccl>
16:47:08.720521 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
16:47:08.743961 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:47:13.126188 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:13.986395 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.124954 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.280604 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.435390 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.591318 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.748504 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.902490 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.057498 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.212968 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.367433 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.522362 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.677122 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.831909 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.986825 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.141745 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.297346 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.453646 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.608716 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.766011 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.920951 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.076713 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.232350 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.386674 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.541487 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.696464 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.852194 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.006998 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.162776 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.319263 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.473786 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.628877 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.784807 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.940631 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.095521 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.251361 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.406742 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.561927 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.716527 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.870655 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.026010 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.179754 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.334995 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.489728 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.644746 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.799075 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.954111 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.108062 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.263243 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.417877 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.571865 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.726779 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.882533 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.037525 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.192380 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.347526 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.502367 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.656379 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.812003 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.966136 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.120678 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.276356 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.430425 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.586736 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.740593 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.895133 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.050719 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.205359 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.360137 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.516473 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.670813 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.827270 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.981995 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.136937 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.292107 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.447149 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.602609 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.757279 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.913839 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.068047 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.223422 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.378376 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.532686 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.687923 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.843047 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.997592 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.152971 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.307283 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.462205 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.618197 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.773343 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.927645 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.082207 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.237625 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.393337 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.547509 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.702921 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.857639 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.012581 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.167400 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.322515 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.478084 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.632629 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.788703 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.943053 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.097987 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.253966 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.408912 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.562773 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.717492 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.871675 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.028538 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.183639 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.338058 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.491962 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.646709 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.802896 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.957004 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.111730 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.266423 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.421272 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.576499 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.731192 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.885705 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.040455 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.195200 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.350350 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.504953 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.659319 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.814167 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.968486 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.123972 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.277945 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.432184 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.587436 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.742563 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.897461 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.052116 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.206688 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.366385 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.517688 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.671781 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.826269 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.982040 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.136034 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.290780 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.445281 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.600007 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.754268 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.908287 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.062780 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.217465 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.371854 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.525959 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.680242 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.834693 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.989174 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.143870 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.297243 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.451402 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.605436 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.760052 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.914941 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.069606 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.223985 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.378411 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.532959 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.687399 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.842304 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.996507 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.150989 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.305307 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.460712 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.615011 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.769547 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.924917 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.079463 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.233738 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.390035 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.544558 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.699438 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.854094 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.009376 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.164333 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.318810 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.473131 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.627677 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.782897 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.938173 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.092317 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.247416 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.402066 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.556944 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.711964 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.866649 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.021354 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.175807 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.330729 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.485047 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.639508 [3] Warning: no training nodes in this partition! Backward fake loss.
15:45:35.490593 [3] proc begin: <DistEnv 3/4 nccl>
15:45:53.850407 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
15:45:53.872382 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:46:08.845916 [3] proc begin: <DistEnv 3/4 nccl>
15:46:08.999142 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
15:46:09.012903 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:50:26.704388 [3] proc begin: <DistEnv 3/4 nccl>
15:50:26.915883 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
15:50:26.928316 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:50:28.001615 [3] L1 tensor(91906.2109, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:52:58.451249 [3] proc begin: <DistEnv 3/4 nccl>
15:52:58.528285 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
15:52:58.540569 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:52:59.554335 [3] L1 tensor(91906.2109, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:00.317374 [3] L2 tensor(438.5738, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:00.358102 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.118431 [3] L1 tensor(91832.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.121646 [3] L2 tensor(442.4138, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.149608 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.201496 [3] L1 tensor(91855.8203, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.226513 [3] L2 tensor(445.8104, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.271312 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.302103 [3] L1 tensor(91963.5078, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.349018 [3] L2 tensor(448.3091, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.352469 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.426313 [3] L1 tensor(92143.7031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.429417 [3] L2 tensor(450.3099, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.457387 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.509357 [3] L1 tensor(92384.4141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.534062 [3] L2 tensor(452.1290, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.579720 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.614734 [3] L1 tensor(92675.2109, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.659073 [3] L2 tensor(454.0485, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.661520 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.677404 [3] L1 tensor(93008.1797, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.699391 [3] L2 tensor(456.0366, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.745540 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.777009 [3] L1 tensor(93376.7422, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.823349 [3] L2 tensor(457.9691, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.825973 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.900738 [3] L1 tensor(93775.1719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.903733 [3] L2 tensor(459.7110, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:01.932230 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.981606 [3] L1 tensor(94198.1797, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.009886 [3] L2 tensor(461.1994, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.054798 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.085977 [3] L1 tensor(94641.2031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.132913 [3] L2 tensor(462.4901, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.136433 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.210206 [3] L1 tensor(95100.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.213289 [3] L2 tensor(463.6808, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.241713 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.290571 [3] L1 tensor(95573.6953, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.319074 [3] L2 tensor(464.8087, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.363749 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.397255 [3] L1 tensor(96059.2422, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.440167 [3] L2 tensor(465.8306, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.443394 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.516609 [3] L1 tensor(96555.1172, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.520865 [3] L2 tensor(466.6919, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.547459 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.600136 [3] L1 tensor(97059.4141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.625512 [3] L2 tensor(467.4421, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.672134 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.706344 [3] L1 tensor(97571.1094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.750891 [3] L2 tensor(468.2280, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.754278 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.786618 [3] L1 tensor(98089.2344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.831713 [3] L2 tensor(469.1948, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.835058 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.909322 [3] L1 tensor(98612.5547, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.912419 [3] L2 tensor(470.4097, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:02.940804 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.992514 [3] L1 tensor(99140.3125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.018554 [3] L2 tensor(471.9096, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.065207 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.074448 [3] L1 tensor(99671.9844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.078042 [3] L2 tensor(473.7235, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.081415 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.155534 [3] L1 tensor(100206.1875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.158350 [3] L2 tensor(475.8296, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.185796 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.236646 [3] L1 tensor(100741.9688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.262534 [3] L2 tensor(478.1782, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.311014 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.343118 [3] L1 tensor(101279.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.389895 [3] L2 tensor(480.7126, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.392900 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.465688 [3] L1 tensor(101816.7188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.468290 [3] L2 tensor(483.3638, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.496357 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.548562 [3] L1 tensor(102353.3750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.572664 [3] L2 tensor(486.0809, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.621002 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.651601 [3] L1 tensor(102887.5938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.698069 [3] L2 tensor(488.8194, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.700553 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.773643 [3] L1 tensor(103417.3438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.776050 [3] L2 tensor(491.5266, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.804058 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.855453 [3] L1 tensor(103939.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.879663 [3] L2 tensor(494.1455, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:03.927819 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.962326 [3] L1 tensor(104451.4609, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.006588 [3] L2 tensor(496.6190, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.009083 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.018964 [3] L1 tensor(104950.7891, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.045095 [3] L2 tensor(498.9019, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.092391 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.123082 [3] L1 tensor(105436.0234, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.169777 [3] L2 tensor(500.9770, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.172393 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.245527 [3] L1 tensor(105906.2500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.248099 [3] L2 tensor(502.8625, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.276099 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.327640 [3] L1 tensor(106361.8828, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.351825 [3] L2 tensor(504.6041, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.400012 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.430871 [3] L1 tensor(106804.1875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.477101 [3] L2 tensor(506.2580, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.479553 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.552176 [3] L1 tensor(107234.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.554612 [3] L2 tensor(507.8826, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.582610 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.633588 [3] L1 tensor(107654.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.657819 [3] L2 tensor(509.5375, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.706225 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.737886 [3] L1 tensor(108064.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.782316 [3] L2 tensor(511.2832, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.784797 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.858491 [3] L1 tensor(108465.7812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.860892 [3] L2 tensor(513.1110, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.888869 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.938565 [3] L1 tensor(108857.4531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:04.965219 [3] L2 tensor(514.9779, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.011305 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.045713 [3] L1 tensor(109240.2734, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.090126 [3] L2 tensor(516.8826, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.092542 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.165372 [3] L1 tensor(109615.5781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.167808 [3] L2 tensor(518.7771, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.195778 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.245318 [3] L1 tensor(109982.1172, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.271960 [3] L2 tensor(520.5668, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.318036 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.349204 [3] L1 tensor(110338.7422, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.395836 [3] L2 tensor(522.2884, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.398298 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.471027 [3] L1 tensor(110692.7422, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.473400 [3] L2 tensor(524.0200, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.501361 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.551041 [3] L1 tensor(111051.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.575125 [3] L2 tensor(525.9103, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.623267 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.654609 [3] L1 tensor(111417.1562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.700994 [3] L2 tensor(528.1434, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.703458 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.775933 [3] L1 tensor(111786.0312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.778400 [3] L2 tensor(530.8064, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.806511 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.856428 [3] L1 tensor(112153.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.880601 [3] L2 tensor(533.7210, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:05.928826 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.959289 [3] L1 tensor(112519.6016, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.005843 [3] L2 tensor(536.6836, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.010775 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.083108 [3] L1 tensor(112882.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.085528 [3] L2 tensor(539.6154, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.110841 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.164882 [3] L1 tensor(113245.2031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.191589 [3] L2 tensor(542.6844, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.237747 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.253385 [3] L1 tensor(113612.4688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.269369 [3] L2 tensor(546.6610, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.315615 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.350523 [3] L1 tensor(113989.8672, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.395008 [3] L2 tensor(551.9967, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.397513 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.471407 [3] L1 tensor(114380.3281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.473770 [3] L2 tensor(558.6206, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.499386 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.553209 [3] L1 tensor(114780.7344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.577446 [3] L2 tensor(565.5466, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.625610 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.660074 [3] L1 tensor(115179.1719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.704535 [3] L2 tensor(571.8951, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.707005 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.780643 [3] L1 tensor(115564.8125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.783072 [3] L2 tensor(577.6732, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.811094 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.860728 [3] L1 tensor(115930.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.884851 [3] L2 tensor(582.9789, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:06.933315 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.963393 [3] L1 tensor(116275.6094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.009773 [3] L2 tensor(587.8942, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.012212 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.084822 [3] L1 tensor(116604.0625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.087244 [3] L2 tensor(592.4735, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.115295 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.167045 [3] L1 tensor(116924.6953, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.193831 [3] L2 tensor(596.7705, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.240558 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.274018 [3] L1 tensor(117243.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.318691 [3] L2 tensor(600.9476, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.321134 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.395404 [3] L1 tensor(117559.8125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.397423 [3] L2 tensor(605.2831, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.423207 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.477273 [3] L1 tensor(117869.3125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.503941 [3] L2 tensor(609.3708, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.549945 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.584594 [3] L1 tensor(118170.4219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.629379 [3] L2 tensor(613.2331, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.631828 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.706403 [3] L1 tensor(118466.0312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.708292 [3] L2 tensor(616.9452, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.736296 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.788006 [3] L1 tensor(118757.4688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.814512 [3] L2 tensor(620.6169, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.860617 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.890914 [3] L1 tensor(119055.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.937355 [3] L2 tensor(624.6840, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:07.939479 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.013569 [3] L1 tensor(119353.8672, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.016018 [3] L2 tensor(629.0459, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.041310 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.094895 [3] L1 tensor(119651.8281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.121610 [3] L2 tensor(633.7097, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.168027 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.200503 [3] L1 tensor(119951.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.247232 [3] L2 tensor(638.6428, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.249664 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.324562 [3] L1 tensor(120246.9609, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.327406 [3] L2 tensor(643.5590, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.355701 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.407949 [3] L1 tensor(120528.7812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.434773 [3] L2 tensor(648.2156, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.481101 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.515822 [3] L1 tensor(120794.8438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.560747 [3] L2 tensor(652.5349, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.563349 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.638604 [3] L1 tensor(121047.7188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.641087 [3] L2 tensor(656.5295, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.669044 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.720758 [3] L1 tensor(121292.0703, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.726205 [3] L2 tensor(660.2439, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.728623 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.736280 [3] L1 tensor(121528.4375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.764394 [3] L2 tensor(663.6747, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.810647 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.842049 [3] L1 tensor(121754.7969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.888669 [3] L2 tensor(666.8158, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.891152 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.965513 [3] L1 tensor(121969.1562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.968005 [3] L2 tensor(669.6705, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:08.995961 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.047505 [3] L1 tensor(122171.3672, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.074454 [3] L2 tensor(672.2507, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.120816 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.151532 [3] L1 tensor(122361.7891, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.198311 [3] L2 tensor(674.5707, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.200881 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.276028 [3] L1 tensor(122540.7188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.278553 [3] L2 tensor(676.6443, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.306278 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.358428 [3] L1 tensor(122708.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.384778 [3] L2 tensor(678.4858, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.431152 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.465306 [3] L1 tensor(122865.5859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.509723 [3] L2 tensor(680.1101, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.512280 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.587430 [3] L1 tensor(123012.6484, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.589946 [3] L2 tensor(681.5339, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.617638 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.669231 [3] L1 tensor(123150.4297, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.695468 [3] L2 tensor(682.7748, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.741804 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.752474 [3] L1 tensor(123279.7344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.757168 [3] L2 tensor(683.8511, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.759994 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.770425 [3] L1 tensor(123401.2266, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.797217 [3] L2 tensor(684.7797, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.842711 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.876801 [3] L1 tensor(123515.4062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.921440 [3] L2 tensor(685.5751, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:09.923889 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.998654 [3] L1 tensor(123622.6016, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.001015 [3] L2 tensor(686.2515, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.028722 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.080317 [3] L1 tensor(123723.1094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.106712 [3] L2 tensor(686.8276, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.153194 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.187486 [3] L1 tensor(123817.4922, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.232180 [3] L2 tensor(687.3495, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.234604 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.309223 [3] L1 tensor(123907.2188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.311587 [3] L2 tensor(687.9585, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.316823 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.325994 [3] L1 tensor(123994.1172, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.350631 [3] L2 tensor(688.7996, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.372606 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.406956 [3] L1 tensor(124077.1094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.431239 [3] L2 tensor(689.6537, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.477698 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.512332 [3] L1 tensor(124155.7812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.556925 [3] L2 tensor(690.4276, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.559313 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.633999 [3] L1 tensor(124230.6875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.636325 [3] L2 tensor(691.1127, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.638694 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.646111 [3] L1 tensor(124302., device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.649736 [3] L2 tensor(691.7106, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.654763 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.665259 [3] L1 tensor(124369.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.671827 [3] L2 tensor(692.2195, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.675776 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.686634 [3] L1 tensor(124433.0625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.689972 [3] L2 tensor(692.6395, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.693086 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.704512 [3] L1 tensor(124492.7578, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.710779 [3] L2 tensor(692.9738, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.715268 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.722125 [3] L1 tensor(124548.8281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.725204 [3] L2 tensor(693.2272, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.727768 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.734265 [3] L1 tensor(124601.5703, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.737312 [3] L2 tensor(693.4050, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.740133 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.747370 [3] L1 tensor(124651.2734, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.749934 [3] L2 tensor(693.5128, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.752512 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.758658 [3] L1 tensor(124698.2031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.761588 [3] L2 tensor(693.5555, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.763913 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.770289 [3] L1 tensor(124742.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.773059 [3] L2 tensor(693.5380, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.775528 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.782676 [3] L1 tensor(124784.8281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.785594 [3] L2 tensor(693.4649, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.788297 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.796090 [3] L1 tensor(124825.0234, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.799105 [3] L2 tensor(693.3407, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.801910 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.808864 [3] L1 tensor(124863.3828, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.811701 [3] L2 tensor(693.1694, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.814582 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.821881 [3] L1 tensor(124900.1250, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.824254 [3] L2 tensor(692.9548, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.826584 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.833712 [3] L1 tensor(124935.3750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.836797 [3] L2 tensor(692.7004, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.839579 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.848376 [3] L1 tensor(124969.2734, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.851629 [3] L2 tensor(692.4095, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.854951 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.860487 [3] L1 tensor(125001.8828, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.863140 [3] L2 tensor(692.0851, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.865791 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.873193 [3] L1 tensor(125033.2969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.875838 [3] L2 tensor(691.7300, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.878562 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.886770 [3] L1 tensor(125063.5781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.893965 [3] L2 tensor(691.3472, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.896530 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.905648 [3] L1 tensor(125092.7812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.908837 [3] L2 tensor(690.9393, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.911673 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.918202 [3] L1 tensor(125120.9609, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.920574 [3] L2 tensor(690.5090, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.923025 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.929503 [3] L1 tensor(125148.1797, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.932357 [3] L2 tensor(690.0588, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.935088 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.941928 [3] L1 tensor(125174.4922, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.944584 [3] L2 tensor(689.5912, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.947262 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.953710 [3] L1 tensor(125199.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.956331 [3] L2 tensor(689.1080, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.958698 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.964142 [3] L1 tensor(125224.6094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.967904 [3] L2 tensor(688.6113, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.970489 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.976568 [3] L1 tensor(125248.5312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.979173 [3] L2 tensor(688.1028, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.981653 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.988078 [3] L1 tensor(125271.7812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.990659 [3] L2 tensor(687.5839, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:10.993049 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.000800 [3] L1 tensor(125294.3984, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.003433 [3] L2 tensor(687.0554, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.005780 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.013445 [3] L1 tensor(125316.4453, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.016296 [3] L2 tensor(686.5184, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.018952 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.027359 [3] L1 tensor(125337.9688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.030866 [3] L2 tensor(685.9733, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.034150 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.042410 [3] L1 tensor(125359.0312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.045254 [3] L2 tensor(685.4206, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.047764 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.055686 [3] L1 tensor(125379.6875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.058209 [3] L2 tensor(684.8604, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.060897 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.068577 [3] L1 tensor(125399.9844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.071279 [3] L2 tensor(684.2928, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.073890 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.081059 [3] L1 tensor(125419.9609, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.083559 [3] L2 tensor(683.7180, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.085887 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.093853 [3] L1 tensor(125439.6953, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.097265 [3] L2 tensor(683.1360, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.101243 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.109280 [3] L1 tensor(125459.2188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.112105 [3] L2 tensor(682.5469, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.115350 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.124973 [3] L1 tensor(125478.6172, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.127608 [3] L2 tensor(681.9510, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.129930 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.136280 [3] L1 tensor(125497.9297, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.138978 [3] L2 tensor(681.3486, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.141708 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.149944 [3] L1 tensor(125517.2578, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.152760 [3] L2 tensor(680.7401, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.155284 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.163163 [3] L1 tensor(125536.6797, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.165921 [3] L2 tensor(680.1261, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.168746 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.175573 [3] L1 tensor(125556.3047, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.178341 [3] L2 tensor(679.5076, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.181240 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.192177 [3] L1 tensor(125576.2812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.198380 [3] L2 tensor(678.8854, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.202998 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.213450 [3] L1 tensor(125596.7422, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.217393 [3] L2 tensor(678.2612, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.220996 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.231953 [3] L1 tensor(125617.9062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.238415 [3] L2 tensor(677.6365, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.243173 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.253434 [3] L1 tensor(125639.9375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.257644 [3] L2 tensor(677.0134, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.261032 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.267788 [3] L1 tensor(125663.0312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.270391 [3] L2 tensor(676.3940, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.272963 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.278984 [3] L1 tensor(125687.2578, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.281902 [3] L2 tensor(675.7798, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.284216 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.292217 [3] L1 tensor(125712.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.295585 [3] L2 tensor(675.1722, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.298365 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.306083 [3] L1 tensor(125739.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.308562 [3] L2 tensor(674.5723, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.310880 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.316708 [3] L1 tensor(125766.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.319329 [3] L2 tensor(673.9815, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.321668 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.327495 [3] L1 tensor(125795.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.331451 [3] L2 tensor(673.4016, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.334039 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.342935 [3] L1 tensor(125824.4844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.345836 [3] L2 tensor(672.8350, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.348424 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.355242 [3] L1 tensor(125854.6250, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.357754 [3] L2 tensor(672.2845, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.360126 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.366600 [3] L1 tensor(125885.4609, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.369403 [3] L2 tensor(671.7540, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.371982 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.378497 [3] L1 tensor(125916.9141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.381086 [3] L2 tensor(671.2478, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.383441 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.389804 [3] L1 tensor(125948.8203, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.392403 [3] L2 tensor(670.7714, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.394773 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.400751 [3] L1 tensor(125981.0156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.403611 [3] L2 tensor(670.3314, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.406203 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.412365 [3] L1 tensor(126013.2656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.415039 [3] L2 tensor(669.9373, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.417367 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.423476 [3] L1 tensor(126045.3281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.425965 [3] L2 tensor(669.6034, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.428272 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.436737 [3] L1 tensor(126077., device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.439344 [3] L2 tensor(669.3516, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.441875 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.448745 [3] L1 tensor(126108.1641, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.451529 [3] L2 tensor(669.2137, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.454354 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.459918 [3] L1 tensor(126138.6797, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.462722 [3] L2 tensor(669.2305, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.465042 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.471806 [3] L1 tensor(126168.5391, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.477221 [3] L2 tensor(669.4468, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.482267 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.492840 [3] L1 tensor(126197.7188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.497072 [3] L2 tensor(669.9043, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.500775 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.511375 [3] L1 tensor(126226.2656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.516557 [3] L2 tensor(670.6352, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.519900 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.529533 [3] L1 tensor(126254.2500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.532505 [3] L2 tensor(671.6612, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.535547 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.541703 [3] L1 tensor(126281.7969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.544268 [3] L2 tensor(672.9954, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.546759 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.553140 [3] L1 tensor(126309.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.555653 [3] L2 tensor(674.6449, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.558260 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.566454 [3] L1 tensor(126336.4375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.570142 [3] L2 tensor(676.6127, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.573748 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.580456 [3] L1 tensor(126364.2031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.583591 [3] L2 tensor(678.8989, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.586708 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.592606 [3] L1 tensor(126392.9219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.594990 [3] L2 tensor(681.5011, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.597466 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.603329 [3] L1 tensor(126423.2500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.605906 [3] L2 tensor(684.4136, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.608367 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.615364 [3] L1 tensor(126455.8672, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.617851 [3] L2 tensor(687.6250, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.620477 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.626755 [3] L1 tensor(126491.2344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.629485 [3] L2 tensor(691.1115, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.632269 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.638707 [3] L1 tensor(126529.3906, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.641345 [3] L2 tensor(694.8279, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.644163 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.650628 [3] L1 tensor(126570.0078, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.653261 [3] L2 tensor(698.7007, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.656024 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.663645 [3] L1 tensor(126612.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.666312 [3] L2 tensor(702.6328, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.669009 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.676847 [3] L1 tensor(126656.5859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.679977 [3] L2 tensor(706.5289, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.682941 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.688718 [3] L1 tensor(126701.7656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.691194 [3] L2 tensor(710.3220, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.693506 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.699608 [3] L1 tensor(126748.0078, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.704117 [3] L2 tensor(713.9838, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.706621 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.715223 [3] L1 tensor(126795.2266, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.717986 [3] L2 tensor(717.5139, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.721275 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.729979 [3] L1 tensor(126842.9688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.733649 [3] L2 tensor(720.9189, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.737761 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.748749 [3] L1 tensor(126890.2969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.752836 [3] L2 tensor(724.1981, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.756862 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.765174 [3] L1 tensor(126936.3516, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.768122 [3] L2 tensor(727.3528, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.771362 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.781535 [3] L1 tensor(126980.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.783949 [3] L2 tensor(730.3943, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.786303 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.792277 [3] L1 tensor(127022.9766, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.794734 [3] L2 tensor(733.3419, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.797204 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.805339 [3] L1 tensor(127063.2969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.811073 [3] L2 tensor(736.2206, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.815027 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.824515 [3] L1 tensor(127101.6484, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.828511 [3] L2 tensor(739.0592, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.832364 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.842471 [3] L1 tensor(127138.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.848370 [3] L2 tensor(741.8875, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.851953 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.861877 [3] L1 tensor(127172.7656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.865269 [3] L2 tensor(744.7334, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.868740 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.878718 [3] L1 tensor(127205.7969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.882856 [3] L2 tensor(747.6213, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.886773 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.893824 [3] L1 tensor(127237.4219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.896656 [3] L2 tensor(750.5713, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.899674 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.905215 [3] L1 tensor(127267.8750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.907888 [3] L2 tensor(753.6005, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.910281 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.916768 [3] L1 tensor(127297.5156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.919324 [3] L2 tensor(756.7241, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.921721 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.927709 [3] L1 tensor(127326.8750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.930369 [3] L2 tensor(759.9565, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.932802 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.938784 [3] L1 tensor(127356.7500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.941275 [3] L2 tensor(763.3117, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.943796 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.951784 [3] L1 tensor(127388.3281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.957461 [3] L2 tensor(766.8024, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.959993 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.967812 [3] L1 tensor(127423.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.970358 [3] L2 tensor(770.4375, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.972907 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.979100 [3] L1 tensor(127462.8516, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.981914 [3] L2 tensor(774.2180, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.984507 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.992204 [3] L1 tensor(127508.5781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.995020 [3] L2 tensor(778.1348, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:11.997367 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:12.004878 [3] L1 tensor(127560.2109, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:12.007680 [3] L2 tensor(782.1676, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:12.010601 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:12.018714 [3] L1 tensor(127616.3828, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
15:53:12.021086 [3] L2 tensor(786.2880, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
15:53:12.023459 [3] Warning: no training nodes in this partition! Backward fake loss.
15:53:58.519874 [3] proc begin: <DistEnv 3/4 nccl>
15:54:16.449995 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
15:54:16.475876 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:54:17.175390 [3] L1 tensor(6418.3506, device='cuda:3', grad_fn=<SumBackward0>) tensor(131.0658, device='cuda:3', grad_fn=<SumBackward0>)
16:28:41.173624 [3] proc begin: <DistEnv 3/4 nccl>
16:28:58.808796 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:28:58.836719 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:28:59.753194 [3] L1 tensor(38548.0586, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.5969, device='cuda:3', grad_fn=<SumBackward0>)
17:02:50.153860 [3] proc begin: <DistEnv 3/4 nccl>
17:03:01.235168 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
17:03:01.278069 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:03:02.772219 [3] L1 tensor(38548.0586, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.5969, device='cuda:3', grad_fn=<SumBackward0>)
17:04:43.486743 [3] proc begin: <DistEnv 3/4 nccl>
17:04:49.145658 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
17:04:49.166835 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:04:50.147404 [3] L1 tensor(38548.0586, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.5969, device='cuda:3', grad_fn=<SumBackward0>)
17:05:11.586337 [3] proc begin: <DistEnv 3/4 nccl>
17:05:17.046287 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
17:05:17.066365 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:05:17.935178 [3] L1 tensor(38548.0586, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.5969, device='cuda:3', grad_fn=<SumBackward0>)
17:19:05.303829 [3] proc begin: <DistEnv 3/4 nccl>
17:19:05.742423 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
17:19:05.756068 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:19:06.863663 [3] L1 tensor(91906.2109, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:07.700718 [3] L2 tensor(438.5738, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:07.744402 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.643239 [3] L1 tensor(91832.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.646747 [3] L2 tensor(442.4138, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.649564 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.660830 [3] L1 tensor(91855.8125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.664678 [3] L2 tensor(445.8104, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.668328 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.677647 [3] L1 tensor(91963.5078, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.680457 [3] L2 tensor(448.3092, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.683168 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.694958 [3] L1 tensor(92143.7031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.701568 [3] L2 tensor(450.3099, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.704711 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.716339 [3] L1 tensor(92384.4141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.721597 [3] L2 tensor(452.1290, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.725992 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.736092 [3] L1 tensor(92675.2109, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.740177 [3] L2 tensor(454.0485, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.744346 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.752449 [3] L1 tensor(93008.1797, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.756008 [3] L2 tensor(456.0366, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.759700 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.767822 [3] L1 tensor(93376.7344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.770822 [3] L2 tensor(457.9691, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.773762 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.783513 [3] L1 tensor(93775.1562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.786225 [3] L2 tensor(459.7110, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.789005 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.795109 [3] L1 tensor(94198.1719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.797933 [3] L2 tensor(461.1994, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.800474 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.807331 [3] L1 tensor(94641.2031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.810155 [3] L2 tensor(462.4901, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.812742 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.819094 [3] L1 tensor(95100.4922, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.822037 [3] L2 tensor(463.6807, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.824903 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.833592 [3] L1 tensor(95573.6875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.840257 [3] L2 tensor(464.8087, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.843651 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.851024 [3] L1 tensor(96059.2188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.854034 [3] L2 tensor(465.8305, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.856645 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.863360 [3] L1 tensor(96555.1094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.866153 [3] L2 tensor(466.6918, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.868781 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.874906 [3] L1 tensor(97059.3906, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.877684 [3] L2 tensor(467.4421, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.880263 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.887400 [3] L1 tensor(97571.1094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.890477 [3] L2 tensor(468.2280, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.893545 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.900012 [3] L1 tensor(98089.2266, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.902484 [3] L2 tensor(469.1947, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.905017 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.912215 [3] L1 tensor(98612.5469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.914834 [3] L2 tensor(470.4096, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.918445 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.926418 [3] L1 tensor(99140.3047, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.933009 [3] L2 tensor(471.9096, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.936042 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.947990 [3] L1 tensor(99671.9766, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.953528 [3] L2 tensor(473.7235, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.956029 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.966036 [3] L1 tensor(100206.1719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.969008 [3] L2 tensor(475.8296, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.971834 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.980426 [3] L1 tensor(100741.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.983646 [3] L2 tensor(478.1783, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.986948 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.995918 [3] L1 tensor(101279.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:08.998508 [3] L2 tensor(480.7126, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.001071 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.008795 [3] L1 tensor(101816.7031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.012205 [3] L2 tensor(483.3637, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.015253 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.025745 [3] L1 tensor(102353.3594, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.031810 [3] L2 tensor(486.0808, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.035515 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.046441 [3] L1 tensor(102887.5781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.053066 [3] L2 tensor(488.8194, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.055669 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.066173 [3] L1 tensor(103417.3359, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.071642 [3] L2 tensor(491.5267, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.076998 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.086619 [3] L1 tensor(103939.6328, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.089357 [3] L2 tensor(494.1456, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.091895 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.102011 [3] L1 tensor(104451.4453, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.105302 [3] L2 tensor(496.6192, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.108060 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.119551 [3] L1 tensor(104950.7656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.125564 [3] L2 tensor(498.9021, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.128655 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.137985 [3] L1 tensor(105436.0156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.140835 [3] L2 tensor(500.9772, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.143326 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.153443 [3] L1 tensor(105906.2344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.156300 [3] L2 tensor(502.8627, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.158981 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.166009 [3] L1 tensor(106361.8750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.168591 [3] L2 tensor(504.6043, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.171112 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.180063 [3] L1 tensor(106804.1875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.182895 [3] L2 tensor(506.2584, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.185547 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.192260 [3] L1 tensor(107234.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.195022 [3] L2 tensor(507.8830, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.197535 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.205379 [3] L1 tensor(107654.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.208307 [3] L2 tensor(509.5380, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.211307 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.217892 [3] L1 tensor(108064.9219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.221094 [3] L2 tensor(511.2837, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.224048 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.234665 [3] L1 tensor(108465.7500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.238127 [3] L2 tensor(513.1116, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.241542 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.251696 [3] L1 tensor(108857.4062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.256441 [3] L2 tensor(514.9783, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.260911 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.267336 [3] L1 tensor(109240.2344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.270221 [3] L2 tensor(516.8829, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.273132 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.279537 [3] L1 tensor(109615.5156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.282113 [3] L2 tensor(518.7773, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.284641 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.290923 [3] L1 tensor(109982.0469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.293531 [3] L2 tensor(520.5670, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.295999 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.304128 [3] L1 tensor(110338.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.307106 [3] L2 tensor(522.2886, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.310300 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.316701 [3] L1 tensor(110692.6250, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.319350 [3] L2 tensor(524.0200, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.321928 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.329234 [3] L1 tensor(111051.7969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.333061 [3] L2 tensor(525.9098, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.336234 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.343437 [3] L1 tensor(111416.9688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.346180 [3] L2 tensor(528.1423, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.348970 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.359644 [3] L1 tensor(111785.8281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.362655 [3] L2 tensor(530.8048, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.365375 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.375859 [3] L1 tensor(112153.7266, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.380702 [3] L2 tensor(533.7192, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.384714 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.396129 [3] L1 tensor(112519.3594, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.400298 [3] L2 tensor(536.6816, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.404460 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.411056 [3] L1 tensor(112882.2344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.414414 [3] L2 tensor(539.6133, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.419175 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.429589 [3] L1 tensor(113244.9219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.433635 [3] L2 tensor(542.6822, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.437811 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.446988 [3] L1 tensor(113612.1719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.451634 [3] L2 tensor(546.6586, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.455389 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.467358 [3] L1 tensor(113989.5312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.473443 [3] L2 tensor(551.9941, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.478059 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.488458 [3] L1 tensor(114379.9844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.492531 [3] L2 tensor(558.6183, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.496446 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.502664 [3] L1 tensor(114780.3672, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.505430 [3] L2 tensor(565.5444, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.508201 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.514738 [3] L1 tensor(115178.8047, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.517718 [3] L2 tensor(571.8928, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.520501 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.527587 [3] L1 tensor(115564.4531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.530602 [3] L2 tensor(577.6707, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.533106 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.539499 [3] L1 tensor(115930.2031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.542294 [3] L2 tensor(582.9760, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.544830 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.551432 [3] L1 tensor(116275.2656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.554060 [3] L2 tensor(587.8910, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.556852 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.565275 [3] L1 tensor(116603.7188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.567933 [3] L2 tensor(592.4698, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.570592 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.578635 [3] L1 tensor(116924.3672, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.581532 [3] L2 tensor(596.7665, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.584373 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.594429 [3] L1 tensor(117242.7812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.597340 [3] L2 tensor(600.9441, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.600303 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.606362 [3] L1 tensor(117559.5391, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.609099 [3] L2 tensor(605.2783, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.611858 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.620089 [3] L1 tensor(117869.0703, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.623144 [3] L2 tensor(609.3643, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.626052 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.632848 [3] L1 tensor(118170.2266, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.635657 [3] L2 tensor(613.2253, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.638346 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.646673 [3] L1 tensor(118465.9219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.650618 [3] L2 tensor(616.9367, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.653837 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.663472 [3] L1 tensor(118757.4531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.666544 [3] L2 tensor(620.6086, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.669677 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.677400 [3] L1 tensor(119055.7500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.680042 [3] L2 tensor(624.6782, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.682630 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.693242 [3] L1 tensor(119354.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.696616 [3] L2 tensor(629.0445, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.700060 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.709549 [3] L1 tensor(119652.2812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.713497 [3] L2 tensor(633.7127, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.717205 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.728858 [3] L1 tensor(119952.5938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.733693 [3] L2 tensor(638.6491, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.737657 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.748053 [3] L1 tensor(120247.7344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.752327 [3] L2 tensor(643.5670, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.755694 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.763108 [3] L1 tensor(120529.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.766090 [3] L2 tensor(648.2248, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.769086 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.775011 [3] L1 tensor(120795.8281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.777731 [3] L2 tensor(652.5451, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.780238 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.787220 [3] L1 tensor(121048.8281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.791398 [3] L2 tensor(656.5410, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.794139 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.801079 [3] L1 tensor(121293.3203, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.803644 [3] L2 tensor(660.2567, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.806125 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.812119 [3] L1 tensor(121529.7969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.814752 [3] L2 tensor(663.6885, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.817266 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.824804 [3] L1 tensor(121756.2656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.827891 [3] L2 tensor(666.8307, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.831174 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.844638 [3] L1 tensor(121970.7266, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.851422 [3] L2 tensor(669.6866, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.855140 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.864449 [3] L1 tensor(122173.0469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.867695 [3] L2 tensor(672.2680, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.870830 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.878687 [3] L1 tensor(122363.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.881350 [3] L2 tensor(674.5891, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.884001 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.892168 [3] L1 tensor(122542.6094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.895008 [3] L2 tensor(676.6639, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.897762 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.904112 [3] L1 tensor(122710.4844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.906712 [3] L2 tensor(678.5065, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.909213 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.915583 [3] L1 tensor(122867.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.921074 [3] L2 tensor(680.1316, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.923975 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.931482 [3] L1 tensor(123014.7969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.934256 [3] L2 tensor(681.5560, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.936760 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.946594 [3] L1 tensor(123152.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.949620 [3] L2 tensor(682.7974, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.953069 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.960561 [3] L1 tensor(123282.0078, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.963357 [3] L2 tensor(683.8741, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.966343 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.973159 [3] L1 tensor(123403.5469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.975727 [3] L2 tensor(684.8029, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.978265 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.984652 [3] L1 tensor(123517.7812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.987488 [3] L2 tensor(685.5984, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:09.990181 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.997614 [3] L1 tensor(123625.0156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.000441 [3] L2 tensor(686.2748, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.003142 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.010985 [3] L1 tensor(123725.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.013719 [3] L2 tensor(686.8505, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.016562 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.023168 [3] L1 tensor(123819.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.025836 [3] L2 tensor(687.3707, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.028384 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.037346 [3] L1 tensor(123909.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.040073 [3] L2 tensor(687.9740, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.042588 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.051083 [3] L1 tensor(123996.5156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.054597 [3] L2 tensor(688.8097, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.058030 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.064021 [3] L1 tensor(124079.4844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.066581 [3] L2 tensor(689.6624, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.069078 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.075378 [3] L1 tensor(124158.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.077887 [3] L2 tensor(690.4357, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.080375 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.086922 [3] L1 tensor(124233.0156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.089739 [3] L2 tensor(691.1203, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.092254 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.100607 [3] L1 tensor(124304.3203, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.103183 [3] L2 tensor(691.7180, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.105670 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.113413 [3] L1 tensor(124371.8125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.116158 [3] L2 tensor(692.2268, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.118752 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.125221 [3] L1 tensor(124435.3438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.128362 [3] L2 tensor(692.6467, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.131189 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.137710 [3] L1 tensor(124495.0312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.141324 [3] L2 tensor(692.9810, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.143942 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.150457 [3] L1 tensor(124551.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.153247 [3] L2 tensor(693.2344, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.156076 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.162283 [3] L1 tensor(124603.8125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.165514 [3] L2 tensor(693.4122, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.168265 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.179208 [3] L1 tensor(124653.4844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.185795 [3] L2 tensor(693.5199, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.190483 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.198774 [3] L1 tensor(124700.3906, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.201658 [3] L2 tensor(693.5624, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.204134 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.210595 [3] L1 tensor(124744.7734, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.213349 [3] L2 tensor(693.5448, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.215853 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.223107 [3] L1 tensor(124786.9219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.225997 [3] L2 tensor(693.4716, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.228892 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.236475 [3] L1 tensor(124827.0469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.239179 [3] L2 tensor(693.3471, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.242230 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.250577 [3] L1 tensor(124865.3672, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.253789 [3] L2 tensor(693.1757, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.257190 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.263316 [3] L1 tensor(124902.0469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.266140 [3] L2 tensor(692.9608, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.268951 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.275042 [3] L1 tensor(124937.2500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.277890 [3] L2 tensor(692.7062, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.280458 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.289441 [3] L1 tensor(124971.0859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.294078 [3] L2 tensor(692.4150, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.298568 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.306332 [3] L1 tensor(125003.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.309997 [3] L2 tensor(692.0902, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.313193 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.320416 [3] L1 tensor(125035.0156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.323197 [3] L2 tensor(691.7349, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.326163 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.331867 [3] L1 tensor(125065.2344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.334415 [3] L2 tensor(691.3517, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.336942 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.343120 [3] L1 tensor(125094.3984, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.345762 [3] L2 tensor(690.9435, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.348318 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.355278 [3] L1 tensor(125122.5312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.358311 [3] L2 tensor(690.5129, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.361261 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.367296 [3] L1 tensor(125149.7188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.369780 [3] L2 tensor(690.0624, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.372250 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.378520 [3] L1 tensor(125176., device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.381238 [3] L2 tensor(689.5944, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.383724 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.390221 [3] L1 tensor(125201.4375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.393245 [3] L2 tensor(689.1108, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.395747 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.402602 [3] L1 tensor(125226.0703, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.405301 [3] L2 tensor(688.6139, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.408064 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.414171 [3] L1 tensor(125249.9609, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.416901 [3] L2 tensor(688.1051, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.419673 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.425801 [3] L1 tensor(125273.1797, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.431230 [3] L2 tensor(687.5858, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.434246 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.442292 [3] L1 tensor(125295.7734, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.444896 [3] L2 tensor(687.0573, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.447474 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.453755 [3] L1 tensor(125317.8125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.456863 [3] L2 tensor(686.5200, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.459578 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.465935 [3] L1 tensor(125339.3125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.468632 [3] L2 tensor(685.9749, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.471204 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.477891 [3] L1 tensor(125360.3594, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.480749 [3] L2 tensor(685.4220, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.483416 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.490049 [3] L1 tensor(125380.9922, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.493079 [3] L2 tensor(684.8618, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.495925 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.502426 [3] L1 tensor(125401.2656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.505902 [3] L2 tensor(684.2943, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.508400 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.515326 [3] L1 tensor(125421.2266, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.518065 [3] L2 tensor(683.7194, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.520947 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.527301 [3] L1 tensor(125440.9375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.529917 [3] L2 tensor(683.1373, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.532477 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.538530 [3] L1 tensor(125460.4453, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.541235 [3] L2 tensor(682.5483, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.543734 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.549795 [3] L1 tensor(125479.8125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.552441 [3] L2 tensor(681.9523, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.555148 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.561319 [3] L1 tensor(125499.1250, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.563866 [3] L2 tensor(681.3497, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.566472 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.572253 [3] L1 tensor(125518.4141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.575046 [3] L2 tensor(680.7411, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.577688 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.585660 [3] L1 tensor(125537.8125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.588449 [3] L2 tensor(680.1269, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.591373 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.598600 [3] L1 tensor(125557.4219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.601770 [3] L2 tensor(679.5081, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.605068 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.611729 [3] L1 tensor(125577.3594, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.614568 [3] L2 tensor(678.8856, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.617471 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.628068 [3] L1 tensor(125597.7969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.633051 [3] L2 tensor(678.2609, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.637092 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.648312 [3] L1 tensor(125618.9141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.653101 [3] L2 tensor(677.6357, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.655648 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.665749 [3] L1 tensor(125640.9062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.668743 [3] L2 tensor(677.0120, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.671784 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.678454 [3] L1 tensor(125663.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.681210 [3] L2 tensor(676.3918, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.683862 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.689943 [3] L1 tensor(125688.1328, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.692656 [3] L2 tensor(675.7770, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.695429 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.702022 [3] L1 tensor(125713.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.704820 [3] L2 tensor(675.1686, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.707326 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.715175 [3] L1 tensor(125739.9688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.717905 [3] L2 tensor(674.5680, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.720792 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.726969 [3] L1 tensor(125767.4688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.729531 [3] L2 tensor(673.9763, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.732100 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.739540 [3] L1 tensor(125795.9062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.742454 [3] L2 tensor(673.3954, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.744993 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.755847 [3] L1 tensor(125825.2422, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.759712 [3] L2 tensor(672.8276, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.762721 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.773650 [3] L1 tensor(125855.3359, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.777310 [3] L2 tensor(672.2759, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.780978 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.792207 [3] L1 tensor(125886.1562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.796967 [3] L2 tensor(671.7439, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.801534 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.807638 [3] L1 tensor(125917.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.810292 [3] L2 tensor(671.2361, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.812879 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.820254 [3] L1 tensor(125949.4531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.826318 [3] L2 tensor(670.7574, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.830793 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.838021 [3] L1 tensor(125981.6172, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.840589 [3] L2 tensor(670.3148, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.843088 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.852914 [3] L1 tensor(126013.8516, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.856299 [3] L2 tensor(669.9174, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.860171 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.866169 [3] L1 tensor(126045.9062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.868894 [3] L2 tensor(669.5789, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.871456 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.878276 [3] L1 tensor(126077.5938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.881214 [3] L2 tensor(669.3206, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.884208 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.890443 [3] L1 tensor(126108.7578, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.893223 [3] L2 tensor(669.1735, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.895747 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.902061 [3] L1 tensor(126139.2891, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.904640 [3] L2 tensor(669.1777, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.907128 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.913867 [3] L1 tensor(126169.1562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.918923 [3] L2 tensor(669.3781, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.922857 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.931834 [3] L1 tensor(126198.3594, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.934348 [3] L2 tensor(669.8165, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.936833 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.947722 [3] L1 tensor(126226.9219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.954349 [3] L2 tensor(670.5261, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.958836 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.967187 [3] L1 tensor(126254.9219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.969840 [3] L2 tensor(671.5295, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.972348 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.978497 [3] L1 tensor(126282.4766, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.981243 [3] L2 tensor(672.8403, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.983851 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.991889 [3] L1 tensor(126309.7812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.994939 [3] L2 tensor(674.4662, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:10.997890 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.007635 [3] L1 tensor(126337.0859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.011447 [3] L2 tensor(676.4105, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.015318 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.023261 [3] L1 tensor(126364.7969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.027653 [3] L2 tensor(678.6738, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.031151 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.040255 [3] L1 tensor(126393.4062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.043466 [3] L2 tensor(681.2538, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.046766 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.059056 [3] L1 tensor(126423.5938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.064907 [3] L2 tensor(684.1448, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.069498 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.079625 [3] L1 tensor(126456.0078, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.083747 [3] L2 tensor(687.3356, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.087311 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.097660 [3] L1 tensor(126491.1484, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.100918 [3] L2 tensor(690.8032, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.104037 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.116594 [3] L1 tensor(126529.1094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.121706 [3] L2 tensor(694.5038, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.125974 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.135054 [3] L1 tensor(126569.5469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.137767 [3] L2 tensor(698.3648, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.140497 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.146409 [3] L1 tensor(126611.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.149002 [3] L2 tensor(702.2902, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.151515 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.159587 [3] L1 tensor(126655.8438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.162315 [3] L2 tensor(706.1835, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.164838 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.171845 [3] L1 tensor(126700.8984, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.174713 [3] L2 tensor(709.9762, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.177599 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.184391 [3] L1 tensor(126747.0156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.187164 [3] L2 tensor(713.6380, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.189706 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.196868 [3] L1 tensor(126794.1250, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.199822 [3] L2 tensor(717.1677, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.202732 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.209761 [3] L1 tensor(126841.7734, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.213168 [3] L2 tensor(720.5723, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.216130 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.222941 [3] L1 tensor(126889.0859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.227534 [3] L2 tensor(723.8508, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.230280 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.240668 [3] L1 tensor(126935.1562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.244755 [3] L2 tensor(727.0045, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.247773 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.259192 [3] L1 tensor(126979.4844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.265164 [3] L2 tensor(730.0438, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.269811 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.279755 [3] L1 tensor(127021.8672, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.283976 [3] L2 tensor(732.9879, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.292156 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.299689 [3] L1 tensor(127062.2500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.302917 [3] L2 tensor(735.8609, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.306378 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.314584 [3] L1 tensor(127100.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.317366 [3] L2 tensor(738.6917, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.320078 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.330423 [3] L1 tensor(127137.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.333821 [3] L2 tensor(741.5101, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.337154 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.347844 [3] L1 tensor(127171.8516, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.352590 [3] L2 tensor(744.3444, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.355456 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.364169 [3] L1 tensor(127204.9219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.367596 [3] L2 tensor(747.2191, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.370737 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.379958 [3] L1 tensor(127236.5391, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.383015 [3] L2 tensor(750.1547, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.386665 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.392608 [3] L1 tensor(127266.9688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.395266 [3] L2 tensor(753.1685, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.398074 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.404695 [3] L1 tensor(127296.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.407265 [3] L2 tensor(756.2761, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.409752 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.415867 [3] L1 tensor(127325.8203, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.418503 [3] L2 tensor(759.4919, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.421255 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.428977 [3] L1 tensor(127355.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.431584 [3] L2 tensor(762.8299, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.434136 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.441374 [3] L1 tensor(127386.7188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.444076 [3] L2 tensor(766.3027, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.446748 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.457124 [3] L1 tensor(127421.0234, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.461250 [3] L2 tensor(769.9196, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.464144 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.475728 [3] L1 tensor(127460.0312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.481113 [3] L2 tensor(773.6823, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.484449 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.494017 [3] L1 tensor(127505.0078, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.500576 [3] L2 tensor(777.5822, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.503567 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.514986 [3] L1 tensor(127555.9609, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.518725 [3] L2 tensor(781.6003, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.521572 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.531186 [3] L1 tensor(127611.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.534266 [3] L2 tensor(785.7085, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:19:11.537387 [3] Warning: no training nodes in this partition! Backward fake loss.
17:23:06.008539 [3] proc begin: <DistEnv 3/4 nccl>
17:23:11.602468 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
17:23:11.628963 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:24:28.121326 [3] proc begin: <DistEnv 3/4 nccl>
17:25:02.869535 [3] proc begin: <DistEnv 3/4 nccl>
17:25:02.946148 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
17:25:02.979721 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:25:04.138022 [3] L1 tensor(91906.2109, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:04.956599 [3] L2 tensor(438.5738, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:04.990759 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.797855 [3] L1 tensor(91832.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.801809 [3] L2 tensor(442.4138, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.804551 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.810286 [3] L1 tensor(91855.8203, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.812960 [3] L2 tensor(445.8104, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.815498 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.822140 [3] L1 tensor(91963.5156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.825014 [3] L2 tensor(448.3091, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.827806 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.833968 [3] L1 tensor(92143.7031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.836561 [3] L2 tensor(450.3099, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.838945 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.845007 [3] L1 tensor(92384.4141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.847713 [3] L2 tensor(452.1290, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.850099 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.856675 [3] L1 tensor(92675.2188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.859852 [3] L2 tensor(454.0485, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.862616 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.869083 [3] L1 tensor(93008.1797, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.871819 [3] L2 tensor(456.0366, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.874373 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.880971 [3] L1 tensor(93376.7500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.883617 [3] L2 tensor(457.9691, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.886195 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.892583 [3] L1 tensor(93775.1719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.895935 [3] L2 tensor(459.7110, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.898724 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.905411 [3] L1 tensor(94198.1875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.908371 [3] L2 tensor(461.1994, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.911256 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.917714 [3] L1 tensor(94641.2031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.920270 [3] L2 tensor(462.4901, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.922815 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.929095 [3] L1 tensor(95100.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.932039 [3] L2 tensor(463.6808, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.934754 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.941219 [3] L1 tensor(95573.6953, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.943867 [3] L2 tensor(464.8087, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.946284 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.953251 [3] L1 tensor(96059.2422, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.956059 [3] L2 tensor(465.8306, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.958455 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.965269 [3] L1 tensor(96555.1172, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.968243 [3] L2 tensor(466.6918, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.972146 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.980207 [3] L1 tensor(97059.4141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.985403 [3] L2 tensor(467.4421, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:05.993165 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.005450 [3] L1 tensor(97571.1250, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.009799 [3] L2 tensor(468.2280, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.013437 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.030349 [3] L1 tensor(98089.2500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.036658 [3] L2 tensor(469.1948, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.039545 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.049514 [3] L1 tensor(98612.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.054115 [3] L2 tensor(470.4097, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.059149 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.068165 [3] L1 tensor(99140.3281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.073943 [3] L2 tensor(471.9097, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.078237 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.087820 [3] L1 tensor(99672., device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.091495 [3] L2 tensor(473.7236, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.095054 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.102172 [3] L1 tensor(100206.1953, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.109394 [3] L2 tensor(475.8297, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.112021 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.122912 [3] L1 tensor(100741.9844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.126432 [3] L2 tensor(478.1783, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.130057 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.139128 [3] L1 tensor(101279.1094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.144036 [3] L2 tensor(480.7127, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.148030 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.157461 [3] L1 tensor(101816.7188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.161881 [3] L2 tensor(483.3639, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.166995 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.174581 [3] L1 tensor(102353.3906, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.177627 [3] L2 tensor(486.0810, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.180403 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.190260 [3] L1 tensor(102887.6094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.192855 [3] L2 tensor(488.8195, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.195415 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.203060 [3] L1 tensor(103417.3672, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.205640 [3] L2 tensor(491.5269, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.208657 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.214539 [3] L1 tensor(103939.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.217351 [3] L2 tensor(494.1458, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.219854 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.226449 [3] L1 tensor(104451.4766, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.229063 [3] L2 tensor(496.6194, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.231542 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.237656 [3] L1 tensor(104950.7969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.240325 [3] L2 tensor(498.9023, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.242856 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.249143 [3] L1 tensor(105436.0312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.251997 [3] L2 tensor(500.9773, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.254689 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.261522 [3] L1 tensor(105906.2656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.264452 [3] L2 tensor(502.8629, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.267466 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.273836 [3] L1 tensor(106361.8906, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.276912 [3] L2 tensor(504.6045, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.279611 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.286901 [3] L1 tensor(106804.2031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.289947 [3] L2 tensor(506.2585, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.293092 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.299894 [3] L1 tensor(107234.6641, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.302886 [3] L2 tensor(507.8830, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.305872 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.314962 [3] L1 tensor(107654.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.318562 [3] L2 tensor(509.5378, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.322054 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.329972 [3] L1 tensor(108064.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.333189 [3] L2 tensor(511.2835, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.336307 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.343771 [3] L1 tensor(108465.7891, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.347282 [3] L2 tensor(513.1113, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.350880 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.358995 [3] L1 tensor(108857.4609, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.362241 [3] L2 tensor(514.9783, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.365540 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.376971 [3] L1 tensor(109240.2969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.381948 [3] L2 tensor(516.8831, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.385690 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.396773 [3] L1 tensor(109615.6094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.401045 [3] L2 tensor(518.7777, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.404884 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.411216 [3] L1 tensor(109982.1562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.413883 [3] L2 tensor(520.5676, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.416522 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.422997 [3] L1 tensor(110338.7969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.425609 [3] L2 tensor(522.2892, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.428203 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.435874 [3] L1 tensor(110692.8125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.438610 [3] L2 tensor(524.0209, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.441071 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.447437 [3] L1 tensor(111052.0469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.450268 [3] L2 tensor(525.9114, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.452771 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.459172 [3] L1 tensor(111417.2578, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.461975 [3] L2 tensor(528.1450, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.464781 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.471450 [3] L1 tensor(111786.1562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.474252 [3] L2 tensor(530.8081, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.476993 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.483317 [3] L1 tensor(112154.0781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.486173 [3] L2 tensor(533.7229, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.488875 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.495512 [3] L1 tensor(112519.7344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.499103 [3] L2 tensor(536.6855, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.502237 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.511703 [3] L1 tensor(112882.6250, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.515353 [3] L2 tensor(539.6174, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.518921 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.526090 [3] L1 tensor(113245.3359, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.529185 [3] L2 tensor(542.6865, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.532073 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.543324 [3] L1 tensor(113612.6172, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.549893 [3] L2 tensor(546.6628, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.554590 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.564237 [3] L1 tensor(113990., device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.568143 [3] L2 tensor(551.9982, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.571397 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.578508 [3] L1 tensor(114380.4844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.581379 [3] L2 tensor(558.6219, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.584242 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.590309 [3] L1 tensor(114780.8594, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.593059 [3] L2 tensor(565.5477, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.595553 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.602078 [3] L1 tensor(115179.2969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.605107 [3] L2 tensor(571.8963, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.607567 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.615289 [3] L1 tensor(115564.9297, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.621812 [3] L2 tensor(577.6746, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.626579 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.636559 [3] L1 tensor(115930.6719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.640526 [3] L2 tensor(582.9805, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.643855 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.655067 [3] L1 tensor(116275.7188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.661639 [3] L2 tensor(587.8960, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.664546 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.672810 [3] L1 tensor(116604.1719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.675702 [3] L2 tensor(592.4757, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.678281 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.685161 [3] L1 tensor(116924.7891, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.687871 [3] L2 tensor(596.7731, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.690272 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.699178 [3] L1 tensor(117243.1719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.702805 [3] L2 tensor(600.9498, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.706371 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.715550 [3] L1 tensor(117559.8750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.718485 [3] L2 tensor(605.2867, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.721540 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.727562 [3] L1 tensor(117869.3281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.730267 [3] L2 tensor(609.3760, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.732703 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.739070 [3] L1 tensor(118170.3984, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.741681 [3] L2 tensor(613.2397, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.744077 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.751961 [3] L1 tensor(118465.9297, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.754775 [3] L2 tensor(616.9527, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.757244 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.765693 [3] L1 tensor(118757.2500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.768363 [3] L2 tensor(620.6242, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.770894 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.782117 [3] L1 tensor(119055.2891, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.788004 [3] L2 tensor(624.6888, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.791357 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.797533 [3] L1 tensor(119353.3750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.800227 [3] L2 tensor(629.0469, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.802681 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.809471 [3] L1 tensor(119651.1641, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.812270 [3] L2 tensor(633.7066, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.815124 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.822462 [3] L1 tensor(119951.1328, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.825082 [3] L2 tensor(638.6360, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.827593 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.834445 [3] L1 tensor(120246., device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.837357 [3] L2 tensor(643.5498, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.840019 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.849935 [3] L1 tensor(120527.7109, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.852767 [3] L2 tensor(648.2047, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.855697 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.861561 [3] L1 tensor(120793.6719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.864103 [3] L2 tensor(652.5225, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.866972 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.872875 [3] L1 tensor(121046.4219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.875697 [3] L2 tensor(656.5157, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.878398 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.884792 [3] L1 tensor(121290.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.887437 [3] L2 tensor(660.2286, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.889882 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.896113 [3] L1 tensor(121526.9141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.898951 [3] L2 tensor(663.6585, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.901670 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.908897 [3] L1 tensor(121753.2031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.912250 [3] L2 tensor(666.7991, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.915030 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.922002 [3] L1 tensor(121967.5156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.924705 [3] L2 tensor(669.6534, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.927081 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.933294 [3] L1 tensor(122169.6953, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.936016 [3] L2 tensor(672.2335, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.938457 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.944745 [3] L1 tensor(122360.0859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.949390 [3] L2 tensor(674.5535, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.954215 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.964188 [3] L1 tensor(122539., device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.968589 [3] L2 tensor(676.6274, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.972101 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.982667 [3] L1 tensor(122706.7656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.988121 [3] L2 tensor(678.4692, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:06.993101 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.001655 [3] L1 tensor(122863.8516, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.005180 [3] L2 tensor(680.0939, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.008563 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.018777 [3] L1 tensor(123010.9062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.021539 [3] L2 tensor(681.5181, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.024279 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.033751 [3] L1 tensor(123148.7109, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.038273 [3] L2 tensor(682.7595, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.042666 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.049917 [3] L1 tensor(123278.0312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.053005 [3] L2 tensor(683.8363, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.056012 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.065530 [3] L1 tensor(123399.5547, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.068392 [3] L2 tensor(684.7654, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.071062 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.077249 [3] L1 tensor(123513.7656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.080069 [3] L2 tensor(685.5615, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.082520 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.089828 [3] L1 tensor(123621.0312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.093262 [3] L2 tensor(686.2384, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.096378 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.102757 [3] L1 tensor(123721.6172, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.105383 [3] L2 tensor(686.8153, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.108137 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.114979 [3] L1 tensor(123816.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.117991 [3] L2 tensor(687.3386, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.120614 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.130524 [3] L1 tensor(123905.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.133161 [3] L2 tensor(687.9507, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.135688 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.143248 [3] L1 tensor(123993.0156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.146476 [3] L2 tensor(688.7937, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.149107 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.155379 [3] L1 tensor(124076.1719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.158032 [3] L2 tensor(689.6476, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.160414 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.167369 [3] L1 tensor(124155.0391, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.170862 [3] L2 tensor(690.4208, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.174425 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.181373 [3] L1 tensor(124230.1250, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.184877 [3] L2 tensor(691.1049, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.188529 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.195512 [3] L1 tensor(124301.6641, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.199021 [3] L2 tensor(691.7021, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.202724 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.209573 [3] L1 tensor(124369.3672, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.213092 [3] L2 tensor(692.2105, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.216785 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.226321 [3] L1 tensor(124433.0703, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.232784 [3] L2 tensor(692.6302, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.236317 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.247262 [3] L1 tensor(124492.8672, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.253084 [3] L2 tensor(692.9646, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.255599 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.264882 [3] L1 tensor(124548.9453, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.269829 [3] L2 tensor(693.2184, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.274283 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.284599 [3] L1 tensor(124601.6328, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.288381 [3] L2 tensor(693.3970, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.291401 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.297787 [3] L1 tensor(124651.2188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.300387 [3] L2 tensor(693.5057, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.302807 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.311059 [3] L1 tensor(124698.0312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.316183 [3] L2 tensor(693.5497, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.319189 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.325537 [3] L1 tensor(124742.3281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.328114 [3] L2 tensor(693.5336, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.330468 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.337311 [3] L1 tensor(124784.3750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.340527 [3] L2 tensor(693.4623, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.343181 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.349751 [3] L1 tensor(124824.4062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.352480 [3] L2 tensor(693.3398, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.355017 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.361426 [3] L1 tensor(124862.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.364034 [3] L2 tensor(693.1705, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.366466 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.373345 [3] L1 tensor(124899.2656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.376059 [3] L2 tensor(692.9579, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.378664 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.388190 [3] L1 tensor(124934.4062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.391995 [3] L2 tensor(692.7056, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.395276 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.402577 [3] L1 tensor(124968.2031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.405577 [3] L2 tensor(692.4167, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.408417 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.417504 [3] L1 tensor(125000.7188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.422179 [3] L2 tensor(692.0943, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.426626 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.437249 [3] L1 tensor(125032.0547, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.441145 [3] L2 tensor(691.7413, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.444450 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.455386 [3] L1 tensor(125062.2656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.461352 [3] L2 tensor(691.3604, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.466166 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.477014 [3] L1 tensor(125091.4062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.480768 [3] L2 tensor(690.9545, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.484475 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.494313 [3] L1 tensor(125119.5312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.498945 [3] L2 tensor(690.5261, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.502506 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.512823 [3] L1 tensor(125146.7031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.517303 [3] L2 tensor(690.0777, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.520882 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.532563 [3] L1 tensor(125172.9844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.537451 [3] L2 tensor(689.6117, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.541445 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.551934 [3] L1 tensor(125198.4141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.556463 [3] L2 tensor(689.1302, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.560192 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.570609 [3] L1 tensor(125223.0625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.575706 [3] L2 tensor(688.6351, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.578801 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.587921 [3] L1 tensor(125246.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.591314 [3] L2 tensor(688.1282, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.594429 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.604236 [3] L1 tensor(125270.1875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.606846 [3] L2 tensor(687.6105, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.609246 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.619230 [3] L1 tensor(125292.8047, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.623629 [3] L2 tensor(687.0834, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.627681 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.634630 [3] L1 tensor(125314.8438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.637445 [3] L2 tensor(686.5475, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.640179 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.651005 [3] L1 tensor(125336.3828, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.653876 [3] L2 tensor(686.0035, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.656731 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.663009 [3] L1 tensor(125357.4531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.665634 [3] L2 tensor(685.4519, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.668324 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.674870 [3] L1 tensor(125378.1328, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.677633 [3] L2 tensor(684.8926, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.679986 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.688812 [3] L1 tensor(125398.4531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.691477 [3] L2 tensor(684.3259, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.694198 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.702580 [3] L1 tensor(125418.4844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.705642 [3] L2 tensor(683.7520, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.708782 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.718107 [3] L1 tensor(125438.2500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.720674 [3] L2 tensor(683.1707, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.723114 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.729293 [3] L1 tensor(125457.8438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.732023 [3] L2 tensor(682.5823, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.734474 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.740637 [3] L1 tensor(125477.3047, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.743671 [3] L2 tensor(681.9871, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.746343 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.756722 [3] L1 tensor(125496.7031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.760234 [3] L2 tensor(681.3853, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.763740 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.770058 [3] L1 tensor(125516.1328, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.772735 [3] L2 tensor(680.7774, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.775057 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.782530 [3] L1 tensor(125535.6875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.785256 [3] L2 tensor(680.1642, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.787812 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.793926 [3] L1 tensor(125555.4766, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.796762 [3] L2 tensor(679.5466, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.799298 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.805858 [3] L1 tensor(125575.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.808772 [3] L2 tensor(678.9255, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.811468 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.818763 [3] L1 tensor(125596.3438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.821488 [3] L2 tensor(678.3028, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.824046 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.830012 [3] L1 tensor(125617.7891, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.832674 [3] L2 tensor(677.6799, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.835275 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.842060 [3] L1 tensor(125640.1641, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.844754 [3] L2 tensor(677.0592, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.847336 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.853577 [3] L1 tensor(125663.6094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.856154 [3] L2 tensor(676.4423, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.858506 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.864594 [3] L1 tensor(125688.2188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.867307 [3] L2 tensor(675.8309, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.869681 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.877137 [3] L1 tensor(125713.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.880022 [3] L2 tensor(675.2263, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.882764 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.890316 [3] L1 tensor(125740.7812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.893707 [3] L2 tensor(674.6296, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.896806 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.903976 [3] L1 tensor(125768.6016, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.906978 [3] L2 tensor(674.0422, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.912308 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.921824 [3] L1 tensor(125797.3359, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.925492 [3] L2 tensor(673.4661, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.929623 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.939150 [3] L1 tensor(125826.9297, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.942527 [3] L2 tensor(672.9038, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.947663 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.958095 [3] L1 tensor(125857.2812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.961266 [3] L2 tensor(672.3586, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.965181 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.974601 [3] L1 tensor(125888.3281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.978539 [3] L2 tensor(671.8345, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.982554 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.991641 [3] L1 tensor(125919.9375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.994889 [3] L2 tensor(671.3359, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:07.998039 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.004836 [3] L1 tensor(125951.9609, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.007723 [3] L2 tensor(670.8690, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.010604 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.018289 [3] L1 tensor(125984.2188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.021328 [3] L2 tensor(670.4412, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.023743 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.030302 [3] L1 tensor(126016.4375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.033057 [3] L2 tensor(670.0634, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.035540 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.041954 [3] L1 tensor(126048.4219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.044625 [3] L2 tensor(669.7524, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.049990 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.057412 [3] L1 tensor(126079.9688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.060527 [3] L2 tensor(669.5336, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.063331 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.073091 [3] L1 tensor(126110.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.076523 [3] L2 tensor(669.4422, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.081901 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.088206 [3] L1 tensor(126141.2656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.090767 [3] L2 tensor(669.5217, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.093470 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.099571 [3] L1 tensor(126170.9219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.102152 [3] L2 tensor(669.8168, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.104647 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.115481 [3] L1 tensor(126199.8828, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.119455 [3] L2 tensor(670.3663, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.122248 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.128511 [3] L1 tensor(126228.2266, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.131237 [3] L2 tensor(671.1985, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.133616 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.141795 [3] L1 tensor(126256.0469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.144823 [3] L2 tensor(672.3314, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.147643 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.154537 [3] L1 tensor(126283.4688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.157312 [3] L2 tensor(673.7753, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.159986 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.169083 [3] L1 tensor(126310.7188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.171632 [3] L2 tensor(675.5351, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.174202 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.180908 [3] L1 tensor(126338.1016, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.183649 [3] L2 tensor(677.6124, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.186163 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.193054 [3] L1 tensor(126366.0625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.196202 [3] L2 tensor(680.0062, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.198812 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.205481 [3] L1 tensor(126395.1562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.208463 [3] L2 tensor(682.7129, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.210796 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.217015 [3] L1 tensor(126426.0781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.219639 [3] L2 tensor(685.7258, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.222002 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.228199 [3] L1 tensor(126459.4375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.231012 [3] L2 tensor(689.0309, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.233684 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.240638 [3] L1 tensor(126495.6016, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.243387 [3] L2 tensor(692.5996, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.245956 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.252133 [3] L1 tensor(126534.4766, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.254820 [3] L2 tensor(696.3790, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.257264 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.264144 [3] L1 tensor(126575.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.267106 [3] L2 tensor(700.2872, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.269930 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.276665 [3] L1 tensor(126618.5781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.279753 [3] L2 tensor(704.2246, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.282461 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.289501 [3] L1 tensor(126662.8594, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.292475 [3] L2 tensor(708.1017, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.295172 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.302105 [3] L1 tensor(126708.2734, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.305623 [3] L2 tensor(711.8633, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.308899 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.315509 [3] L1 tensor(126754.7422, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.320410 [3] L2 tensor(715.4906, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.323800 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.332437 [3] L1 tensor(126802.1094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.335459 [3] L2 tensor(718.9883, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.338439 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.344232 [3] L1 tensor(126849.7656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.346949 [3] L2 tensor(722.3608, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.349325 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.355965 [3] L1 tensor(126896.7266, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.358729 [3] L2 tensor(725.6066, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.361476 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.367585 [3] L1 tensor(126942.2266, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.370344 [3] L2 tensor(728.7300, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.372820 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.379571 [3] L1 tensor(126985.8750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.382390 [3] L2 tensor(731.7451, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.385629 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.391832 [3] L1 tensor(127027.5469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.394653 [3] L2 tensor(734.6733, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.397471 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.403688 [3] L1 tensor(127067.2422, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.406634 [3] L2 tensor(737.5409, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.409272 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.417772 [3] L1 tensor(127104.9688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.420940 [3] L2 tensor(740.3770, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.423939 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.431095 [3] L1 tensor(127140.8438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.434262 [3] L2 tensor(743.2106, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.437547 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.444572 [3] L1 tensor(127174.9922, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.447418 [3] L2 tensor(746.0685, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.449774 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.455834 [3] L1 tensor(127207.5781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.458554 [3] L2 tensor(748.9731, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.460892 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.471718 [3] L1 tensor(127238.8125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.476401 [3] L2 tensor(751.9435, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.480541 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.492468 [3] L1 tensor(127268.9922, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.496663 [3] L2 tensor(754.9952, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.499536 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.507093 [3] L1 tensor(127298.5156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.510143 [3] L2 tensor(758.1432, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.513072 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.519468 [3] L1 tensor(127327.9844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.522219 [3] L2 tensor(761.4014, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.524718 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.530716 [3] L1 tensor(127358.3125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.533603 [3] L2 tensor(764.7833, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.536054 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.543203 [3] L1 tensor(127390.7812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.550157 [3] L2 tensor(768.3010, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.554538 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.564217 [3] L1 tensor(127426.9844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.567641 [3] L2 tensor(771.9609, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.571158 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.577378 [3] L1 tensor(127468.4453, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.579903 [3] L2 tensor(775.7610, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.582264 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.588530 [3] L1 tensor(127515.8906, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.591244 [3] L2 tensor(779.6886, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.593889 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.600219 [3] L1 tensor(127568.8047, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.603106 [3] L2 tensor(783.7209, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.605752 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.612347 [3] L1 tensor(127625.5312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.615156 [3] L2 tensor(787.8282, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
17:25:08.617545 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:41.032807 [3] proc begin: <DistEnv 3/4 nccl>
17:25:41.091425 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
17:25:41.124500 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:31:54.077585 [3] proc begin: <DistEnv 3/4 nccl>
20:31:54.356637 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
20:31:54.376850 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:31:55.478223 [3] L1 tensor(91906.2109, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:56.375240 [3] L2 tensor(438.5738, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:56.418372 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.343204 [3] L1 tensor(91832.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.347355 [3] L2 tensor(442.4138, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.350948 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.359355 [3] L1 tensor(91855.8203, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.363686 [3] L2 tensor(445.8104, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.367300 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.374998 [3] L1 tensor(91963.5078, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.377861 [3] L2 tensor(448.3091, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.380386 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.387331 [3] L1 tensor(92143.7031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.390457 [3] L2 tensor(450.3099, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.393378 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.399860 [3] L1 tensor(92384.4141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.402809 [3] L2 tensor(452.1291, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.405381 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.413904 [3] L1 tensor(92675.2109, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.417967 [3] L2 tensor(454.0486, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.422152 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.433688 [3] L1 tensor(93008.1797, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.438340 [3] L2 tensor(456.0366, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.440878 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.447956 [3] L1 tensor(93376.7500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.450759 [3] L2 tensor(457.9691, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.454253 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.461819 [3] L1 tensor(93775.1719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.464743 [3] L2 tensor(459.7110, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.467244 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.476502 [3] L1 tensor(94198.1797, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.480172 [3] L2 tensor(461.1994, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.483520 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.490506 [3] L1 tensor(94641.2031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.493378 [3] L2 tensor(462.4901, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.495911 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.502991 [3] L1 tensor(95100.4922, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.506119 [3] L2 tensor(463.6808, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.508726 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.516103 [3] L1 tensor(95573.6953, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.519237 [3] L2 tensor(464.8087, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.522079 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.529421 [3] L1 tensor(96059.2422, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.532092 [3] L2 tensor(465.8305, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.534620 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.541159 [3] L1 tensor(96555.1172, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.544288 [3] L2 tensor(466.6918, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.547049 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.554044 [3] L1 tensor(97059.4141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.556906 [3] L2 tensor(467.4421, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.559391 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.565868 [3] L1 tensor(97571.1094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.568735 [3] L2 tensor(468.2280, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.571276 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.578580 [3] L1 tensor(98089.2344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.581485 [3] L2 tensor(469.1948, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.584368 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.590883 [3] L1 tensor(98612.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.594141 [3] L2 tensor(470.4097, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.599746 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.608862 [3] L1 tensor(99140.3125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.612054 [3] L2 tensor(471.9096, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.615334 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.622482 [3] L1 tensor(99672., device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.625307 [3] L2 tensor(473.7236, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.627933 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.634861 [3] L1 tensor(100206.1875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.637845 [3] L2 tensor(475.8296, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.640825 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.647491 [3] L1 tensor(100741.9766, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.651037 [3] L2 tensor(478.1783, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.654263 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.666433 [3] L1 tensor(101279.1094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.670929 [3] L2 tensor(480.7127, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.674767 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.685581 [3] L1 tensor(101816.7188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.689623 [3] L2 tensor(483.3638, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.693808 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.704310 [3] L1 tensor(102353.3828, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.709078 [3] L2 tensor(486.0809, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.713943 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.720282 [3] L1 tensor(102887.5938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.723763 [3] L2 tensor(488.8194, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.726677 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.733686 [3] L1 tensor(103417.3516, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.736524 [3] L2 tensor(491.5267, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.739112 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.745891 [3] L1 tensor(103939.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.748864 [3] L2 tensor(494.1456, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.751545 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.758693 [3] L1 tensor(104451.4609, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.761608 [3] L2 tensor(496.6192, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.764309 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.774023 [3] L1 tensor(104950.7891, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.779887 [3] L2 tensor(498.9020, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.783894 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.794229 [3] L1 tensor(105436.0156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.797406 [3] L2 tensor(500.9771, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.800661 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.809160 [3] L1 tensor(105906.2500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.812141 [3] L2 tensor(502.8626, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.815587 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.822011 [3] L1 tensor(106361.8828, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.824718 [3] L2 tensor(504.6041, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.827246 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.836240 [3] L1 tensor(106804.1875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.839287 [3] L2 tensor(506.2581, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.842278 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.848909 [3] L1 tensor(107234.6484, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.851742 [3] L2 tensor(507.8827, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.854216 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.860426 [3] L1 tensor(107654.6484, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.863265 [3] L2 tensor(509.5375, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.867709 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.877237 [3] L1 tensor(108064.9375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.882155 [3] L2 tensor(511.2832, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.887016 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.896423 [3] L1 tensor(108465.7656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.900153 [3] L2 tensor(513.1110, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.903351 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.912190 [3] L1 tensor(108857.4375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.915678 [3] L2 tensor(514.9779, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.918842 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.928326 [3] L1 tensor(109240.2734, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.931275 [3] L2 tensor(516.8826, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.933764 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.943428 [3] L1 tensor(109615.5859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.946643 [3] L2 tensor(518.7772, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.949986 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.957686 [3] L1 tensor(109982.1328, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.960567 [3] L2 tensor(520.5671, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.963298 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.972614 [3] L1 tensor(110338.7656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.975813 [3] L2 tensor(522.2888, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.979302 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.990930 [3] L1 tensor(110692.7812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.994502 [3] L2 tensor(524.0205, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:57.998066 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.008445 [3] L1 tensor(111052., device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.013153 [3] L2 tensor(525.9110, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.016264 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.026397 [3] L1 tensor(111417.2188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.030149 [3] L2 tensor(528.1443, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.033289 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.044497 [3] L1 tensor(111786.1250, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.050908 [3] L2 tensor(530.8073, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.055605 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.066756 [3] L1 tensor(112154.0391, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.070529 [3] L2 tensor(533.7219, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.073519 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.087661 [3] L1 tensor(112519.6875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.095188 [3] L2 tensor(536.6843, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.097659 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.107498 [3] L1 tensor(112882.5859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.110489 [3] L2 tensor(539.6161, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.113187 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.123043 [3] L1 tensor(113245.3047, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.126732 [3] L2 tensor(542.6851, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.130010 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.139403 [3] L1 tensor(113612.5781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.142497 [3] L2 tensor(546.6616, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.145583 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.155480 [3] L1 tensor(113989.9844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.158835 [3] L2 tensor(551.9972, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.162052 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.169965 [3] L1 tensor(114380.4688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.173695 [3] L2 tensor(558.6211, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.176953 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.184621 [3] L1 tensor(114780.8750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.187708 [3] L2 tensor(565.5475, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.190663 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.196956 [3] L1 tensor(115179.3203, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.199538 [3] L2 tensor(571.8965, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.201941 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.208279 [3] L1 tensor(115564.9688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.211130 [3] L2 tensor(577.6750, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.213618 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.220987 [3] L1 tensor(115930.7344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.224006 [3] L2 tensor(582.9812, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.226929 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.233163 [3] L1 tensor(116275.7891, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.236027 [3] L2 tensor(587.8970, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.238437 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.244882 [3] L1 tensor(116604.2500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.247826 [3] L2 tensor(592.4769, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.250350 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.258159 [3] L1 tensor(116924.8828, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.261149 [3] L2 tensor(596.7745, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.264250 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.273234 [3] L1 tensor(117243.2812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.276330 [3] L2 tensor(600.9517, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.279948 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.286799 [3] L1 tensor(117560.0078, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.289534 [3] L2 tensor(605.2882, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.291964 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.298546 [3] L1 tensor(117869.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.301398 [3] L2 tensor(609.3769, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.304085 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.310403 [3] L1 tensor(118170.5938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.313672 [3] L2 tensor(613.2401, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.316326 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.323331 [3] L1 tensor(118466.1797, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.326296 [3] L2 tensor(616.9527, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.329060 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.335764 [3] L1 tensor(118757.5469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.338760 [3] L2 tensor(620.6245, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.341540 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.348715 [3] L1 tensor(119055.6719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.351551 [3] L2 tensor(624.6906, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.353964 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.360478 [3] L1 tensor(119353.8125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.363340 [3] L2 tensor(629.0503, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.365977 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.372930 [3] L1 tensor(119651.6719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.375792 [3] L2 tensor(633.7117, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.378214 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.385861 [3] L1 tensor(119951.7031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.389097 [3] L2 tensor(638.6426, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.392354 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.399586 [3] L1 tensor(120246.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.402771 [3] L2 tensor(643.5576, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.405714 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.412618 [3] L1 tensor(120528.3984, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.415417 [3] L2 tensor(648.2136, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.417904 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.424733 [3] L1 tensor(120794.4141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.427573 [3] L2 tensor(652.5324, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.430156 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.438964 [3] L1 tensor(121047.2344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.443380 [3] L2 tensor(656.5266, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.447429 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.458149 [3] L1 tensor(121291.5312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.462238 [3] L2 tensor(660.2404, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.466317 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.474827 [3] L1 tensor(121527.8516, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.478021 [3] L2 tensor(663.6710, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.480740 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.488176 [3] L1 tensor(121754.1875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.491126 [3] L2 tensor(666.8120, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.494044 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.501462 [3] L1 tensor(121968.5312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.505231 [3] L2 tensor(669.6666, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.507760 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.514452 [3] L1 tensor(122170.7188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.517485 [3] L2 tensor(672.2468, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.520159 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.527273 [3] L1 tensor(122361.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.530165 [3] L2 tensor(674.5669, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.533090 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.539789 [3] L1 tensor(122540.0625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.542733 [3] L2 tensor(676.6407, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.545292 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.552146 [3] L1 tensor(122707.8359, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.555330 [3] L2 tensor(678.4823, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.558200 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.564861 [3] L1 tensor(122864.9375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.567717 [3] L2 tensor(680.1068, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.570095 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.576748 [3] L1 tensor(123011.9922, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.579713 [3] L2 tensor(681.5306, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.582347 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.589492 [3] L1 tensor(123149.7812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.592443 [3] L2 tensor(682.7717, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.595186 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.601446 [3] L1 tensor(123279.1094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.604330 [3] L2 tensor(683.8482, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.606745 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.615808 [3] L1 tensor(123400.6250, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.620274 [3] L2 tensor(684.7769, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.623815 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.632972 [3] L1 tensor(123514.8438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.636132 [3] L2 tensor(685.5725, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.639274 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.650164 [3] L1 tensor(123622.0781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.653573 [3] L2 tensor(686.2490, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.656986 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.667027 [3] L1 tensor(123722.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.671007 [3] L2 tensor(686.8252, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.674726 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.684392 [3] L1 tensor(123817.1016, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.688367 [3] L2 tensor(687.3473, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.692415 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.698715 [3] L1 tensor(123906.9219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.701252 [3] L2 tensor(687.9568, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.703804 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.710887 [3] L1 tensor(123993.9375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.714161 [3] L2 tensor(688.7982, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.716802 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.723630 [3] L1 tensor(124077.0625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.726464 [3] L2 tensor(689.6525, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.729134 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.736016 [3] L1 tensor(124155.8750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.738650 [3] L2 tensor(690.4263, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.741273 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.747329 [3] L1 tensor(124230.9219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.749884 [3] L2 tensor(691.1112, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.752273 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.758441 [3] L1 tensor(124302.4062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.761265 [3] L2 tensor(691.7091, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.763741 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.769839 [3] L1 tensor(124370.0625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.772606 [3] L2 tensor(692.2183, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.775112 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.781535 [3] L1 tensor(124433.7344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.784551 [3] L2 tensor(692.6386, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.787129 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.793925 [3] L1 tensor(124493.4844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.796935 [3] L2 tensor(692.9735, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.799736 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.806314 [3] L1 tensor(124549.5781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.809149 [3] L2 tensor(693.2279, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.811702 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.818599 [3] L1 tensor(124602.2656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.821778 [3] L2 tensor(693.4070, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.824344 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.830873 [3] L1 tensor(124651.8828, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.833666 [3] L2 tensor(693.5162, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.836221 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.842618 [3] L1 tensor(124698.7188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.845293 [3] L2 tensor(693.5605, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.847741 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.854055 [3] L1 tensor(124743.0312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.856909 [3] L2 tensor(693.5450, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.859466 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.865941 [3] L1 tensor(124785.1016, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.869163 [3] L2 tensor(693.4740, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.872054 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.878525 [3] L1 tensor(124825.1719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.881395 [3] L2 tensor(693.3520, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.884134 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.890887 [3] L1 tensor(124863.4375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.893770 [3] L2 tensor(693.1831, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.896487 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.903310 [3] L1 tensor(124900.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.906765 [3] L2 tensor(692.9709, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.909575 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.916669 [3] L1 tensor(124935.2578, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.919834 [3] L2 tensor(692.7190, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.922448 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.929512 [3] L1 tensor(124969.0703, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.932804 [3] L2 tensor(692.4306, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.935910 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.943665 [3] L1 tensor(125001.6094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.946484 [3] L2 tensor(692.1086, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.948850 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.955351 [3] L1 tensor(125032.9453, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.958107 [3] L2 tensor(691.7561, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.960857 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.967381 [3] L1 tensor(125063.1719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.970355 [3] L2 tensor(691.3757, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.972962 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.981028 [3] L1 tensor(125092.3281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.986221 [3] L2 tensor(690.9703, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:58.988679 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.997661 [3] L1 tensor(125120.4531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.000783 [3] L2 tensor(690.5424, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.004330 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.011378 [3] L1 tensor(125147.6484, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.014200 [3] L2 tensor(690.0947, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.019463 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.025913 [3] L1 tensor(125173.9141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.028420 [3] L2 tensor(689.6293, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.030946 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.037485 [3] L1 tensor(125199.3438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.040910 [3] L2 tensor(689.1484, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.046414 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.053460 [3] L1 tensor(125223.9922, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.056254 [3] L2 tensor(688.6539, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.059208 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.066332 [3] L1 tensor(125247.8984, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.069257 [3] L2 tensor(688.1476, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.072733 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.079474 [3] L1 tensor(125271.1172, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.082539 [3] L2 tensor(687.6306, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.085210 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.091494 [3] L1 tensor(125293.7344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.094231 [3] L2 tensor(687.1043, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.096951 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.103281 [3] L1 tensor(125315.7656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.105936 [3] L2 tensor(686.5692, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.108349 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.118183 [3] L1 tensor(125337.2969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.120838 [3] L2 tensor(686.0261, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.123641 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.130374 [3] L1 tensor(125358.3672, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.133239 [3] L2 tensor(685.4752, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.135955 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.142531 [3] L1 tensor(125379.0312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.145318 [3] L2 tensor(684.9169, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.147787 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.154471 [3] L1 tensor(125399.3359, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.157155 [3] L2 tensor(684.3511, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.159785 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.166137 [3] L1 tensor(125419.3438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.169255 [3] L2 tensor(683.7780, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.172031 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.178517 [3] L1 tensor(125439.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.182038 [3] L2 tensor(683.1976, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.184700 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.191888 [3] L1 tensor(125458.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.194581 [3] L2 tensor(682.6101, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.197333 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.203642 [3] L1 tensor(125478.0859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.206479 [3] L2 tensor(682.0157, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.209083 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.215203 [3] L1 tensor(125497.4531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.217984 [3] L2 tensor(681.4148, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.220405 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.226914 [3] L1 tensor(125516.8438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.229464 [3] L2 tensor(680.8077, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.231941 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.238446 [3] L1 tensor(125536.3438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.241287 [3] L2 tensor(680.1953, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.243808 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.250388 [3] L1 tensor(125556.0781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.253388 [3] L2 tensor(679.5782, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.256143 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.262814 [3] L1 tensor(125576.1562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.265432 [3] L2 tensor(678.9578, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.268030 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.274712 [3] L1 tensor(125596.7734, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.277286 [3] L2 tensor(678.3353, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.282457 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.289072 [3] L1 tensor(125618.1094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.292120 [3] L2 tensor(677.7127, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.295105 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.301797 [3] L1 tensor(125640.3438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.304375 [3] L2 tensor(677.0919, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.306856 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.312950 [3] L1 tensor(125663.6641, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.315920 [3] L2 tensor(676.4749, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.318615 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.324944 [3] L1 tensor(125688.1172, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.327767 [3] L2 tensor(675.8633, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.330209 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.336439 [3] L1 tensor(125713.7344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.339465 [3] L2 tensor(675.2583, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.342198 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.351063 [3] L1 tensor(125740.4375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.356910 [3] L2 tensor(674.6611, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.359900 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.366392 [3] L1 tensor(125768.1484, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.369248 [3] L2 tensor(674.0732, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.371842 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.378485 [3] L1 tensor(125796.7969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.381243 [3] L2 tensor(673.4963, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.383681 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.393777 [3] L1 tensor(125826.2969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.396724 [3] L2 tensor(672.9331, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.399392 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.405835 [3] L1 tensor(125856.5703, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.408610 [3] L2 tensor(672.3866, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.411051 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.417862 [3] L1 tensor(125887.5391, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.421137 [3] L2 tensor(671.8606, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.423918 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.431074 [3] L1 tensor(125919.0781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.433883 [3] L2 tensor(671.3599, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.436571 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.443441 [3] L1 tensor(125951.0625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.446185 [3] L2 tensor(670.8901, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.448944 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.455187 [3] L1 tensor(125983.2969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.457997 [3] L2 tensor(670.4584, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.460665 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.470122 [3] L1 tensor(126015.5391, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.474143 [3] L2 tensor(670.0754, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.477906 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.487737 [3] L1 tensor(126047.5469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.491145 [3] L2 tensor(669.7568, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.494442 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.504799 [3] L1 tensor(126079.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.510193 [3] L2 tensor(669.5269, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.512985 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.520540 [3] L1 tensor(126110.1797, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.523388 [3] L2 tensor(669.4199, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.526029 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.532320 [3] L1 tensor(126140.5781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.534920 [3] L2 tensor(669.4784, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.537310 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.543399 [3] L1 tensor(126170.3125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.546039 [3] L2 tensor(669.7474, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.548712 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.558271 [3] L1 tensor(126199.3516, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.562590 [3] L2 tensor(670.2666, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.566587 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.573063 [3] L1 tensor(126227.7656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.575993 [3] L2 tensor(671.0658, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.578812 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.588636 [3] L1 tensor(126255.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.591510 [3] L2 tensor(672.1641, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.594418 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.601013 [3] L1 tensor(126283.1094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.604073 [3] L2 tensor(673.5726, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.606809 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.613854 [3] L1 tensor(126310.3828, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.616548 [3] L2 tensor(675.2971, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.618964 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.625316 [3] L1 tensor(126337.7422, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.628031 [3] L2 tensor(677.3395, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.630498 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.636768 [3] L1 tensor(126365.6328, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.639572 [3] L2 tensor(679.6990, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.642164 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.649517 [3] L1 tensor(126394.6094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.652418 [3] L2 tensor(682.3726, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.655987 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.665242 [3] L1 tensor(126425.3281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.668388 [3] L2 tensor(685.3538, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.671602 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.678283 [3] L1 tensor(126458.4375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.680960 [3] L2 tensor(688.6296, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.683370 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.694816 [3] L1 tensor(126494.3438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.701308 [3] L2 tensor(692.1733, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.704432 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.715269 [3] L1 tensor(126532.9844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.723265 [3] L2 tensor(695.9346, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.730180 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.741220 [3] L1 tensor(126573.9688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.745716 [3] L2 tensor(699.8345, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.749831 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.761941 [3] L1 tensor(126616.7969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.766876 [3] L2 tensor(703.7738, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.771060 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.780582 [3] L1 tensor(126661., device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.784657 [3] L2 tensor(707.6608, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.788470 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.795583 [3] L1 tensor(126706.3516, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.799063 [3] L2 tensor(711.4360, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.801855 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.808538 [3] L1 tensor(126752.7500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.811404 [3] L2 tensor(715.0779, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.813820 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.820360 [3] L1 tensor(126800.0859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.823317 [3] L2 tensor(718.5892, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.826002 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.834832 [3] L1 tensor(126847.7969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.837966 [3] L2 tensor(721.9753, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.841197 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.847330 [3] L1 tensor(126894.9141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.850002 [3] L2 tensor(725.2344, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.852650 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.859620 [3] L1 tensor(126940.5938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.862744 [3] L2 tensor(728.3699, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.865257 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.871689 [3] L1 tensor(126984.4688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.876182 [3] L2 tensor(731.3950, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.879047 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.886228 [3] L1 tensor(127026.3750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.888968 [3] L2 tensor(734.3306, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.891545 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.902558 [3] L1 tensor(127066.2656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.906733 [3] L2 tensor(737.2029, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.910197 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.923694 [3] L1 tensor(127104.2109, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.928324 [3] L2 tensor(740.0406, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.932006 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.944270 [3] L1 tensor(127140.2734, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.947643 [3] L2 tensor(742.8735, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.950609 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.961074 [3] L1 tensor(127174.5859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.966549 [3] L2 tensor(745.7286, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.970478 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.980669 [3] L1 tensor(127207.3125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.985014 [3] L2 tensor(748.6293, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.989407 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.996356 [3] L1 tensor(127238.6797, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:31:59.999944 [3] L2 tensor(751.5946, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.003235 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.010089 [3] L1 tensor(127268.9375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.012670 [3] L2 tensor(754.6411, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.015205 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.022841 [3] L1 tensor(127298.5078, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.027145 [3] L2 tensor(757.7836, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.029791 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.037172 [3] L1 tensor(127327.9219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.040616 [3] L2 tensor(761.0363, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.043473 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.051589 [3] L1 tensor(127358.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.054866 [3] L2 tensor(764.4130, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.058376 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.065021 [3] L1 tensor(127390.2656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.067783 [3] L2 tensor(767.9260, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.070531 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.080025 [3] L1 tensor(127426.0391, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.082806 [3] L2 tensor(771.5828, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.085690 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.092947 [3] L1 tensor(127466.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.097060 [3] L2 tensor(775.3823, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.099608 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.107147 [3] L1 tensor(127513.9062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.110071 [3] L2 tensor(779.3130, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.112754 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.119002 [3] L1 tensor(127566.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.121902 [3] L2 tensor(783.3530, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.124620 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.131069 [3] L1 tensor(127623.1562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.133993 [3] L2 tensor(787.4728, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:3', grad_fn=<SumBackward0>)
20:32:00.136542 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:10.191201 [3] proc begin: <DistEnv 3/4 nccl>
20:33:10.291384 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
20:33:10.305130 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:33:11.360043 [3] L1 tensor(91906.2109, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:12.327746 [3] L2 tensor(8166.0913, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:12.338378 [3] L3 tensor(8193.4414, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:12.342174 [3] L4 tensor(460.0110, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:12.370918 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.197221 [3] L1 tensor(92158.0234, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.201001 [3] L2 tensor(8003.2510, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.203695 [3] L3 tensor(8178.0820, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.206088 [3] L4 tensor(466.4110, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.208733 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.225010 [3] L1 tensor(92284.5781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.230561 [3] L2 tensor(7882.0869, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.233797 [3] L3 tensor(8160.8027, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.236566 [3] L4 tensor(471.2665, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.239100 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.248081 [3] L1 tensor(92265.5859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.250834 [3] L2 tensor(7761.1973, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.253709 [3] L3 tensor(8135.3574, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.256719 [3] L4 tensor(474.9828, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.259790 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.271847 [3] L1 tensor(92279.3828, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.277040 [3] L2 tensor(7658.1450, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.280219 [3] L3 tensor(8111.2231, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.282658 [3] L4 tensor(478.8577, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.285192 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.302831 [3] L1 tensor(92242.1875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.308658 [3] L2 tensor(7563.7065, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.312287 [3] L3 tensor(8093.5737, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.315125 [3] L4 tensor(481.4165, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.317613 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.334575 [3] L1 tensor(92148.2969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.339946 [3] L2 tensor(7474.9492, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.343906 [3] L3 tensor(8078.6621, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.346192 [3] L4 tensor(483.2287, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.348648 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.362897 [3] L1 tensor(92004.4766, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.368671 [3] L2 tensor(7388.9067, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.371822 [3] L3 tensor(8064.4102, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.374206 [3] L4 tensor(484.1880, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.376653 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.390882 [3] L1 tensor(91864.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.395945 [3] L2 tensor(7305.5322, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.398531 [3] L3 tensor(8050.8521, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.401333 [3] L4 tensor(485.2452, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.403934 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.418691 [3] L1 tensor(91703.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.423193 [3] L2 tensor(7224.3052, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.426967 [3] L3 tensor(8039.9307, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.430493 [3] L4 tensor(486.2715, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.432959 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.445910 [3] L1 tensor(91546.7656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.449674 [3] L2 tensor(7142.9131, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.453015 [3] L3 tensor(8030.0137, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.456392 [3] L4 tensor(486.3793, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.459304 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.472985 [3] L1 tensor(91400.9062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.476618 [3] L2 tensor(7063.3184, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.479437 [3] L3 tensor(8020.9849, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.481703 [3] L4 tensor(486.5335, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.484578 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.499898 [3] L1 tensor(91263.8906, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.505790 [3] L2 tensor(6990.2002, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.508324 [3] L3 tensor(8012.2500, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.510602 [3] L4 tensor(486.5843, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.513042 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.530792 [3] L1 tensor(91150.9844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.536627 [3] L2 tensor(6922.4463, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.539772 [3] L3 tensor(8003.8706, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.542053 [3] L4 tensor(486.6861, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.544496 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.554940 [3] L1 tensor(91041.7422, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.558111 [3] L2 tensor(6858.5234, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.560366 [3] L3 tensor(7995.8789, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.562905 [3] L4 tensor(486.6684, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.565334 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.578506 [3] L1 tensor(90944.8906, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.581171 [3] L2 tensor(6798.0029, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.583737 [3] L3 tensor(7988.0479, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.586342 [3] L4 tensor(486.6975, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.589126 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.604360 [3] L1 tensor(90839.7344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.610314 [3] L2 tensor(6739.6226, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.612584 [3] L3 tensor(7982.2271, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.614863 [3] L4 tensor(486.7326, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.617292 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.635219 [3] L1 tensor(90740.9141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.640613 [3] L2 tensor(6684.4434, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.644034 [3] L3 tensor(7978.1631, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.648796 [3] L4 tensor(486.8357, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.656373 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.666183 [3] L1 tensor(90655.0781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.672373 [3] L2 tensor(6633.7432, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.675857 [3] L3 tensor(7974.1128, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.678120 [3] L4 tensor(486.9752, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.680586 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.694317 [3] L1 tensor(90576.1328, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.700133 [3] L2 tensor(6587.6748, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.703422 [3] L3 tensor(7970.1611, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.705697 [3] L4 tensor(487.1800, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.708139 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.716861 [3] L1 tensor(90507.4062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.719621 [3] L2 tensor(6544.5615, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.722686 [3] L3 tensor(7965.9619, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.725869 [3] L4 tensor(487.3255, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.728528 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.739678 [3] L1 tensor(90447.1250, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.743079 [3] L2 tensor(6504.0693, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.746136 [3] L3 tensor(7961.7642, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.749170 [3] L4 tensor(487.4957, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.751928 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.766651 [3] L1 tensor(90383.7891, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.772987 [3] L2 tensor(6466.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.776302 [3] L3 tensor(7957.6348, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.779390 [3] L4 tensor(487.7361, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.782381 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.792456 [3] L1 tensor(90323.0156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.797107 [3] L2 tensor(6432.1631, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.800690 [3] L3 tensor(7953.8613, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.803618 [3] L4 tensor(488.0311, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.806097 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.823062 [3] L1 tensor(90261.3281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.828952 [3] L2 tensor(6398.6284, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.831997 [3] L3 tensor(7950.1064, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.834425 [3] L4 tensor(488.2441, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.837022 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.850280 [3] L1 tensor(90206.2969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.856281 [3] L2 tensor(6365.5928, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.860012 [3] L3 tensor(7946.6143, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.862789 [3] L4 tensor(488.3713, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.865390 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.874300 [3] L1 tensor(90156.0469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.878397 [3] L2 tensor(6332.5015, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.882153 [3] L3 tensor(7944.2500, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.884448 [3] L4 tensor(488.5137, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.886927 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.904195 [3] L1 tensor(90114.7656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.910456 [3] L2 tensor(6298.9160, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.912704 [3] L3 tensor(7941.5576, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.914959 [3] L4 tensor(488.6197, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.917409 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.926624 [3] L1 tensor(90077.8828, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.929141 [3] L2 tensor(6265.0801, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.931436 [3] L3 tensor(7938.5840, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.933754 [3] L4 tensor(488.7382, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.936249 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.947079 [3] L1 tensor(90045.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.950429 [3] L2 tensor(6230.8857, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.953325 [3] L3 tensor(7935.4272, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.955840 [3] L4 tensor(488.8261, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.958393 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.976197 [3] L1 tensor(90015.5938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.981954 [3] L2 tensor(6196.9541, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.984193 [3] L3 tensor(7932.2021, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.986475 [3] L4 tensor(488.9282, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:13.988894 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.999999 [3] L1 tensor(89986.9375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.002742 [3] L2 tensor(6162.7266, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.005990 [3] L3 tensor(7929.9180, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.009111 [3] L4 tensor(489.0490, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.011809 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.026877 [3] L1 tensor(89958.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.033257 [3] L2 tensor(6129.5806, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.036472 [3] L3 tensor(7927.6050, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.038949 [3] L4 tensor(489.1157, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.041698 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.051113 [3] L1 tensor(89930.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.055567 [3] L2 tensor(6097.7295, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.059711 [3] L3 tensor(7925.4434, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.062973 [3] L4 tensor(489.2115, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.065480 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.082418 [3] L1 tensor(89903.3125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.088377 [3] L2 tensor(6066.8984, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.091558 [3] L3 tensor(7923.3789, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.093822 [3] L4 tensor(489.3509, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.096247 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.107408 [3] L1 tensor(89876.5938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.114087 [3] L2 tensor(6037.3926, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.116794 [3] L3 tensor(7921.4487, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.119246 [3] L4 tensor(489.5436, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.121690 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.133627 [3] L1 tensor(89850.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.139281 [3] L2 tensor(6008.5723, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.143633 [3] L3 tensor(7920.0205, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.146600 [3] L4 tensor(489.7487, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.149054 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.162594 [3] L1 tensor(89827.0078, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.166078 [3] L2 tensor(5980.7783, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.169187 [3] L3 tensor(7919.0127, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.172320 [3] L4 tensor(489.9792, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.175184 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.189046 [3] L1 tensor(89805.1328, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.192288 [3] L2 tensor(5954.4502, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.195105 [3] L3 tensor(7918.4092, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.197982 [3] L4 tensor(490.2450, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.200422 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.215056 [3] L1 tensor(89782.6172, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.220399 [3] L2 tensor(5929.5557, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.223976 [3] L3 tensor(7917.1353, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.226263 [3] L4 tensor(490.4838, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.228707 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.237404 [3] L1 tensor(89760.8828, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.240930 [3] L2 tensor(5906.1587, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.244307 [3] L3 tensor(7915.2920, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.247684 [3] L4 tensor(490.7103, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.250876 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.262392 [3] L1 tensor(89742.0859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.266197 [3] L2 tensor(5883.9072, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.269815 [3] L3 tensor(7913.0674, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.272180 [3] L4 tensor(490.8849, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.274625 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.291694 [3] L1 tensor(89725.9844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.296383 [3] L2 tensor(5863.2334, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.300058 [3] L3 tensor(7910.7129, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.302315 [3] L4 tensor(491.0353, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.304918 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.319254 [3] L1 tensor(89711.5156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.324005 [3] L2 tensor(5844.0439, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.327229 [3] L3 tensor(7908.2539, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.329478 [3] L4 tensor(491.1834, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.331934 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.340721 [3] L1 tensor(89696.6797, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.344553 [3] L2 tensor(5826.1553, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.347586 [3] L3 tensor(7906.1543, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.350769 [3] L4 tensor(491.3640, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.353884 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.364672 [3] L1 tensor(89685.2969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.369488 [3] L2 tensor(5809.2324, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.373969 [3] L3 tensor(7903.4854, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.376339 [3] L4 tensor(491.2783, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.378901 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.389268 [3] L1 tensor(89668.5156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.395290 [3] L2 tensor(5782.9229, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.399697 [3] L3 tensor(7899.7290, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.402610 [3] L4 tensor(491.1868, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.405129 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.415607 [3] L1 tensor(89654.3672, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.422208 [3] L2 tensor(5758.6558, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.424806 [3] L3 tensor(7895.5166, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.427254 [3] L4 tensor(491.1088, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.429706 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.442098 [3] L1 tensor(89639.9453, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.446928 [3] L2 tensor(5733.5200, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.451388 [3] L3 tensor(7891.9409, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.454883 [3] L4 tensor(491.0822, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.457402 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.469125 [3] L1 tensor(89627.0625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.472601 [3] L2 tensor(5711.6113, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.475487 [3] L3 tensor(7888.9756, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.478617 [3] L4 tensor(491.1104, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.481851 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.497384 [3] L1 tensor(89618.0391, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.502429 [3] L2 tensor(5692.4775, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.506124 [3] L3 tensor(7886.5928, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.510733 [3] L4 tensor(491.1937, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.513161 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.528524 [3] L1 tensor(89613.0547, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.531973 [3] L2 tensor(5675.2920, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.535844 [3] L3 tensor(7883.5737, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.539467 [3] L4 tensor(491.2465, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.541904 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.558254 [3] L1 tensor(89608.4219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.563708 [3] L2 tensor(5659.4434, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.567825 [3] L3 tensor(7879.7822, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.570610 [3] L4 tensor(491.3054, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.573045 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.587779 [3] L1 tensor(89605.6953, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.593572 [3] L2 tensor(5645.0059, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.595954 [3] L3 tensor(7875.9121, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.598871 [3] L4 tensor(491.3718, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.601288 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.615604 [3] L1 tensor(89606.6719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.621421 [3] L2 tensor(5631.7451, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.623911 [3] L3 tensor(7871.9946, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.626178 [3] L4 tensor(491.4679, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.628699 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.641721 [3] L1 tensor(89606.4219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.645155 [3] L2 tensor(5619.5215, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.648625 [3] L3 tensor(7867.1426, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.652103 [3] L4 tensor(491.5798, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.655560 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.669955 [3] L1 tensor(89606.5469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.674076 [3] L2 tensor(5607.9160, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.677426 [3] L3 tensor(7862.7344, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.680976 [3] L4 tensor(491.7184, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.683978 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.700069 [3] L1 tensor(89606.2344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.705945 [3] L2 tensor(5597.3418, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.708325 [3] L3 tensor(7857.4141, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.710561 [3] L4 tensor(491.8526, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.713005 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.726386 [3] L1 tensor(89612.2891, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.732206 [3] L2 tensor(5587.6641, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.735927 [3] L3 tensor(7851.6646, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.738249 [3] L4 tensor(491.7527, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.740630 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.755649 [3] L1 tensor(89632.0625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.760800 [3] L2 tensor(5579.0195, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.764849 [3] L3 tensor(7845.3750, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.767741 [3] L4 tensor(491.6373, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.770139 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.783130 [3] L1 tensor(89654.5938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.789176 [3] L2 tensor(5570.6724, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.793194 [3] L3 tensor(7839.6465, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.795793 [3] L4 tensor(491.5720, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.798366 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.814223 [3] L1 tensor(89675.2266, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.820371 [3] L2 tensor(5563.5771, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.823685 [3] L3 tensor(7834.9033, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.825955 [3] L4 tensor(491.5689, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.828430 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.843626 [3] L1 tensor(89687.7031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.849128 [3] L2 tensor(5557.3779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.852071 [3] L3 tensor(7831.0566, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.854417 [3] L4 tensor(491.6295, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.856932 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.869431 [3] L1 tensor(89693.2031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.876000 [3] L2 tensor(5551.9375, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.879775 [3] L3 tensor(7828.0684, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.882118 [3] L4 tensor(491.7559, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.884630 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.900016 [3] L1 tensor(89693., device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.905425 [3] L2 tensor(5545.9360, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.908120 [3] L3 tensor(7824.0986, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.911358 [3] L4 tensor(491.8821, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.913888 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.931304 [3] L1 tensor(89679.2031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.937898 [3] L2 tensor(5538.7637, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.940340 [3] L3 tensor(7819.1602, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.942576 [3] L4 tensor(492.0254, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.944967 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.956350 [3] L1 tensor(89648.5234, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.961287 [3] L2 tensor(5529.5820, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.964322 [3] L3 tensor(7814.4331, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.966998 [3] L4 tensor(492.1658, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.969375 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.984464 [3] L1 tensor(89593.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.990815 [3] L2 tensor(5518.6353, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.995378 [3] L3 tensor(7808.9355, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:14.997895 [3] L4 tensor(492.2938, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.000412 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.015717 [3] L1 tensor(89530.0625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.020768 [3] L2 tensor(5505.3262, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.024787 [3] L3 tensor(7803.7461, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.027680 [3] L4 tensor(492.4462, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.030175 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.046610 [3] L1 tensor(89445.8906, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.052419 [3] L2 tensor(5488.7334, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.055775 [3] L3 tensor(7797.5410, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.058774 [3] L4 tensor(492.5581, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.061312 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.077140 [3] L1 tensor(89355.7266, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.083180 [3] L2 tensor(5471.3774, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.087403 [3] L3 tensor(7791.6758, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.089743 [3] L4 tensor(492.6813, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.092268 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.109016 [3] L1 tensor(89248.2734, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.114606 [3] L2 tensor(5452.1006, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.119001 [3] L3 tensor(7784.8213, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.121279 [3] L4 tensor(492.5934, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.123746 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.139248 [3] L1 tensor(89145.8047, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.145257 [3] L2 tensor(5433.4092, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.147996 [3] L3 tensor(7779.3276, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.150342 [3] L4 tensor(492.5703, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.152804 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.167960 [3] L1 tensor(89048.8438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.173315 [3] L2 tensor(5415.8945, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.176137 [3] L3 tensor(7774.9517, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.178572 [3] L4 tensor(492.6166, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.181457 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.197084 [3] L1 tensor(88959.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.202112 [3] L2 tensor(5399.6489, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.204475 [3] L3 tensor(7770.6099, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.206784 [3] L4 tensor(492.6955, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.209449 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.227126 [3] L1 tensor(88878.0781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.232301 [3] L2 tensor(5384.5879, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.235607 [3] L3 tensor(7765.0078, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.238740 [3] L4 tensor(492.7455, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.241232 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.256962 [3] L1 tensor(88802.2422, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.262345 [3] L2 tensor(5370.6543, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.264553 [3] L3 tensor(7758.4502, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.266852 [3] L4 tensor(492.7861, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.269473 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.287807 [3] L1 tensor(88732.7500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.293999 [3] L2 tensor(5357.8354, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.297390 [3] L3 tensor(7752.1958, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.300300 [3] L4 tensor(492.8394, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.302831 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.317330 [3] L1 tensor(88671.4844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.322787 [3] L2 tensor(5346.4995, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.327101 [3] L3 tensor(7746.3706, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.329327 [3] L4 tensor(492.9205, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.331734 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.347026 [3] L1 tensor(88615.1016, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.352810 [3] L2 tensor(5336.1084, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.355772 [3] L3 tensor(7739.6787, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.358003 [3] L4 tensor(492.9848, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.360389 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.376406 [3] L1 tensor(88563.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.381980 [3] L2 tensor(5327.1206, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.384235 [3] L3 tensor(7734.1802, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.386771 [3] L4 tensor(493.0863, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.389189 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.406625 [3] L1 tensor(88516.3828, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.412474 [3] L2 tensor(5318.8306, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.415701 [3] L3 tensor(7729.6104, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.418018 [3] L4 tensor(493.2258, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.420540 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.434100 [3] L1 tensor(88471.7578, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.439723 [3] L2 tensor(5310.9170, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.443798 [3] L3 tensor(7724.9180, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.447110 [3] L4 tensor(493.3699, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.449625 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.467147 [3] L1 tensor(88430.3281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.472763 [3] L2 tensor(5303.4268, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.475872 [3] L3 tensor(7718.8301, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.478182 [3] L4 tensor(493.3211, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.480684 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.494916 [3] L1 tensor(88391.3281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.500815 [3] L2 tensor(5296.4053, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.504433 [3] L3 tensor(7711.6699, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.507821 [3] L4 tensor(493.2770, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.511652 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.527443 [3] L1 tensor(88354.8281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.533933 [3] L2 tensor(5289.8555, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.536303 [3] L3 tensor(7704.6060, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.539056 [3] L4 tensor(493.2583, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.541454 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.553295 [3] L1 tensor(88320.0859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.559977 [3] L2 tensor(5283.6401, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.563924 [3] L3 tensor(7696.5981, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.566440 [3] L4 tensor(493.2222, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.568838 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.583276 [3] L1 tensor(88286.7422, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.587896 [3] L2 tensor(5277.6035, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.590490 [3] L3 tensor(7687.3916, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.593047 [3] L4 tensor(493.1475, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.595809 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.610903 [3] L1 tensor(88254.2266, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.616457 [3] L2 tensor(5272.0479, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.620088 [3] L3 tensor(7679.6270, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.622458 [3] L4 tensor(493.1252, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.624844 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.642956 [3] L1 tensor(88223.7500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.648815 [3] L2 tensor(5267.0005, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.652246 [3] L3 tensor(7673.1987, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.654926 [3] L4 tensor(493.1563, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.657370 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.670951 [3] L1 tensor(88195.3750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.673491 [3] L2 tensor(5262.1768, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.676111 [3] L3 tensor(7665.4185, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.678790 [3] L4 tensor(493.1565, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.681748 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.699617 [3] L1 tensor(88169.5234, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.705729 [3] L2 tensor(5257.5640, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.708117 [3] L3 tensor(7657.7012, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.710402 [3] L4 tensor(493.1763, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.712791 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.726158 [3] L1 tensor(88146.3125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.729829 [3] L2 tensor(5253.3750, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.733871 [3] L3 tensor(7649.1050, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.736597 [3] L4 tensor(493.1922, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.739206 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.754715 [3] L1 tensor(88124.6719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.760412 [3] L2 tensor(5249.3330, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.764043 [3] L3 tensor(7641.1553, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.766882 [3] L4 tensor(493.2054, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.769291 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.778435 [3] L1 tensor(88103.2969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.784252 [3] L2 tensor(5245.1709, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.787806 [3] L3 tensor(7633.7900, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.790086 [3] L4 tensor(493.2280, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.792533 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.807609 [3] L1 tensor(88083.1562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.814187 [3] L2 tensor(5241.0825, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.816446 [3] L3 tensor(7625.3706, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.820743 [3] L4 tensor(493.2283, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.823616 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.840947 [3] L1 tensor(88063.4609, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.846492 [3] L2 tensor(5237.4404, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.848955 [3] L3 tensor(7618.4922, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.851288 [3] L4 tensor(493.2749, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.853703 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.862976 [3] L1 tensor(88045.2500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.866302 [3] L2 tensor(5234.2280, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.869274 [3] L3 tensor(7613.3066, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.871893 [3] L4 tensor(493.3685, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.874300 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.886029 [3] L1 tensor(88028.2969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.892691 [3] L2 tensor(5230.9102, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.896135 [3] L3 tensor(7607.8213, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.898789 [3] L4 tensor(493.4725, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.901194 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.914601 [3] L1 tensor(88012.5234, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.917244 [3] L2 tensor(5227.5815, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.919909 [3] L3 tensor(7602.0537, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.922592 [3] L4 tensor(493.5916, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.925505 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.942886 [3] L1 tensor(87998.4141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.947767 [3] L2 tensor(5224.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.950337 [3] L3 tensor(7595.0410, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.952881 [3] L4 tensor(493.6966, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.955419 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.970676 [3] L1 tensor(87984.1484, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.977328 [3] L2 tensor(5220.7720, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.980646 [3] L3 tensor(7588.8262, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.983394 [3] L4 tensor(493.8004, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.985845 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.994859 [3] L1 tensor(87970.6172, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.997494 [3] L2 tensor(5216.4814, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:15.999987 [3] L3 tensor(7580.7461, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.002798 [3] L4 tensor(493.8594, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.005585 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.017670 [3] L1 tensor(87958.2656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.020408 [3] L2 tensor(5212.1792, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.022759 [3] L3 tensor(7571.4580, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.025143 [3] L4 tensor(493.8966, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.027741 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.044117 [3] L1 tensor(87945.2500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.050414 [3] L2 tensor(5207.4111, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.052767 [3] L3 tensor(7564.4092, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.055277 [3] L4 tensor(493.9751, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.058537 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.070688 [3] L1 tensor(87933.4375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.077377 [3] L2 tensor(5203.1699, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.080339 [3] L3 tensor(7559.2153, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.082798 [3] L4 tensor(494.0815, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.085250 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.094597 [3] L1 tensor(87923.9219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.097148 [3] L2 tensor(5198.9482, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.099534 [3] L3 tensor(7551.9668, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.101916 [3] L4 tensor(494.1476, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.104401 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.115151 [3] L1 tensor(87916.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.119626 [3] L2 tensor(5195.6895, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.123867 [3] L3 tensor(7545.8496, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.127558 [3] L4 tensor(494.2112, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.130024 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.139969 [3] L1 tensor(87910.0547, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.145498 [3] L2 tensor(5192.7354, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.150199 [3] L3 tensor(7538.3682, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.153045 [3] L4 tensor(494.2507, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.155701 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.167936 [3] L1 tensor(87903.9219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.173594 [3] L2 tensor(5189.7148, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.178046 [3] L3 tensor(7530.5679, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.180671 [3] L4 tensor(494.3002, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.183101 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.192650 [3] L1 tensor(87898.8750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.196188 [3] L2 tensor(5186.5176, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.199474 [3] L3 tensor(7522.4727, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.202553 [3] L4 tensor(494.3649, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.204945 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.216286 [3] L1 tensor(87894.3594, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.219371 [3] L2 tensor(5182.8486, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.222074 [3] L3 tensor(7512.7363, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.224758 [3] L4 tensor(494.2433, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.227669 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.244184 [3] L1 tensor(87889.8359, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.250080 [3] L2 tensor(5177.3799, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.252546 [3] L3 tensor(7504.2822, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.255168 [3] L4 tensor(494.1383, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.257818 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.267000 [3] L1 tensor(87877.6484, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.269710 [3] L2 tensor(5158.8506, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.272615 [3] L3 tensor(7497.7524, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.275633 [3] L4 tensor(494.0794, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.278727 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.292239 [3] L1 tensor(87862.9062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.297072 [3] L2 tensor(5139.9697, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.300008 [3] L3 tensor(7493.0527, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.302427 [3] L4 tensor(494.0676, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.305051 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.319979 [3] L1 tensor(87850.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.326154 [3] L2 tensor(5121.5024, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.328604 [3] L3 tensor(7486.5840, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.331043 [3] L4 tensor(494.0327, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.333686 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.348221 [3] L1 tensor(87847.7891, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.354225 [3] L2 tensor(5107.2832, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.357275 [3] L3 tensor(7478.6582, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.360257 [3] L4 tensor(493.9893, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.363506 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.372607 [3] L1 tensor(87847.3359, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.375825 [3] L2 tensor(5094.4785, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.379781 [3] L3 tensor(7472.2329, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.383662 [3] L4 tensor(493.9891, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.386130 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.398079 [3] L1 tensor(87846.8516, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.403627 [3] L2 tensor(5082.9023, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.407530 [3] L3 tensor(7464.3438, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.410878 [3] L4 tensor(493.9799, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.413532 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.423664 [3] L1 tensor(87845.4062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.426396 [3] L2 tensor(5071.0684, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.431867 [3] L3 tensor(7454.1279, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.435380 [3] L4 tensor(493.9351, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.437806 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.448645 [3] L1 tensor(87839.1719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.454474 [3] L2 tensor(5058.6934, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.459021 [3] L3 tensor(7445.1729, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.461367 [3] L4 tensor(493.8968, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.463940 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.475683 [3] L1 tensor(87832.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.480435 [3] L2 tensor(5046.3984, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.485627 [3] L3 tensor(7435.6611, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.488978 [3] L4 tensor(493.8749, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.492360 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.507127 [3] L1 tensor(87827.7500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.512877 [3] L2 tensor(5035.6021, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.516105 [3] L3 tensor(7425.6304, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.519387 [3] L4 tensor(493.8727, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.521795 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.536343 [3] L1 tensor(87824.9375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.542175 [3] L2 tensor(5026.0786, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.544757 [3] L3 tensor(7415.1670, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.548044 [3] L4 tensor(493.8905, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.550453 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.566453 [3] L1 tensor(87822.3906, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.571659 [3] L2 tensor(5017.4712, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.575581 [3] L3 tensor(7405.8457, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.578219 [3] L4 tensor(493.9127, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.580628 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.591689 [3] L1 tensor(87819.9609, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.597496 [3] L2 tensor(5009.7021, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.600054 [3] L3 tensor(7398.2642, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.602294 [3] L4 tensor(493.9695, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.604705 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.616996 [3] L1 tensor(87820.1172, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.623438 [3] L2 tensor(5002.7212, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.627723 [3] L3 tensor(7392.2432, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.630440 [3] L4 tensor(494.0606, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.634278 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.651484 [3] L1 tensor(87821.1875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.658290 [3] L2 tensor(4996.1846, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.660484 [3] L3 tensor(7383.5356, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.662690 [3] L4 tensor(494.1093, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.665127 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.675882 [3] L1 tensor(87822.3359, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.678574 [3] L2 tensor(4990.2451, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.681019 [3] L3 tensor(7373.4629, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.683491 [3] L4 tensor(494.1425, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.686141 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.703286 [3] L1 tensor(87823.6875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.708120 [3] L2 tensor(4984.8652, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.711975 [3] L3 tensor(7365.1074, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.714363 [3] L4 tensor(494.2082, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.716785 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.729773 [3] L1 tensor(87825.0547, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.733098 [3] L2 tensor(4980.0254, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.735676 [3] L3 tensor(7355.3647, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.738887 [3] L4 tensor(494.2617, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.742219 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.757513 [3] L1 tensor(87826.6328, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.762998 [3] L2 tensor(4975.6338, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.765217 [3] L3 tensor(7346.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.767933 [3] L4 tensor(494.3123, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.770702 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.787124 [3] L1 tensor(87828.1094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.792310 [3] L2 tensor(4971.6523, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.796832 [3] L3 tensor(7339.1299, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.800958 [3] L4 tensor(494.3657, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.805342 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.819745 [3] L1 tensor(87830.5781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.825466 [3] L2 tensor(4968.1919, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.828075 [3] L3 tensor(7332.8350, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.831427 [3] L4 tensor(494.4484, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.833836 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.843055 [3] L1 tensor(87834.9766, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.845876 [3] L2 tensor(4964.9360, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.848135 [3] L3 tensor(7323.6309, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.850797 [3] L4 tensor(494.4904, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.853549 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.865795 [3] L1 tensor(87839.1094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.870512 [3] L2 tensor(4962.0059, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.873148 [3] L3 tensor(7316.2178, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.876111 [3] L4 tensor(494.5668, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.879031 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.895817 [3] L1 tensor(87842.6094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.901621 [3] L2 tensor(4959.1416, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.904173 [3] L3 tensor(7307.6260, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.906878 [3] L4 tensor(494.6472, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.909292 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.921548 [3] L1 tensor(87845.6484, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.927874 [3] L2 tensor(4956.2622, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.930873 [3] L3 tensor(7297.9170, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.933956 [3] L4 tensor(494.7349, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.936436 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.946119 [3] L1 tensor(87848.4766, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.949607 [3] L2 tensor(4953.4678, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.953160 [3] L3 tensor(7287.2148, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.955900 [3] L4 tensor(494.8544, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.958315 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.968810 [3] L1 tensor(87851.2578, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.972175 [3] L2 tensor(4950.8477, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.975098 [3] L3 tensor(7274.8057, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.978409 [3] L4 tensor(494.9356, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:16.981617 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.995150 [3] L1 tensor(87853.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.000309 [3] L2 tensor(4948.4082, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.003369 [3] L3 tensor(7260.9014, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.005630 [3] L4 tensor(494.9958, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.008184 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.023651 [3] L1 tensor(87856.4609, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.029409 [3] L2 tensor(4946.2856, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.031923 [3] L3 tensor(7249.3174, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.035361 [3] L4 tensor(495.0984, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.037756 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.054737 [3] L1 tensor(87858.3828, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.061513 [3] L2 tensor(4944.3633, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.064090 [3] L3 tensor(7239.1226, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.066912 [3] L4 tensor(495.1625, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.069308 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.083676 [3] L1 tensor(87860., device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.090155 [3] L2 tensor(4942.6050, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.094174 [3] L3 tensor(7230.2114, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.096361 [3] L4 tensor(495.2415, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.098785 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.115263 [3] L1 tensor(87860.3750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.121767 [3] L2 tensor(4941.0249, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.124226 [3] L3 tensor(7223.1445, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.126925 [3] L4 tensor(495.3672, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.129337 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.146655 [3] L1 tensor(87858., device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.152370 [3] L2 tensor(4939.2759, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.155682 [3] L3 tensor(7212.8096, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.158171 [3] L4 tensor(495.4567, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.160682 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.176105 [3] L1 tensor(87852.9375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.181828 [3] L2 tensor(4937.5049, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.184433 [3] L3 tensor(7200.2646, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.187757 [3] L4 tensor(495.3598, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.190159 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.204551 [3] L1 tensor(87847.4844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.210251 [3] L2 tensor(4935.8750, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.213105 [3] L3 tensor(7186.3662, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.216195 [3] L4 tensor(495.2668, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.218593 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.235432 [3] L1 tensor(87842.7266, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.241575 [3] L2 tensor(4934.3506, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.244140 [3] L3 tensor(7171.2842, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.246371 [3] L4 tensor(495.1876, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.248762 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.263293 [3] L1 tensor(87838.3203, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.268100 [3] L2 tensor(4933.0273, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.272021 [3] L3 tensor(7158.6123, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.275377 [3] L4 tensor(495.1689, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.278035 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.289226 [3] L1 tensor(87834.4219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.294208 [3] L2 tensor(4931.7891, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.299614 [3] L3 tensor(7148.1392, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.302426 [3] L4 tensor(495.2084, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.304803 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.316202 [3] L1 tensor(87830.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.322568 [3] L2 tensor(4930.4761, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.325087 [3] L3 tensor(7136.3931, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.327651 [3] L4 tensor(495.2696, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.330044 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.346519 [3] L1 tensor(87827.0625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.352435 [3] L2 tensor(4929.1138, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.356086 [3] L3 tensor(7123.4688, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.358833 [3] L4 tensor(495.3551, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.361249 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.373171 [3] L1 tensor(87824.0859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.376388 [3] L2 tensor(4927.7314, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.379488 [3] L3 tensor(7109.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.382578 [3] L4 tensor(495.4666, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.385949 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.402726 [3] L1 tensor(87821.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.406189 [3] L2 tensor(4926.4697, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.409258 [3] L3 tensor(7097.2656, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.412405 [3] L4 tensor(495.5835, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.415358 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.429380 [3] L1 tensor(87819.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.433104 [3] L2 tensor(4925.3184, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.435870 [3] L3 tensor(7086.5986, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.438821 [3] L4 tensor(495.7116, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.441843 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.458603 [3] L1 tensor(87817.3438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.462712 [3] L2 tensor(4924.1738, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.466939 [3] L3 tensor(7072.6729, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.469725 [3] L4 tensor(495.8012, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.472314 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.489695 [3] L1 tensor(87816.3125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.494536 [3] L2 tensor(4923.0361, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.499172 [3] L3 tensor(7055.8457, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.501495 [3] L4 tensor(495.8672, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.504047 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.517546 [3] L1 tensor(87816.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.522743 [3] L2 tensor(4922.0430, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.524930 [3] L3 tensor(7041.5811, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.527141 [3] L4 tensor(495.9786, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.529671 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.547479 [3] L1 tensor(87816.3672, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.553232 [3] L2 tensor(4920.9980, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.555957 [3] L3 tensor(7025.3525, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.558649 [3] L4 tensor(495.9046, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.561097 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.569923 [3] L1 tensor(87816.6328, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.572491 [3] L2 tensor(4919.9131, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.574805 [3] L3 tensor(7007.4951, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.577251 [3] L4 tensor(495.8124, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.579902 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.595512 [3] L1 tensor(87816.7969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.601250 [3] L2 tensor(4918.9038, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.604044 [3] L3 tensor(6989.8047, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.607028 [3] L4 tensor(495.7487, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.609424 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.619230 [3] L1 tensor(87817.1250, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.621852 [3] L2 tensor(4918.0283, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.624035 [3] L3 tensor(6974.7393, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.626246 [3] L4 tensor(495.7438, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.628635 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.642117 [3] L1 tensor(87817.3672, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.646690 [3] L2 tensor(4917.2441, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.650201 [3] L3 tensor(6962.0801, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.653915 [3] L4 tensor(495.7943, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.656537 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.670714 [3] L1 tensor(87817.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.675122 [3] L2 tensor(4916.4922, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.679375 [3] L3 tensor(6951.0078, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.683364 [3] L4 tensor(495.8583, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.686139 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.701815 [3] L1 tensor(87817.5859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.707169 [3] L2 tensor(4915.6729, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.711474 [3] L3 tensor(6938.9263, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.714177 [3] L4 tensor(495.9427, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.716558 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.730533 [3] L1 tensor(87817.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.736022 [3] L2 tensor(4914.9434, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.739899 [3] L3 tensor(6925.6074, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.743203 [3] L4 tensor(496.0112, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.745965 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.762196 [3] L1 tensor(87817.3516, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.767871 [3] L2 tensor(4914.1479, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.771764 [3] L3 tensor(6911.4238, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.774486 [3] L4 tensor(496.1013, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.776886 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.791248 [3] L1 tensor(87817.0781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.797744 [3] L2 tensor(4913.3506, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.801207 [3] L3 tensor(6895.6016, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.803805 [3] L4 tensor(496.1635, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.806209 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.816324 [3] L1 tensor(87816.9844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.822126 [3] L2 tensor(4912.5967, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.826975 [3] L3 tensor(6881.6929, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.829434 [3] L4 tensor(496.2386, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.832026 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.847259 [3] L1 tensor(87816.7109, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.851989 [3] L2 tensor(4911.8398, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.856122 [3] L3 tensor(6866.4502, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.859573 [3] L4 tensor(496.1516, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.861993 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.877373 [3] L1 tensor(87816.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.884140 [3] L2 tensor(4911.1885, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.887944 [3] L3 tensor(6850.4536, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.890655 [3] L4 tensor(496.0685, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.893043 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.906447 [3] L1 tensor(87816.3594, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.910453 [3] L2 tensor(4910.6191, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.914215 [3] L3 tensor(6837.1035, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.918042 [3] L4 tensor(496.0426, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.920515 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.936708 [3] L1 tensor(87816.3594, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.940942 [3] L2 tensor(4909.9902, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.944673 [3] L3 tensor(6822.9180, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.947577 [3] L4 tensor(496.0471, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.949964 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.966699 [3] L1 tensor(87816.3516, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.972992 [3] L2 tensor(4909.4629, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.976057 [3] L3 tensor(6811.2065, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.978668 [3] L4 tensor(496.1016, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.981090 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.990042 [3] L1 tensor(87817.8906, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.992528 [3] L2 tensor(4908.9551, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.996456 [3] L3 tensor(6802.2363, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:17.998948 [3] L4 tensor(496.1695, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.001671 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.015860 [3] L1 tensor(87818.4844, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.021299 [3] L2 tensor(4908.3877, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.023821 [3] L3 tensor(6790.6660, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.026601 [3] L4 tensor(496.2030, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.029083 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.044110 [3] L1 tensor(87819.0391, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.049943 [3] L2 tensor(4907.8096, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.052383 [3] L3 tensor(6776.8120, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.054712 [3] L4 tensor(496.2157, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.057102 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.070328 [3] L1 tensor(87819.3047, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.075776 [3] L2 tensor(4907.1206, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.078324 [3] L3 tensor(6762.2666, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.080871 [3] L4 tensor(496.2554, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.083428 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.098906 [3] L1 tensor(87819.4375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.105371 [3] L2 tensor(4906.5566, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.108884 [3] L3 tensor(6750.1934, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.111551 [3] L4 tensor(496.3405, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.113958 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.131099 [3] L1 tensor(87820.1328, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.136851 [3] L2 tensor(4905.9561, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.139746 [3] L3 tensor(6737.9741, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.141937 [3] L4 tensor(496.3931, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.144306 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.153265 [3] L1 tensor(87820.9609, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.155858 [3] L2 tensor(4905.3721, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.158703 [3] L3 tensor(6729.5947, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.161771 [3] L4 tensor(496.4587, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.164971 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.175608 [3] L1 tensor(87821.6875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.180726 [3] L2 tensor(4904.7139, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.185341 [3] L3 tensor(6719.8472, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.188305 [3] L4 tensor(496.5469, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.190788 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.206771 [3] L1 tensor(87822.3906, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.211871 [3] L2 tensor(4904.1538, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.215804 [3] L3 tensor(6712.1387, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.218833 [3] L4 tensor(496.6760, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.221223 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.235066 [3] L1 tensor(87823.0547, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.239616 [3] L2 tensor(4903.5703, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.243648 [3] L3 tensor(6702.2544, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.247021 [3] L4 tensor(496.6421, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.249392 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.262591 [3] L1 tensor(87823.6875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.266945 [3] L2 tensor(4903.0459, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.270971 [3] L3 tensor(6690.9297, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.274709 [3] L4 tensor(496.6011, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.277060 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.285660 [3] L1 tensor(87824.3438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.288619 [3] L2 tensor(4902.5718, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.291097 [3] L3 tensor(6678.4316, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.293551 [3] L4 tensor(496.5623, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.296008 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.310723 [3] L1 tensor(87824.9688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.314726 [3] L2 tensor(4902.1846, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.318469 [3] L3 tensor(6668.1768, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.321072 [3] L4 tensor(496.5762, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.323468 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.339265 [3] L1 tensor(87825.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.345603 [3] L2 tensor(4901.8174, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.349080 [3] L3 tensor(6659.1904, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.351670 [3] L4 tensor(496.6059, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.354054 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.370597 [3] L1 tensor(87825.8750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.377051 [3] L2 tensor(4901.3413, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.379989 [3] L3 tensor(6648.9121, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.382613 [3] L4 tensor(496.6585, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.384999 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.401514 [3] L1 tensor(87825.9297, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.407309 [3] L2 tensor(4900.7842, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.411140 [3] L3 tensor(6636.2969, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.413349 [3] L4 tensor(496.6808, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.415753 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.430103 [3] L1 tensor(87825.6250, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.435865 [3] L2 tensor(4900.2134, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.439610 [3] L3 tensor(6621.6216, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.442866 [3] L4 tensor(496.6862, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.445251 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.456934 [3] L1 tensor(87825.2344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.462458 [3] L2 tensor(4899.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.467166 [3] L3 tensor(6606.3105, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.469381 [3] L4 tensor(496.7193, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.471784 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.487358 [3] L1 tensor(87824.8281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.493879 [3] L2 tensor(4899.0005, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.496099 [3] L3 tensor(6593.4424, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.498312 [3] L4 tensor(496.7961, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.500707 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.514949 [3] L1 tensor(87824.4688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.519673 [3] L2 tensor(4898.4658, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.524069 [3] L3 tensor(6581.9854, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.527391 [3] L4 tensor(496.8827, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.529867 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.546230 [3] L1 tensor(87823.9609, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.551962 [3] L2 tensor(4897.8975, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.555341 [3] L3 tensor(6568.4492, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.557545 [3] L4 tensor(496.9326, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.559959 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.568847 [3] L1 tensor(87823.5547, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.572070 [3] L2 tensor(4897.4316, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.574714 [3] L3 tensor(6556.8013, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.577504 [3] L4 tensor(497.0202, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.580079 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.596149 [3] L1 tensor(87823.2344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.601939 [3] L2 tensor(4896.9404, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.604161 [3] L3 tensor(6543.8242, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.607413 [3] L4 tensor(496.9702, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.609825 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.622985 [3] L1 tensor(87822.9609, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.628797 [3] L2 tensor(4896.4204, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.631864 [3] L3 tensor(6529.0703, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.634088 [3] L4 tensor(496.9082, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.636471 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.652396 [3] L1 tensor(87822.8984, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.658148 [3] L2 tensor(4895.8350, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.660378 [3] L3 tensor(6513.7393, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.662609 [3] L4 tensor(496.8792, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:3', grad_fn=<SumBackward0>)
20:33:18.665005 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:54.061272 [3] proc begin: <DistEnv 3/4 nccl>
20:33:54.097815 [3] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 3, |V|: 677, |E|: 2869>
20:33:54.110736 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3856 KiB |   3878 KiB |   3925 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     66 KiB |     88 KiB |    135 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3851 KiB |   3872 KiB |   3917 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     62 KiB |     83 KiB |    128 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18671 KiB |  18707 KiB |  18804 KiB | 135680 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1981 KiB |   2045 KiB |   2114 KiB | 135680 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:33:55.171687 [3] L1 tensor(91906.2109, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:56.529118 [3] L2 tensor(8166.0913, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:56.540286 [3] L3 tensor(8193.4414, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:56.543891 [3] L4 tensor(8182.3984, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:56.546869 [3] L5 tensor(8131.8223, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:56.549842 [3] L6 tensor(8150.5381, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:56.553311 [3] L7 tensor(8157.7505, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:56.556407 [3] L8 tensor(450.2043, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:56.590571 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.458897 [3] L1 tensor(92030.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.463794 [3] L2 tensor(8282.8516, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.466906 [3] L3 tensor(8357.2812, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.469888 [3] L4 tensor(8181.0479, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.473167 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.476126 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.479369 [3] L7 tensor(7993.9106, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.482335 [3] L8 tensor(459.1643, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.486426 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.510768 [3] L1 tensor(92075.4531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.515769 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.519750 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.522477 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.524780 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.527087 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.529402 [3] L7 tensor(7922.6777, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.531658 [3] L8 tensor(468.1117, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.534093 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.550856 [3] L1 tensor(92072.3750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.556167 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.559950 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.562927 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.565625 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.567938 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.570132 [3] L7 tensor(8014.0752, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.572353 [3] L8 tensor(476.9705, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.574852 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.591615 [3] L1 tensor(92088.2344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.598256 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.602011 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.604203 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.606401 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.608750 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.611201 [3] L7 tensor(8104.1484, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.614213 [3] L8 tensor(484.1437, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.617353 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.641701 [3] L1 tensor(92158.3281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.647024 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.649405 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.651993 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.654541 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.656868 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.659206 [3] L7 tensor(8171.7554, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.661478 [3] L8 tensor(491.6121, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.663966 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.687142 [3] L1 tensor(92215.9141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.693680 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.696141 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.698409 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.700721 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.703040 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.705406 [3] L7 tensor(8224.4043, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.707770 [3] L8 tensor(497.6973, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.710267 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.727870 [3] L1 tensor(92258.8984, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.733739 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.735943 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.738136 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.740307 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.742616 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.744896 [3] L7 tensor(8195.3975, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.747240 [3] L8 tensor(503.7690, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.749677 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.767826 [3] L1 tensor(92304.7188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.773596 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.776845 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.779158 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.781313 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.783571 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.785945 [3] L7 tensor(8266.2363, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.788228 [3] L8 tensor(510.6747, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.790684 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.814452 [3] L1 tensor(92351.0312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.819532 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.822180 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.825074 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.828614 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.830974 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.833226 [3] L7 tensor(8319.0059, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.835461 [3] L8 tensor(517.6276, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.837848 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.854539 [3] L1 tensor(92393.8594, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.860884 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.864132 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.866865 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.868999 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.871181 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.873316 [3] L7 tensor(8378.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.875609 [3] L8 tensor(524.2488, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.878052 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.894821 [3] L1 tensor(92392.6875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.901197 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.904361 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.906978 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.909112 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.911247 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.913535 [3] L7 tensor(8392.5078, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.915865 [3] L8 tensor(530.5532, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.918303 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.939383 [3] L1 tensor(92393.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.945095 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.947779 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.950422 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.952849 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.955128 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.957412 [3] L7 tensor(8399.4043, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.959668 [3] L8 tensor(537.3357, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.962003 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.979679 [3] L1 tensor(92393.1875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.985385 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.987927 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.990092 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.992218 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.994419 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.996710 [3] L7 tensor(8414.0801, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:57.998979 [3] L8 tensor(544.3667, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.001361 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.025657 [3] L1 tensor(92428.7656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.030886 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.035414 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.037725 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.039990 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.042264 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.044518 [3] L7 tensor(8413.8184, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.046810 [3] L8 tensor(550.5079, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.049163 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.066481 [3] L1 tensor(92428.5859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.071540 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.075732 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.078349 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.080497 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.082692 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.085070 [3] L7 tensor(8355.0488, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.087431 [3] L8 tensor(555.9419, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.090049 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.107491 [3] L1 tensor(92428.6953, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.114106 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.117549 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.120013 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.122158 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.124340 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.126787 [3] L7 tensor(8290.1934, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.129140 [3] L8 tensor(562.5477, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.131637 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.151998 [3] L1 tensor(92418.9375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.158114 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.160309 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.163359 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.165657 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.167850 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.170155 [3] L7 tensor(8219.0967, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.172503 [3] L8 tensor(569.4016, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.174926 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.198339 [3] L1 tensor(92402.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.203993 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.207752 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.210175 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.212301 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.214490 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.216837 [3] L7 tensor(8148.9014, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.219152 [3] L8 tensor(576.3892, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.221556 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.245078 [3] L1 tensor(92384.8047, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.251398 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.254933 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.257229 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.259659 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.261935 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.264154 [3] L7 tensor(8093.5479, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.266481 [3] L8 tensor(582.6779, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.268999 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.292254 [3] L1 tensor(92361.9453, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.298481 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.300714 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.302950 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.305171 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.307510 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.309837 [3] L7 tensor(8046.7803, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.312177 [3] L8 tensor(589.5211, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.314672 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.336062 [3] L1 tensor(92339.8750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.340059 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.343740 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.347256 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.349551 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.351917 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.354241 [3] L7 tensor(8028.6914, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.356557 [3] L8 tensor(596.7709, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.359896 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.384558 [3] L1 tensor(92315.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.390352 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.392595 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.394960 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.397304 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.399667 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.402000 [3] L7 tensor(7966.2676, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.404298 [3] L8 tensor(604.4969, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.406792 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.425601 [3] L1 tensor(92295.8906, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.430689 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.432952 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.435201 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.437390 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.439631 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.441877 [3] L7 tensor(7895.0420, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.444107 [3] L8 tensor(612.1814, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.446586 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.470723 [3] L1 tensor(92276.7891, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.475685 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.479892 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.482241 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.484572 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.486911 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.489215 [3] L7 tensor(7822.0205, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.491534 [3] L8 tensor(619.7263, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.494020 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.519053 [3] L1 tensor(92255.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.524047 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.527495 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.530048 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.532908 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.535799 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.538620 [3] L7 tensor(7735.8633, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.541468 [3] L8 tensor(627.0621, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.544412 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.567603 [3] L1 tensor(92245.4141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.573303 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.575957 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.578570 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.581098 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.583944 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.586783 [3] L7 tensor(7640.6553, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.589654 [3] L8 tensor(634.7565, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.592581 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.614785 [3] L1 tensor(92235.1875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.618477 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.621221 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.624241 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.627013 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.629592 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.634015 [3] L7 tensor(7652.3271, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.637132 [3] L8 tensor(642.3046, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.639981 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.663043 [3] L1 tensor(92220.7734, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.667513 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.671718 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.674291 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.677260 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.680181 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.683158 [3] L7 tensor(7614.2900, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.686068 [3] L8 tensor(650.1603, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.689110 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.706637 [3] L1 tensor(92207.3125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.712802 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.716010 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.718709 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.721364 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.724311 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.727227 [3] L7 tensor(7582.5835, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.730244 [3] L8 tensor(658.2415, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.733300 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.753583 [3] L1 tensor(92190.8438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.759116 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.761289 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.763493 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.765659 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.767814 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.770019 [3] L7 tensor(7547.9399, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.772177 [3] L8 tensor(665.7829, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.774590 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.795805 [3] L1 tensor(92185.7344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.802472 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.806094 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.808284 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.810446 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.812586 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.814775 [3] L7 tensor(7515.8604, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.816935 [3] L8 tensor(672.8223, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.819385 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.842141 [3] L1 tensor(92183.1094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.846418 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.849042 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.852014 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.854952 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.860322 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.864873 [3] L7 tensor(7476.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.868021 [3] L8 tensor(680.0869, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.870481 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.893615 [3] L1 tensor(92181.9062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.899092 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.902792 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.904980 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.907154 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.909322 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.911541 [3] L7 tensor(7443.5044, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.913722 [3] L8 tensor(687.2549, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.916149 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.939157 [3] L1 tensor(92180.3672, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.945643 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.950535 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.954688 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.959474 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.963450 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.965667 [3] L7 tensor(7404.4849, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.967851 [3] L8 tensor(694.3385, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:58.970289 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.998602 [3] L1 tensor(92164.9062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.004083 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.007800 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.010168 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.012452 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.014643 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.016829 [3] L7 tensor(7326.3232, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.019018 [3] L8 tensor(700.2833, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.021399 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.043658 [3] L1 tensor(92148.5391, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.047922 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.050910 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.054020 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.056486 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.058740 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.060945 [3] L7 tensor(7246.6230, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.063139 [3] L8 tensor(706.7782, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.065579 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.087316 [3] L1 tensor(92127.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.093861 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.096211 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.098526 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.100804 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.104559 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.108201 [3] L7 tensor(7189.6465, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.113577 [3] L8 tensor(713.7858, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.116518 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.141320 [3] L1 tensor(92107.5078, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.146892 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.151440 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.153643 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.155818 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.158084 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.160287 [3] L7 tensor(7135.2983, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.162460 [3] L8 tensor(720.7952, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.164855 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.185872 [3] L1 tensor(92092.4688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.192417 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.195667 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.197921 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.200291 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.202490 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.204696 [3] L7 tensor(7084.1738, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.206916 [3] L8 tensor(728.2336, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.209316 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.230808 [3] L1 tensor(92074.7422, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.235846 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.239732 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.242457 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.244615 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.246775 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.248975 [3] L7 tensor(7037.1904, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.251147 [3] L8 tensor(735.5088, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.253533 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.274799 [3] L1 tensor(92067.1562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.281287 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.283995 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.286196 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.288354 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.290562 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.292742 [3] L7 tensor(7023.4854, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.294973 [3] L8 tensor(743.4106, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.297371 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.321742 [3] L1 tensor(92060.0547, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.326505 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.331243 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.333586 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.335802 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.338011 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.340173 [3] L7 tensor(6988.1309, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.342354 [3] L8 tensor(752.2716, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.344777 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.361339 [3] L1 tensor(92051.6719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.367695 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.371820 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.374618 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.376763 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.378912 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.381076 [3] L7 tensor(6953.8452, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.383244 [3] L8 tensor(761.9441, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.385669 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.402376 [3] L1 tensor(92048.3516, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.408291 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.412044 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.414710 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.416857 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.419012 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.421202 [3] L7 tensor(6912.2715, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.423381 [3] L8 tensor(771.5643, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.425784 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.446879 [3] L1 tensor(92044.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.452693 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.455796 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.457969 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.460128 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.462299 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.464502 [3] L7 tensor(6886.2417, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.466679 [3] L8 tensor(781.2713, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.469069 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.486554 [3] L1 tensor(92047.0391, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.492734 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.495732 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.498096 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.500361 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.502725 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.504923 [3] L7 tensor(6890.9771, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.507092 [3] L8 tensor(790.4954, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.509509 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.532826 [3] L1 tensor(92048.0469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.537540 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.541228 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.544126 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.546306 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.548483 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.550697 [3] L7 tensor(6886.4268, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.552867 [3] L8 tensor(799.3633, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.555315 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.578028 [3] L1 tensor(92049.0625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.583323 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.587487 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.589692 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.592006 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.594244 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.596457 [3] L7 tensor(6894.8955, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.598642 [3] L8 tensor(808.7374, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.601041 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.624966 [3] L1 tensor(92048.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.630560 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.633844 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.636328 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.638491 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.640630 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.642835 [3] L7 tensor(6898.0254, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.645010 [3] L8 tensor(818.4190, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.647444 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.670528 [3] L1 tensor(92054.2812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.676434 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.679884 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.682393 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.684544 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.686709 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.688903 [3] L7 tensor(6895.3540, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.691077 [3] L8 tensor(826.7911, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.693467 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.716403 [3] L1 tensor(92055.1953, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.722344 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.727064 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.731986 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.736765 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.739886 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.742134 [3] L7 tensor(6929.4932, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.744338 [3] L8 tensor(835.1722, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.746789 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.763983 [3] L1 tensor(92060.4219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.770165 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.772798 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.775152 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.777297 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.779459 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.781663 [3] L7 tensor(6966.1211, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.783825 [3] L8 tensor(843.9580, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.786260 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.804276 [3] L1 tensor(92063.9688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.810303 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.812869 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.815308 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.817724 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.819874 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.822053 [3] L7 tensor(7004.0137, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.824191 [3] L8 tensor(852.5330, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.826601 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.843826 [3] L1 tensor(92065.7422, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.850290 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.852477 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.854684 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.856860 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.859077 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.861252 [3] L7 tensor(6996.9292, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.863465 [3] L8 tensor(861.7837, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.865866 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.890315 [3] L1 tensor(92066.0156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.895293 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.897766 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.900587 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.903260 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.905555 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.907773 [3] L7 tensor(6986.4253, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.910022 [3] L8 tensor(871.3632, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.912531 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.935844 [3] L1 tensor(92066.9062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.943330 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.945789 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.948289 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.950796 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.953328 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.955842 [3] L7 tensor(6986.7188, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.958384 [3] L8 tensor(881.7352, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.961039 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.984043 [3] L1 tensor(92066.3828, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.990153 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.992663 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.995596 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:33:59.997844 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.000344 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.002942 [3] L7 tensor(6997.6528, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.005504 [3] L8 tensor(893.2026, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.008181 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.030090 [3] L1 tensor(92064.5859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.035090 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.039450 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.041912 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.044339 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.046943 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.049517 [3] L7 tensor(6996.4224, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.052125 [3] L8 tensor(905.3839, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.054924 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.076642 [3] L1 tensor(92061.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.082177 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.085351 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.088432 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.090662 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.092836 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.095105 [3] L7 tensor(7007.0684, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.097315 [3] L8 tensor(917.2354, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.099777 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.122631 [3] L1 tensor(92057.4062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.128735 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.134175 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.136588 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.138989 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.141576 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.144178 [3] L7 tensor(7016.7778, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.146786 [3] L8 tensor(928.8240, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.149480 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.174207 [3] L1 tensor(92052.2812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.178633 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.183139 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.185739 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.188145 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.190558 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.193153 [3] L7 tensor(7031.3984, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.195744 [3] L8 tensor(938.5357, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.198448 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.220129 [3] L1 tensor(92047.0469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.225912 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.228339 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.231287 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.233530 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.236109 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.238660 [3] L7 tensor(7052.1240, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.241241 [3] L8 tensor(948.2116, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.243932 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.265689 [3] L1 tensor(92040.3438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.271078 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.275443 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.277995 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.280223 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.282781 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.285298 [3] L7 tensor(7068.1011, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.287909 [3] L8 tensor(958.4678, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.290597 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.314218 [3] L1 tensor(92032.8984, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.318674 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.323142 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.325930 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.328658 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.331246 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.333812 [3] L7 tensor(7079.6675, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.336419 [3] L8 tensor(967.2245, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.339160 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.358623 [3] L1 tensor(92024.8281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.364341 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.367629 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.369824 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.372308 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.374943 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.377455 [3] L7 tensor(7084.8979, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.379674 [3] L8 tensor(976.2678, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.382114 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.405345 [3] L1 tensor(92016.1562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.410629 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.414144 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.416732 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.419285 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.421912 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.424390 [3] L7 tensor(7097.3198, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.426984 [3] L8 tensor(985.8372, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.429687 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.451912 [3] L1 tensor(92006.9766, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.457491 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.461219 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.463923 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.466227 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.468868 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.471402 [3] L7 tensor(7090.5596, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.474009 [3] L8 tensor(996.5497, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.476708 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.499071 [3] L1 tensor(91997.3281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.504562 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.508020 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.510921 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.513052 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.515496 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.518041 [3] L7 tensor(7083.6445, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.520633 [3] L8 tensor(1006.6285, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.523332 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.546150 [3] L1 tensor(91987.2891, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.551322 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.554088 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.556695 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.559213 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.561545 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.564125 [3] L7 tensor(7090.2759, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.566733 [3] L8 tensor(1017.0908, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.569472 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.593750 [3] L1 tensor(91976.7500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.599464 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.603384 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.605565 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.608120 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.610746 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.613324 [3] L7 tensor(7101.8350, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.615969 [3] L8 tensor(1028.5674, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.618711 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.641756 [3] L1 tensor(91965.6250, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.646806 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.649201 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.652156 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.654766 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.657347 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.659909 [3] L7 tensor(7102.4204, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.662541 [3] L8 tensor(1040.0034, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.665243 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.683088 [3] L1 tensor(91954.2734, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.688813 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.691760 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.693974 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.696580 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.699168 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.701745 [3] L7 tensor(7109.1299, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.704386 [3] L8 tensor(1050.3302, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.707161 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.730332 [3] L1 tensor(91942.6094, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.734852 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.738077 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.740803 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.743249 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.745772 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.748278 [3] L7 tensor(7115.3667, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.750893 [3] L8 tensor(1061.2329, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.753534 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.771275 [3] L1 tensor(91930.6797, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.775186 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.779573 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.781845 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.784448 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.787042 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.789593 [3] L7 tensor(7121.3765, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.792185 [3] L8 tensor(1071.2010, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.794977 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.814346 [3] L1 tensor(91918.4531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.819584 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.823629 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.826411 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.828546 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.831123 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.833690 [3] L7 tensor(7129.3828, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.836004 [3] L8 tensor(1080.0259, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.838377 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.858315 [3] L1 tensor(91905.5078, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.864420 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.867871 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.870477 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.872625 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.874752 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.876895 [3] L7 tensor(7137.1338, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.879166 [3] L8 tensor(1087.9192, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.881509 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.898464 [3] L1 tensor(91894.9219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.904509 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.907969 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.910694 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.912834 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.914964 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.917114 [3] L7 tensor(7156.0615, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.919238 [3] L8 tensor(1097.4600, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.921575 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.938366 [3] L1 tensor(91883.9766, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.944408 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.947909 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.950532 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.952684 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.954870 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.957011 [3] L7 tensor(7170.8271, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.959186 [3] L8 tensor(1107.8916, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.961603 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.984979 [3] L1 tensor(91872.5938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.990827 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.993944 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.996470 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:00.998626 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.000790 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.003017 [3] L7 tensor(7185.2031, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.005229 [3] L8 tensor(1118.9893, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.007651 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.024739 [3] L1 tensor(91861., device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.030137 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.032298 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.034551 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.037299 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.039818 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.042286 [3] L7 tensor(7199.2324, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.044897 [3] L8 tensor(1130.3167, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.047624 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.065198 [3] L1 tensor(91849.3047, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.070384 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.075171 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.080634 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.084074 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.086363 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.088893 [3] L7 tensor(7210.0137, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.091481 [3] L8 tensor(1142.5631, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.094176 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.117942 [3] L1 tensor(91838.0625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.123145 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.127469 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.129885 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.132226 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.134797 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.137322 [3] L7 tensor(7226.8770, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.139935 [3] L8 tensor(1154.7158, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.142630 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.162231 [3] L1 tensor(91821.5547, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.165697 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.168095 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.170419 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.172833 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.175320 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.177558 [3] L7 tensor(7240.4722, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.179794 [3] L8 tensor(1167.4799, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.182496 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.214691 [3] L1 tensor(91805.3438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.220500 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.226022 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.231212 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.234752 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.238915 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.245913 [3] L7 tensor(7257.1367, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.249133 [3] L8 tensor(1180.9768, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.252058 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.270775 [3] L1 tensor(91789.4141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.275954 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.280446 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.285202 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.290412 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.294595 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.299664 [3] L7 tensor(7272.3467, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.303732 [3] L8 tensor(1194.1365, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.307059 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.327727 [3] L1 tensor(91775.6016, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.331322 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.334124 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.336910 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.339433 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.341606 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.344034 [3] L7 tensor(7296.3428, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.346615 [3] L8 tensor(1206.1471, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.349371 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.372095 [3] L1 tensor(91761.8359, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.375821 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.378232 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.380968 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.384264 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.388450 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.391336 [3] L7 tensor(7308.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.394292 [3] L8 tensor(1218.3230, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.397214 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.423842 [3] L1 tensor(91748.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.428401 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.432883 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.438587 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.442965 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.445396 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.447800 [3] L7 tensor(7320.4033, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.450024 [3] L8 tensor(1230.9602, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.452898 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.479726 [3] L1 tensor(91733.7422, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.486196 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.491036 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.495677 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.500962 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.506619 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.511696 [3] L7 tensor(7331.7754, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.515734 [3] L8 tensor(1243.7366, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.520118 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.543862 [3] L1 tensor(91725.0156, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.547681 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.551240 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.557397 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.560507 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.562814 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.565779 [3] L7 tensor(7327.8857, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.568855 [3] L8 tensor(1255.2939, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.573251 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.596129 [3] L1 tensor(91716.0234, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.599336 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.601767 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.604128 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.606381 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.608778 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.611102 [3] L7 tensor(7322.5342, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.613468 [3] L8 tensor(1266.2397, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.621283 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.645383 [3] L1 tensor(91704.2969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.649891 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.654278 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.656605 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.660087 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.663581 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.666987 [3] L7 tensor(7321.3926, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.670345 [3] L8 tensor(1276.5854, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.674400 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.695706 [3] L1 tensor(91693.7188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.703767 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.709733 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.715564 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.721238 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.727139 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.732538 [3] L7 tensor(7320.0356, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.738166 [3] L8 tensor(1287.2778, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.747165 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.767123 [3] L1 tensor(91684.1562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.772764 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.777971 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.783622 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.788404 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.793407 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.798080 [3] L7 tensor(7318.8408, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.800707 [3] L8 tensor(1298.6079, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.805177 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.824624 [3] L1 tensor(91675.5469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.830275 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.834255 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.837318 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.839991 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.843006 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.845941 [3] L7 tensor(7317.9033, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.849041 [3] L8 tensor(1309.6160, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.851898 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.887321 [3] L1 tensor(91665.1484, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.894518 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.899756 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.905694 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.911639 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.917109 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.922650 [3] L7 tensor(7317.0186, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.927562 [3] L8 tensor(1321.7622, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.932370 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.952870 [3] L1 tensor(91655.7656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.961502 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.965566 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.968246 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.971006 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.974227 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.977438 [3] L7 tensor(7316.6641, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.980596 [3] L8 tensor(1335.4668, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:01.984042 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.016059 [3] L1 tensor(91647.3438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.021218 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.024078 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.027642 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.031733 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.035476 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.039210 [3] L7 tensor(7315.8281, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.042904 [3] L8 tensor(1349.3062, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.047936 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.074002 [3] L1 tensor(91639.7344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.080255 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.085861 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.091585 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.097334 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.103001 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.108611 [3] L7 tensor(7315.3213, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.114334 [3] L8 tensor(1363.8130, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.120755 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.147126 [3] L1 tensor(91632.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.156381 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.159669 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.165403 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.171097 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.177518 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.182439 [3] L7 tensor(7315.0063, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.188190 [3] L8 tensor(1377.6981, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.193993 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.232368 [3] L1 tensor(91626.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.238923 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.244645 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.250318 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.255968 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.261774 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.268234 [3] L7 tensor(7314.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.273922 [3] L8 tensor(1391.4271, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.280852 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.302113 [3] L1 tensor(91620.2812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.305309 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.308355 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.311438 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.314035 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.317968 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.320469 [3] L7 tensor(7314.2070, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.323013 [3] L8 tensor(1405.6222, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.325754 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.349985 [3] L1 tensor(91615.0547, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.355552 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.360345 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.365247 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.369698 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.372191 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.374835 [3] L7 tensor(7313.7842, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.377624 [3] L8 tensor(1418.0425, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.380454 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.408450 [3] L1 tensor(91611.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.412344 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.417013 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.421716 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.427259 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.431537 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.435945 [3] L7 tensor(7313.3398, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.440155 [3] L8 tensor(1429.7803, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.444600 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.468148 [3] L1 tensor(91607.5391, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.472458 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.475675 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.478340 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.480969 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.483632 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.486176 [3] L7 tensor(7312.8184, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.488763 [3] L8 tensor(1440.7744, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.491906 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.512064 [3] L1 tensor(91604.3438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.515694 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.518541 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.522618 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.525304 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.528464 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.532818 [3] L7 tensor(7312.3081, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.537059 [3] L8 tensor(1452.0017, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.541628 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.563590 [3] L1 tensor(91601.4688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.568632 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.572898 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.579139 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.584008 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.588989 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.593868 [3] L7 tensor(7311.5332, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.596803 [3] L8 tensor(1463.4434, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.603850 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.629675 [3] L1 tensor(91598.8984, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.636033 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.641851 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.647458 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.651472 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.655727 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.658149 [3] L7 tensor(7310.7427, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.660428 [3] L8 tensor(1475.3798, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.664035 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.690137 [3] L1 tensor(91596.6016, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.694528 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.698929 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.703578 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.708048 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.712637 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.717703 [3] L7 tensor(7310.3271, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.720737 [3] L8 tensor(1487.6847, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.724370 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.753884 [3] L1 tensor(91594.5391, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.760042 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.763582 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.765827 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.768216 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.770696 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.773082 [3] L7 tensor(7309.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.776585 [3] L8 tensor(1500.0754, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.779737 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.801628 [3] L1 tensor(91592.7031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.810778 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.815891 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.819874 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.823217 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.826725 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.828921 [3] L7 tensor(7309.5820, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.831127 [3] L8 tensor(1513.7412, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.833712 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.858673 [3] L1 tensor(91591.0859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.864528 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.867602 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.869796 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.873662 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.879063 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.881288 [3] L7 tensor(7309.2988, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.883524 [3] L8 tensor(1527.6387, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.886034 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.909032 [3] L1 tensor(91589.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.915549 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.919718 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.922168 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.924425 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.926652 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.928962 [3] L7 tensor(7308.9502, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.931311 [3] L8 tensor(1541.2878, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.933811 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.957992 [3] L1 tensor(91586.4062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.962809 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.965254 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.967732 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.970155 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.972390 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.974708 [3] L7 tensor(7308.2002, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.976990 [3] L8 tensor(1554.8827, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:02.979461 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.997611 [3] L1 tensor(91583.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.001980 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.004682 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.006894 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.009287 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.011636 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.013957 [3] L7 tensor(7307.4434, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.016305 [3] L8 tensor(1566.3264, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.018764 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.041914 [3] L1 tensor(91581.1328, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.047370 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.051545 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.054075 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.056315 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.058537 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.060742 [3] L7 tensor(7306.6514, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.063086 [3] L8 tensor(1578.8008, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.065588 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.088795 [3] L1 tensor(91578.8750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.095320 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.098987 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.101164 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.103361 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.105514 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.107744 [3] L7 tensor(7305.5166, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.109945 [3] L8 tensor(1590.5959, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.112401 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.132946 [3] L1 tensor(91576.8438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.138613 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.143469 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.147722 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.150091 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.152330 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.154587 [3] L7 tensor(7304.7495, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.156789 [3] L8 tensor(1602.1790, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.159291 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.182178 [3] L1 tensor(91575.0938, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.188350 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.191567 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.193844 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.196118 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.198428 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.200698 [3] L7 tensor(7304.0596, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.202973 [3] L8 tensor(1612.7454, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.205465 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.235952 [3] L1 tensor(91573.5469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.241804 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.244066 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.246653 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.248971 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.251393 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.253826 [3] L7 tensor(7303.4873, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.256237 [3] L8 tensor(1625.0591, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.258773 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.279724 [3] L1 tensor(91572.4141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.284086 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.287287 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.290766 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.292992 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.295191 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.297403 [3] L7 tensor(7303.1309, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.299637 [3] L8 tensor(1637.4131, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.302134 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.327044 [3] L1 tensor(91569.6953, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.333537 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.335970 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.338245 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.340510 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.342754 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.345047 [3] L7 tensor(7303.3247, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.347350 [3] L8 tensor(1649.8020, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.349840 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.367838 [3] L1 tensor(91567.2656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.374162 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.376380 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.378713 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.380976 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.383264 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.385545 [3] L7 tensor(7303.9424, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.387830 [3] L8 tensor(1662.8516, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.390306 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.407814 [3] L1 tensor(91565.0859, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.414250 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.416450 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.419077 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.421326 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.423545 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.425770 [3] L7 tensor(7304.7437, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.427958 [3] L8 tensor(1675.3856, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.430405 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.454082 [3] L1 tensor(91563.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.456983 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.459384 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.461762 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.464180 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.466595 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.468878 [3] L7 tensor(7305.5039, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.471138 [3] L8 tensor(1688.0088, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.473553 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.496320 [3] L1 tensor(91561.4062, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.502425 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.504808 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.507642 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.509836 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.512090 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.514272 [3] L7 tensor(7305.4678, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.516543 [3] L8 tensor(1699.0746, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.519134 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.542041 [3] L1 tensor(91559.8438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.546434 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.551136 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.553711 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.557238 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.562891 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.567279 [3] L7 tensor(7305.4199, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.569501 [3] L8 tensor(1709.9758, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.572005 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.594950 [3] L1 tensor(91558.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.601055 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.604339 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.607020 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.609193 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.613420 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.618376 [3] L7 tensor(7305.3774, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.622874 [3] L8 tensor(1720.4250, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.625532 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.649018 [3] L1 tensor(91557.2969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.653074 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.656860 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.660128 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.662417 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.664723 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.667119 [3] L7 tensor(7305.5938, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.669421 [3] L8 tensor(1731.5405, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.671931 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.695125 [3] L1 tensor(91556.2188, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.701444 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.704108 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.706606 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.708803 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.711018 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.713324 [3] L7 tensor(7305.9639, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.715664 [3] L8 tensor(1741.6255, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.718168 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.741587 [3] L1 tensor(91555.2812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.747008 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.750941 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.753222 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.755716 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.757955 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.760201 [3] L7 tensor(7305.7988, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.762551 [3] L8 tensor(1751.8569, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.765053 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.787233 [3] L1 tensor(91554.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.793046 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.796018 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.798466 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.800802 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.803190 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.805566 [3] L7 tensor(7305.7085, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.807953 [3] L8 tensor(1761.5076, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.810475 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.828877 [3] L1 tensor(91553.7969, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.834460 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.836668 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.838886 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.841110 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.843532 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.845930 [3] L7 tensor(7305.6274, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.848224 [3] L8 tensor(1771.3596, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.850953 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.871950 [3] L1 tensor(91558.5312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.876040 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.878997 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.882201 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.884432 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.887043 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.889379 [3] L7 tensor(7305.7729, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.891714 [3] L8 tensor(1778.0099, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.894182 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.918075 [3] L1 tensor(91562.8281, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.923850 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.927278 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.929653 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.932135 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.934506 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.936816 [3] L7 tensor(7305.7598, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.939133 [3] L8 tensor(1782.4081, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.941645 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.965824 [3] L1 tensor(91572.8438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.971224 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.975431 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.977821 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.981229 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.983417 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.985686 [3] L7 tensor(7305.9077, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.987987 [3] L8 tensor(1790.0247, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:03.990511 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.014582 [3] L1 tensor(91581.9688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.021143 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.023942 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.026261 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.028545 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.030862 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.033164 [3] L7 tensor(7305.8701, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.035554 [3] L8 tensor(1799.4231, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.038034 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.061825 [3] L1 tensor(91590.2578, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.068287 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.071582 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.073807 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.076007 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.078227 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.080451 [3] L7 tensor(7305.6919, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.082804 [3] L8 tensor(1807.3804, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.085272 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.103418 [3] L1 tensor(91597.8125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.109915 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.112116 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.114485 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.116761 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.119031 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.121338 [3] L7 tensor(7305.4805, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.123677 [3] L8 tensor(1815.8324, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.126176 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.149616 [3] L1 tensor(91604.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.154684 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.156884 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.159095 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.161287 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.163577 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.165908 [3] L7 tensor(7305.0278, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.168203 [3] L8 tensor(1826.3904, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.170687 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.188664 [3] L1 tensor(91600.9688, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.194070 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.198442 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.202988 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.208292 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.211716 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.213925 [3] L7 tensor(7305.0186, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.216187 [3] L8 tensor(1836.8647, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.218654 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.239849 [3] L1 tensor(91597.6875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.243461 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.246369 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.249581 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.252145 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.254545 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.256931 [3] L7 tensor(7305.2495, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.259179 [3] L8 tensor(1847.7650, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.261695 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.283739 [3] L1 tensor(91594.6719, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.289517 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.292026 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.294637 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.296996 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.299302 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.301613 [3] L7 tensor(7305.3184, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.303953 [3] L8 tensor(1859.8657, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.306452 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.324830 [3] L1 tensor(91600.2266, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.328429 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.331590 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.334959 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.337167 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.339365 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.341577 [3] L7 tensor(7305.7202, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.343932 [3] L8 tensor(1872.3767, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.346400 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.363198 [3] L1 tensor(91605.2578, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.367638 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.371128 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.374346 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.376559 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.378797 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.381043 [3] L7 tensor(7305.9404, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.383273 [3] L8 tensor(1884.2997, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.385788 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.402930 [3] L1 tensor(91609.7891, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.408905 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.412321 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.415068 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.417238 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.419416 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.421605 [3] L7 tensor(7306.0884, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.423830 [3] L8 tensor(1895.4619, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.426363 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.443402 [3] L1 tensor(91613.8359, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.449923 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.453624 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.456104 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.458295 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.460501 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.462701 [3] L7 tensor(7306.2393, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.464992 [3] L8 tensor(1905.0767, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.467485 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.491654 [3] L1 tensor(91617.4922, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.495853 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.498829 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.502108 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.504560 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.506741 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.508981 [3] L7 tensor(7306.7227, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.511308 [3] L8 tensor(1914.0349, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.513780 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.532116 [3] L1 tensor(91622.1250, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.536995 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.541020 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.543772 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.545936 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.548078 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.550309 [3] L7 tensor(7307.0469, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.552614 [3] L8 tensor(1921.9152, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.555084 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.574391 [3] L1 tensor(91626.3203, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.580156 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.583611 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.585996 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.588487 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.590816 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.593182 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.595543 [3] L8 tensor(1929.7485, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.598025 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.621971 [3] L1 tensor(91630.1016, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.627165 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.631311 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.633755 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.635956 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.638183 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.640386 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.642638 [3] L8 tensor(1938.5867, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.645119 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.663384 [3] L1 tensor(91630.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.669125 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.672013 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.674201 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.676368 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.678567 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.680901 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.683214 [3] L8 tensor(1947.7693, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.685680 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.703612 [3] L1 tensor(91630.8203, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.709502 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.712036 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.714328 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.716886 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.719414 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.721739 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.724069 [3] L8 tensor(1954.5996, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.726519 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.744243 [3] L1 tensor(91631.0547, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.750237 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.752536 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.754893 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.757201 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.759555 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.761885 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.764203 [3] L8 tensor(1961.9805, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.766704 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.784891 [3] L1 tensor(91631.1875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.790417 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.792617 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.794832 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.797024 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.799236 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.801544 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.803859 [3] L8 tensor(1966.5870, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.806365 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.824266 [3] L1 tensor(91631.2031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.830181 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.832424 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.834785 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.837097 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.839439 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.841807 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.847248 [3] L8 tensor(1970.8381, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.849829 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.869093 [3] L1 tensor(91626.4219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.874401 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.876600 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.878809 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.881224 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.883596 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.885822 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.887988 [3] L8 tensor(1978.1454, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.890451 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.914865 [3] L1 tensor(91621.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.920922 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.924283 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.927011 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.929210 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.931422 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.933649 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.935958 [3] L8 tensor(1984.0890, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.938596 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.962000 [3] L1 tensor(91617.7578, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.966836 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.969328 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.972119 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.974878 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.977043 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.979332 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.981646 [3] L8 tensor(1990.6523, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:04.984084 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.001993 [3] L1 tensor(91613.7344, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.007143 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.011458 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.014043 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.016334 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.018643 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.021070 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.023592 [3] L8 tensor(1995.3953, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.026275 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.044541 [3] L1 tensor(91609.9141, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.050097 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.052992 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.056188 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.058422 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.060611 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.063089 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.065408 [3] L8 tensor(2000.4619, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.067901 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.086721 [3] L1 tensor(91606.2734, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.092506 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.095765 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.098077 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.100525 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.102850 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.105220 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.107546 [3] L8 tensor(2007.9070, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.110064 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.129654 [3] L1 tensor(91602.7812, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.134743 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.139124 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.141404 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.143880 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.146197 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.148500 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.150863 [3] L8 tensor(2017.1493, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.153338 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.177348 [3] L1 tensor(91591.8125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.182614 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.186921 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.189303 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.191628 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.193948 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.196292 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.198647 [3] L8 tensor(2026.3612, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.201150 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.223172 [3] L1 tensor(91581.6953, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.229415 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.232185 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.234629 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.236847 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.239174 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.241512 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.243868 [3] L8 tensor(2036.9098, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.246342 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.264426 [3] L1 tensor(91572.3750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.270243 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.272508 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.274813 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.277102 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.279466 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.281803 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.284088 [3] L8 tensor(2045.9766, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.286566 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.304304 [3] L1 tensor(91563.7656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.310330 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.312674 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.315133 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.317568 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.319973 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.322458 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.324883 [3] L8 tensor(2055.1167, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.327555 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.347526 [3] L1 tensor(91555.8047, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.354369 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.359061 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.361249 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.363528 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.366669 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.369943 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.372710 [3] L8 tensor(2064.7595, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.375449 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.398355 [3] L1 tensor(91548.4375, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.404052 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.407975 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.410447 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.412636 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.414945 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.417268 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.419590 [3] L8 tensor(2074.9424, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.422088 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.445113 [3] L1 tensor(91536.5312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.450799 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.454728 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.456951 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.459208 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.461508 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.463888 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.466202 [3] L8 tensor(2086.4458, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.468672 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.491089 [3] L1 tensor(91525.6406, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.496846 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.499901 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.502134 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.504333 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.506536 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.508831 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.511151 [3] L8 tensor(2098.4275, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.513584 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.532281 [3] L1 tensor(91515.6562, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.538018 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.540221 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.542461 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.544653 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.546898 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.549245 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.551596 [3] L8 tensor(2107.1714, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.554126 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.572544 [3] L1 tensor(91506.5000, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.578349 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.580707 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.583816 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.586121 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.588512 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.591035 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.593482 [3] L8 tensor(2116.1304, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.596165 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.614418 [3] L1 tensor(91498.1172, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.617197 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.619814 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.622764 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.625254 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.627616 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.629895 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.632076 [3] L8 tensor(2126.3237, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.634530 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.652319 [3] L1 tensor(91488.9219, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.657803 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.660737 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.663229 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.665697 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.668177 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.670673 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.673128 [3] L8 tensor(2136.7031, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.675787 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.694777 [3] L1 tensor(91479.0469, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.700524 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.703737 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.706097 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.708536 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.710807 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.713150 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.715461 [3] L8 tensor(2145.9819, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.717920 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.741855 [3] L1 tensor(91470.0312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.748478 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.751558 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.753873 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.756175 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.758524 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.760857 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.764626 [3] L8 tensor(2156.6565, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.767434 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.785387 [3] L1 tensor(91460.4531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.790448 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.792628 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.794971 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.797289 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.799587 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.801917 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.804196 [3] L8 tensor(2167.7449, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.806668 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.824485 [3] L1 tensor(91451.7031, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.830523 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.832831 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.835373 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.837797 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.840199 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.842673 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.845114 [3] L8 tensor(2176.8025, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.847710 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.870492 [3] L1 tensor(91440.7656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.876952 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.879915 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.882224 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.884534 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.886847 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.889196 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.891551 [3] L8 tensor(2188.1221, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.894056 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.917864 [3] L1 tensor(91429.5625, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.922671 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.927014 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.929511 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.931779 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.934083 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.936599 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.938918 [3] L8 tensor(2197.4336, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.941421 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.964994 [3] L1 tensor(91419.4531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.970345 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.973670 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.976413 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.978615 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.980819 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.983181 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.985468 [3] L8 tensor(2208.6543, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:05.987931 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.010400 [3] L1 tensor(91414.3438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.015643 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.018293 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.020941 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.023338 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.025512 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.027737 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.030065 [3] L8 tensor(2220.6272, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.032515 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.050139 [3] L1 tensor(91411.1875, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.055487 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.059417 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.062055 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.064435 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.066733 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.069012 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.071377 [3] L8 tensor(2233.1575, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.073864 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.097570 [3] L1 tensor(91408.3125, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.102228 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.107004 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.109307 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.111578 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.113921 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.116262 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.118607 [3] L8 tensor(2246.8120, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.121054 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.139063 [3] L1 tensor(91405.7500, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.144114 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.147227 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.150355 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.152587 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.154835 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.157087 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.159324 [3] L8 tensor(2259.6453, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.161860 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.185552 [3] L1 tensor(91403.9531, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.190673 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.194792 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.197126 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.199452 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.201832 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.204132 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.206442 [3] L8 tensor(2271.0356, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.208916 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.230618 [3] L1 tensor(91402.3438, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.236635 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.239807 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.242035 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.244240 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.246469 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.248744 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.251140 [3] L8 tensor(2285.2490, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.253900 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.272065 [3] L1 tensor(91400.8906, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.277903 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.280127 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.282340 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.284549 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.286755 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.289101 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.291415 [3] L8 tensor(2299.2354, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.293901 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.311649 [3] L1 tensor(91399.5781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.317506 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.320241 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.322588 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.324885 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.327249 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.329679 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.332398 [3] L8 tensor(2312.9934, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.335106 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.358312 [3] L1 tensor(91398.3984, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.362842 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.367358 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.369825 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.372125 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.374443 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.376779 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.379154 [3] L8 tensor(2323.4746, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.381627 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.399052 [3] L1 tensor(91397.3359, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.404478 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.408003 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.410883 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.413085 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.415285 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.417518 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.419743 [3] L8 tensor(2333.1545, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.422308 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.439458 [3] L1 tensor(91396.3750, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.445196 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.449160 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.451976 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.454535 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.456840 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.459122 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.461405 [3] L8 tensor(2343.0676, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.463954 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.486338 [3] L1 tensor(91395.5312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.491327 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.493879 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.496430 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.498878 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.501043 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.503289 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.505570 [3] L8 tensor(2350.1367, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.508028 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.526317 [3] L1 tensor(91394.7656, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.532159 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.535621 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.538018 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.540564 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.543019 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.545489 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.548004 [3] L8 tensor(2357.3701, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.550642 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.569930 [3] L1 tensor(91394.0781, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.574726 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.578693 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.580944 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.583440 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.585779 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.588109 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.590430 [3] L8 tensor(2365.2251, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.592907 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.611139 [3] L1 tensor(91393.4453, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.616424 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.620085 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.623029 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.625227 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.627443 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.629688 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.631983 [3] L8 tensor(2376.1802, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.634592 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.658550 [3] L1 tensor(91392.8906, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.664109 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.667817 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.670209 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.672553 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.674888 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.677696 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.680736 [3] L8 tensor(2386.9541, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.684005 [3] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.702429 [3] L1 tensor(91392.0312, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.707086 [3] L2 tensor(8343.7402, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.711546 [3] L3 tensor(8466.9336, device='cuda:3', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.714132 [3] L4 tensor(8180.1406, device='cuda:3', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.716327 [3] L5 tensor(8129.6323, device='cuda:3', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.718555 [3] L6 tensor(8153.8779, device='cuda:3', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.720884 [3] L7 tensor(7307.2773, device='cuda:3', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.723241 [3] L8 tensor(2398.0083, device='cuda:3', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:3', grad_fn=<SumBackward0>)
20:34:06.725745 [3] Warning: no training nodes in this partition! Backward fake loss.
21:06:44.178764 [3] proc begin: <DistEnv 3/4 nccl>
21:06:56.319565 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
21:06:56.338974 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:07:47.877897 [3] proc begin: <DistEnv 3/4 nccl>
21:07:54.052416 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
21:07:54.073724 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:09:08.046103 [3] proc begin: <DistEnv 3/4 nccl>
21:09:12.804985 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
21:09:12.828095 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:12:17.094586 [3] proc begin: <DistEnv 3/4 nccl>
21:12:21.770814 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
21:12:21.794871 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:13:53.434264 [3] proc begin: <DistEnv 3/4 nccl>
21:13:58.588331 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
21:13:58.609041 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:14:50.878779 [3] proc begin: <DistEnv 3/4 nccl>
21:14:56.198560 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
21:14:56.230249 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:19:51.074610 [3] proc begin: <DistEnv 3/4 nccl>
21:19:55.168070 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
21:19:55.195733 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:20:43.758197 [3] proc begin: <DistEnv 3/4 nccl>
09:20:49.494984 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:20:49.511831 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:22:21.011044 [3] proc begin: <DistEnv 3/4 nccl>
09:22:43.021517 [3] proc begin: <DistEnv 3/4 nccl>
09:22:47.564155 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:22:47.595949 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:25:10.681273 [3] proc begin: <DistEnv 3/4 nccl>
09:25:16.422528 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:25:16.457009 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:29:00.121622 [3] proc begin: <DistEnv 3/4 nccl>
09:29:06.549015 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
09:29:06.574560 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:30:21.098883 [3] proc begin: <DistEnv 3/4 nccl>
09:30:42.095522 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
09:30:42.115931 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:32:08.896722 [3] proc begin: <DistEnv 3/4 nccl>
09:32:13.806680 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
09:32:13.829925 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:32:20.412649 [3] Warning: no training nodes in this partition! Backward fake loss.
09:33:37.557861 [3] proc begin: <DistEnv 3/4 nccl>
09:33:41.629848 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
09:33:41.650639 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:33:46.629991 [3] Warning: no training nodes in this partition! Backward fake loss.
09:35:50.954580 [3] proc begin: <DistEnv 3/4 nccl>
09:35:55.319157 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
09:35:55.338632 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:36:00.380363 [3] Warning: no training nodes in this partition! Backward fake loss.
10:10:18.064762 [3] proc begin: <DistEnv 3/4 nccl>
10:10:22.561131 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
10:10:22.592263 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:10:27.761399 [3] Warning: no training nodes in this partition! Backward fake loss.
10:12:45.504142 [3] proc begin: <DistEnv 3/4 nccl>
10:12:51.346315 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
10:12:51.370231 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:15:17.354620 [3] proc begin: <DistEnv 3/4 nccl>
10:15:20.968814 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
10:15:20.986035 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:16:49.943560 [3] proc begin: <DistEnv 3/4 nccl>
10:16:54.465746 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
10:16:54.485820 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:17:39.986217 [3] proc begin: <DistEnv 3/4 nccl>
10:17:44.564143 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
10:17:44.586241 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:19:07.281446 [3] proc begin: <DistEnv 3/4 nccl>
10:19:13.065363 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
10:19:13.087716 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:20:05.052345 [3] proc begin: <DistEnv 3/4 nccl>
10:20:09.386604 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
10:20:09.406840 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:20:34.627350 [3] proc begin: <DistEnv 3/4 nccl>
10:20:39.323101 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
10:20:39.373600 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:20:44.577467 [3] Warning: no training nodes in this partition! Backward fake loss.
16:21:09.442565 [3] proc begin: <DistEnv 3/4 nccl>
16:21:25.813667 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:21:25.833777 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:28:45.033554 [3] proc begin: <DistEnv 3/4 nccl>
16:28:51.364096 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:28:51.387239 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:29:18.844330 [3] proc begin: <DistEnv 3/4 nccl>
16:29:25.010870 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:29:25.032445 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:30:57.482673 [3] proc begin: <DistEnv 3/4 nccl>
16:31:03.820346 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:31:03.841133 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:32:02.522872 [3] proc begin: <DistEnv 3/4 nccl>
16:32:08.979430 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:32:09.006934 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:32:48.100201 [3] proc begin: <DistEnv 3/4 nccl>
16:32:53.305266 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:32:53.334296 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:35:16.292075 [3] proc begin: <DistEnv 3/4 nccl>
16:35:22.096777 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:35:22.145010 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:38:07.601768 [3] proc begin: <DistEnv 3/4 nccl>
16:38:12.518086 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:38:12.548578 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:56:16.410000 [3] proc begin: <DistEnv 3/4 nccl>
16:56:21.781628 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:56:21.804500 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:03:05.481016 [3] proc begin: <DistEnv 3/4 nccl>
17:03:11.714169 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
17:03:11.731269 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:03:31.316251 [3] proc begin: <DistEnv 3/4 nccl>
17:03:36.888603 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
17:03:36.922931 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:06:08.680205 [3] proc begin: <DistEnv 3/4 nccl>
17:06:14.208965 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
17:06:14.236700 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

18:34:53.619184 [3] proc begin: <DistEnv 3/4 nccl>
18:41:35.560347 [3] proc begin: <DistEnv 3/4 nccl>
18:44:17.497412 [3] proc begin: <DistEnv 3/4 nccl>
18:46:19.186408 [3] proc begin: <DistEnv 3/4 nccl>
18:46:30.748394 [3] proc begin: <DistEnv 3/4 nccl>
18:49:17.547285 [3] proc begin: <DistEnv 3/4 nccl>
18:55:48.620611 [3] proc begin: <DistEnv 3/4 nccl>
19:38:28.122731 [3] proc begin: <DistEnv 3/4 nccl>
19:38:47.343329 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
19:38:47.363054 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:16:18.633361 [3] proc begin: <DistEnv 3/4 nccl>
20:16:23.069547 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:16:23.097740 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:27:35.465360 [3] proc begin: <DistEnv 3/4 nccl>
20:27:50.956838 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
20:27:50.974973 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:27:59.760052 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:02.629314 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:04.474833 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:06.305205 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:08.133735 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:09.962654 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:11.792817 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:13.616996 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:15.438853 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:17.262074 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:19.086264 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:20.908001 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:22.732586 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:24.553360 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:26.376595 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:28.200816 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:30.026499 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:31.850521 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:33.674889 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:35.499658 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:37.328911 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:39.157633 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:40.979800 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:42.802127 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:44.626826 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:46.450596 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:48.273259 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:50.095460 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:51.917053 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:53.740959 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:55.563424 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:57.386568 [3] Warning: no training nodes in this partition! Backward fake loss.
20:28:59.210921 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:01.033671 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:02.919461 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:04.742283 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:06.564345 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:08.385456 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:10.207902 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:12.029868 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:13.852404 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:15.673598 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:17.496000 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:19.319005 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:21.140048 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:22.962351 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:24.783476 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:26.605516 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:28.427568 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:30.251425 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:32.074402 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:33.712524 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:35.536593 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:37.174287 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:38.997063 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:40.635615 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:42.458390 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:44.096232 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:45.919793 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:47.558122 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:49.380671 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:51.018826 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:52.841616 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:54.480341 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:56.302673 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:57.941325 [3] Warning: no training nodes in this partition! Backward fake loss.
20:29:59.764848 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:01.400641 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:03.285530 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:04.926143 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:06.752735 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:08.395405 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:10.220042 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:11.859300 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:13.681558 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:15.320886 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:17.144062 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:18.782773 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:20.605582 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:22.243918 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:24.068174 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:25.706092 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:27.530002 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:29.169497 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:30.992108 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:32.630772 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:34.454956 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:36.093744 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:37.918061 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:39.556762 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:41.380073 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:43.018745 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:44.841009 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:46.478855 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:48.305021 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:49.943379 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:51.765394 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:53.403511 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:55.227098 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:56.867465 [3] Warning: no training nodes in this partition! Backward fake loss.
20:30:58.691461 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:00.330582 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:02.187943 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:03.855436 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:05.678220 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:07.318416 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:09.142782 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:10.781920 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:12.605678 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:14.245010 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:16.070151 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:17.710622 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:19.533585 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:21.171404 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:22.994314 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:24.632071 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:26.455097 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:28.093386 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:29.917413 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:31.555630 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:33.378165 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:35.015193 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:36.837409 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:38.475266 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:40.297247 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:41.935813 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:43.757911 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:45.401026 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:47.227895 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:48.865763 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:50.690534 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:52.329020 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:54.152354 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:55.790135 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.613260 [3] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.253841 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:01.076491 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:02.743708 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:04.592108 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:06.231647 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:08.057038 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:09.696751 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:11.520884 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:13.159233 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:14.983172 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:16.620773 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:18.442558 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:20.079885 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:21.903535 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:23.541040 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:25.363407 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:27.000365 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:28.822991 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:30.461982 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:32.285492 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:33.923242 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:35.745880 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:37.384086 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:39.206997 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:40.844215 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:42.667101 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:44.304223 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:46.127417 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:47.764243 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:49.587131 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:51.224932 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:53.048370 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:54.686363 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:56.507784 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:58.146103 [3] Warning: no training nodes in this partition! Backward fake loss.
20:32:59.969596 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:01.613811 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:03.491142 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:05.130390 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:06.954543 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:08.595109 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:10.420009 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:12.057876 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.881204 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.518257 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.341264 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.979577 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:20.803160 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:22.440741 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:24.264205 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:25.904651 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:27.728724 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:29.368044 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:31.191063 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:32.830758 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:34.654001 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:36.292889 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:38.117958 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:39.757613 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:41.581415 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:43.220220 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:45.044373 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:46.681872 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:48.505924 [3] Warning: no training nodes in this partition! Backward fake loss.
20:33:50.143953 [3] Warning: no training nodes in this partition! Backward fake loss.
20:39:45.347418 [3] proc begin: <DistEnv 3/4 nccl>
20:39:51.155168 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:39:51.175974 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:46:10.184741 [3] proc begin: <DistEnv 3/4 nccl>
20:46:16.342825 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:46:16.366144 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:47:48.589819 [3] proc begin: <DistEnv 3/4 nccl>
20:47:54.493381 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:47:54.513689 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:27:35.156284 [3] proc begin: <DistEnv 3/4 nccl>
21:27:38.862181 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
21:27:38.907811 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:28:45.750934 [3] proc begin: <DistEnv 3/4 nccl>
21:28:51.192935 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
21:28:51.214582 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:30:37.786946 [3] proc begin: <DistEnv 3/4 nccl>
21:30:43.461819 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
21:30:43.482682 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:32:23.898902 [3] proc begin: <DistEnv 3/4 nccl>
21:32:43.527131 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
21:32:43.547415 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:49:23.027794 [3] proc begin: <DistEnv 3/4 nccl>
21:49:28.636678 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
21:49:28.657262 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:00:42.298719 [3] proc begin: <DistEnv 3/4 nccl>
22:00:59.186920 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
22:00:59.207978 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:01:08.133225 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:10.962004 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:12.802491 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:14.634217 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:16.466611 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:18.300057 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:20.135027 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:21.967006 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:23.800772 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:25.631685 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:27.459903 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:29.285238 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:31.107761 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:32.932342 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:34.757128 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:36.581873 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:38.409974 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:40.236163 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:42.063035 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:43.891770 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:45.718145 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:47.544903 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:49.369167 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:51.195899 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:53.025537 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:54.852860 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:56.679910 [3] Warning: no training nodes in this partition! Backward fake loss.
22:01:58.509624 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:00.338427 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:02.177428 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:04.058987 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:05.891857 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:07.726978 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:09.561060 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:11.395827 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:13.228489 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:15.057444 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:16.884316 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:18.713012 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:20.548692 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:22.379321 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:24.204596 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:26.030196 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:27.855601 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:29.682610 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:31.508628 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:33.331396 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:35.158950 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:36.986297 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:38.815828 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:40.644697 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:42.287986 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:44.113926 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:45.755735 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:47.583138 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:49.223472 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:51.050776 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:52.688931 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:54.511966 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:56.150019 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:57.973363 [3] Warning: no training nodes in this partition! Backward fake loss.
22:02:59.614383 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:01.439892 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:03.139979 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:04.967747 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:06.611288 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:08.441599 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:10.085641 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:11.913964 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:13.557576 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:15.385172 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:17.027427 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:18.854534 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:20.495872 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:22.322962 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:23.963596 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:25.789887 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:27.428229 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:29.251747 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:30.893659 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:32.720170 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:34.361270 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:36.186413 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:37.827553 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:39.650219 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:41.288599 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:43.111400 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:44.749130 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:46.572823 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:48.210627 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:50.034304 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:51.672820 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:53.496331 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:55.135223 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:56.961867 [3] Warning: no training nodes in this partition! Backward fake loss.
22:03:58.603413 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:00.429274 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:02.103734 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:03.952320 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:05.595190 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:07.425369 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:09.068469 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:10.900762 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:12.541356 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:14.367105 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:16.006877 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:17.835716 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:19.476510 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:21.304091 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:22.945600 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:24.774186 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:26.418208 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:28.246722 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:29.888903 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:31.719556 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:33.363970 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:35.192750 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:36.832474 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:38.657274 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:40.294296 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:42.118954 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:43.762905 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:45.588708 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:47.228834 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:49.057657 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:50.697042 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:52.522709 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:54.162011 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:55.987821 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:57.631604 [3] Warning: no training nodes in this partition! Backward fake loss.
22:04:59.460310 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:01.102816 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:02.980556 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:04.631639 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:06.458302 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:08.100966 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:09.926068 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:11.566425 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:13.393593 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:15.034111 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:16.860469 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:18.499149 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:20.323853 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:21.964435 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:23.789118 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:25.427771 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:27.251562 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:28.889632 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:30.713045 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:32.355189 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:34.181764 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:35.825748 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:37.652233 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:39.294305 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:41.120942 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:42.762678 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:44.589425 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:46.229765 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:48.056746 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:49.700008 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:51.525795 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:53.167904 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:54.994420 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:56.636302 [3] Warning: no training nodes in this partition! Backward fake loss.
22:05:58.466568 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:00.110119 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:01.967383 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:03.642013 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:05.471750 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:07.118686 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:08.948441 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:10.593694 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:12.424342 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:14.068425 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:15.892875 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:17.533349 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:19.359263 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:20.997942 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:22.822845 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:24.465046 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:26.292419 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:27.931864 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:29.756556 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:31.395554 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:33.221019 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:34.861934 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:36.688332 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:38.330055 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:40.156719 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:41.797910 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:43.626009 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:45.267433 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:47.095405 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:48.736864 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:50.563760 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:52.204895 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:54.032046 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:55.672093 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:57.499080 [3] Warning: no training nodes in this partition! Backward fake loss.
22:06:59.139727 [3] Warning: no training nodes in this partition! Backward fake loss.
22:28:58.096626 [3] proc begin: <DistEnv 3/4 nccl>
22:29:04.327327 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
22:29:04.349157 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:08:23.006001 [3] proc begin: <DistEnv 3/4 nccl>
14:08:27.501626 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:08:27.519077 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:08:32.470853 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:34.074712 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:34.598971 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:35.121583 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:35.645768 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:36.168238 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:36.690036 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:37.212296 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:37.736147 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:38.260896 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:38.783325 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:39.306847 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:39.830592 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:40.354512 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:40.876644 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:41.399366 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:41.921192 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:42.444244 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:42.968822 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:43.491615 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:44.013590 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:44.535688 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:45.058299 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:45.579695 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:46.101457 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:46.623718 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:47.145289 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:47.667291 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:48.190606 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:48.712981 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:49.236989 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:49.760684 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:50.285156 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:50.812093 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:51.337465 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:51.863936 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:52.389712 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:52.914839 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:53.440361 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:53.966404 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:54.492240 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:55.018362 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:55.543805 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:56.068627 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:56.594136 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:57.120768 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:57.646368 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:58.172584 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:58.698782 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:59.223320 [3] Warning: no training nodes in this partition! Backward fake loss.
14:08:59.748627 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:00.275781 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:00.801509 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:01.331068 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:01.876252 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:02.421611 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:02.954829 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:03.479884 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:04.005633 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:04.531542 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:05.058206 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:05.583861 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:06.109142 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:06.633447 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:07.158693 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:07.683253 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:08.207388 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:08.734110 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:09.262008 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:09.788437 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:10.315844 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:10.843339 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:11.370443 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:11.896326 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:12.423040 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:12.947841 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:13.472293 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:13.996703 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:14.522548 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:15.047508 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:15.573780 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:16.098730 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:16.623021 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:17.147260 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:17.670191 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:18.194536 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:18.718386 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:19.243209 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:19.766714 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:20.290492 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:20.812458 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:21.339711 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:21.867020 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:22.393133 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:22.920597 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:23.447326 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:23.971919 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:24.498540 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:25.025353 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:25.550258 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:26.075880 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:26.600271 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:27.126476 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:27.650295 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:28.174966 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:28.699144 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:29.225769 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:29.750628 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:30.276862 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:30.803326 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:31.330012 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:31.856766 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:32.380927 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:32.906227 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:33.432430 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:33.956327 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:34.481720 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:35.006682 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:35.533858 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:36.061683 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:36.587649 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:37.114500 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:37.641046 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:38.166428 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:38.693462 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:39.219535 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:39.745452 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:40.271507 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:40.797813 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:41.323825 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:41.849281 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:42.375473 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:42.903065 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:43.430645 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:43.956293 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:44.482825 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:45.009388 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:45.536817 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:46.063816 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:46.592971 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:47.120485 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:47.646998 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:48.171154 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:48.695546 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:49.219339 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:49.742957 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:50.267357 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:50.792825 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:51.316839 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:51.842653 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:52.368963 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:52.895481 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:53.420821 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:53.948044 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:54.476326 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:55.003087 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:55.529802 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:56.055321 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:56.580429 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:57.106747 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:57.632629 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:58.158951 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:58.684588 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:59.210873 [3] Warning: no training nodes in this partition! Backward fake loss.
14:09:59.735482 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:00.261514 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:00.787823 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:01.314001 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:01.842052 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:02.388732 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:02.935004 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:03.471429 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:03.997021 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:04.522887 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:05.047563 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:05.572548 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:06.098465 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:06.624947 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:07.150885 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:07.676269 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:08.201822 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:08.727778 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:09.252639 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:09.776790 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:10.301545 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:10.827322 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:11.352807 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:11.875897 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:12.402087 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:12.928493 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:13.453022 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:13.976906 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:14.502753 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:15.028358 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:15.554942 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:16.082238 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:16.610069 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:17.137629 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:17.665719 [3] Warning: no training nodes in this partition! Backward fake loss.
14:10:18.191915 [3] Warning: no training nodes in this partition! Backward fake loss.
14:39:41.347957 [3] proc begin: <DistEnv 3/4 nccl>
14:39:45.521429 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
14:39:45.542441 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:39:51.864431 [3] Warning: no training nodes in this partition! Backward fake loss.
14:39:53.928029 [3] Warning: no training nodes in this partition! Backward fake loss.
14:39:54.936204 [3] Warning: no training nodes in this partition! Backward fake loss.
14:39:55.945928 [3] Warning: no training nodes in this partition! Backward fake loss.
14:39:56.954649 [3] Warning: no training nodes in this partition! Backward fake loss.
14:39:57.959596 [3] Warning: no training nodes in this partition! Backward fake loss.
14:39:58.970161 [3] Warning: no training nodes in this partition! Backward fake loss.
14:39:59.974605 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:00.979467 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:02.008217 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:03.040306 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:04.051629 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:05.058349 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:06.065693 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:07.071874 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:08.076941 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:09.083168 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:10.089868 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:11.095604 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:12.104016 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:13.109641 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:14.113978 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:15.116277 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:16.120871 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:17.127792 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:18.129996 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:19.133161 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:20.133770 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:21.136699 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:22.138957 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:23.139590 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:24.140090 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:25.140704 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:26.142537 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:27.144768 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:28.146962 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:29.152075 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:30.156091 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:31.157836 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:32.160724 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:33.163277 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:34.166450 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:35.168873 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:36.172840 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:37.179706 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:38.185072 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:39.187583 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:40.191307 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:41.192299 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:42.194080 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:43.194709 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:44.196502 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:45.198150 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:46.199683 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:47.201442 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:48.201518 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:49.202215 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:50.204274 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:51.206214 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:52.205817 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:53.205485 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:54.203331 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:55.202375 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:56.201228 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:57.203370 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:58.201540 [3] Warning: no training nodes in this partition! Backward fake loss.
14:40:59.201510 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:00.198960 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:01.196959 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:02.208128 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:03.248546 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:04.260068 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:05.263143 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:06.264522 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:07.264170 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:08.262985 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:09.263613 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:10.263909 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:11.265901 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:12.269091 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:13.267519 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:14.266414 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:15.264596 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:16.264466 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:17.265076 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:18.265722 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:19.268071 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:20.266375 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:21.266611 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:22.266732 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:23.266221 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:24.265517 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:25.265662 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:26.265896 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:27.271799 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:28.280425 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:29.288131 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:30.290046 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:31.289436 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:32.288878 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:33.288706 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:34.288697 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:35.289334 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:36.287581 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:37.288162 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:38.288792 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:39.295530 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:40.298327 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:41.302042 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:42.305870 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:43.309536 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:44.313246 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:45.315463 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:46.317350 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:47.320672 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:48.324294 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:49.326180 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:50.327487 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:51.328828 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:52.330863 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:53.335608 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:54.338600 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:55.340654 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:56.344226 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:57.346657 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:58.345977 [3] Warning: no training nodes in this partition! Backward fake loss.
14:41:59.346773 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:00.346950 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:01.346990 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:02.381710 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:03.407170 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:04.417329 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:05.420703 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:06.423874 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:07.427441 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:08.434258 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:09.439323 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:10.445371 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:11.452605 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:12.465135 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:13.473412 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:14.477030 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:15.479873 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:16.479766 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:17.477584 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:18.476848 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:19.475605 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:20.475948 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:21.473349 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:22.472585 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:23.475041 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:24.476838 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:25.477781 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:26.479548 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:27.478769 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:28.477282 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:29.479071 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:30.476404 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:31.474262 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:32.473634 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:33.475553 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:34.477174 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:35.476495 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:36.476752 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:37.476235 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:38.484178 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:39.487944 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:40.490162 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:41.492730 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:42.493749 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:43.494035 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:44.497523 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:45.500093 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:46.503714 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:47.505443 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:48.506118 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:49.508286 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:50.508797 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:51.513429 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:52.516468 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:53.518360 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:54.519116 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:55.519630 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:56.517648 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:57.516184 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:58.515440 [3] Warning: no training nodes in this partition! Backward fake loss.
14:42:59.516591 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:00.515353 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:01.529767 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:02.570569 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:03.581571 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:04.586533 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:05.587885 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:06.590377 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:07.591657 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:08.592625 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:09.595016 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:10.598199 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:11.601437 [3] Warning: no training nodes in this partition! Backward fake loss.
14:43:12.601600 [3] Warning: no training nodes in this partition! Backward fake loss.
15:29:58.812615 [3] proc begin: <DistEnv 3/4 nccl>
15:30:18.118121 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
15:30:18.137338 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:30:55.416402 [3] proc begin: <DistEnv 3/4 nccl>
15:31:00.087883 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
15:31:00.114986 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:32:43.762024 [3] proc begin: <DistEnv 3/4 nccl>
15:32:49.115950 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
15:32:49.131960 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

18:25:25.124397 [3] proc begin: <DistEnv 3/4 nccl>
18:25:31.311749 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
18:25:31.335690 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

18:30:41.067893 [3] proc begin: <DistEnv 3/4 nccl>
18:30:46.572519 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
18:30:46.595003 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:00:40.873880 [3] proc begin: <DistEnv 3/4 nccl>
20:00:46.622148 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:00:46.642089 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:03:41.474344 [3] proc begin: <DistEnv 3/4 nccl>
20:03:46.610649 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:03:46.638957 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:05:10.025875 [3] proc begin: <DistEnv 3/4 nccl>
20:05:15.670551 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:05:15.694553 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:06:19.567054 [3] proc begin: <DistEnv 3/4 nccl>
20:06:24.022979 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
20:06:24.039158 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:34:01.705430 [3] proc begin: <DistEnv 3/4 nccl>
11:34:17.019341 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
11:34:17.039045 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:34:17.956745 [3] L1 tensor(77137.2891, device='cuda:3', grad_fn=<SumBackward0>) tensor(260.2319, device='cuda:3', grad_fn=<SumBackward0>)
15:51:57.283872 [3] proc begin: <DistEnv 3/4 nccl>
15:52:07.197093 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
15:52:07.238191 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:52:23.525584 [3] proc begin: <DistEnv 3/4 nccl>
15:52:28.787282 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
15:52:28.808821 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:02:35.329248 [3] proc begin: <DistEnv 3/4 nccl>
16:02:39.966872 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:02:40.012954 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:21:50.811573 [3] proc begin: <DistEnv 3/4 nccl>
16:21:56.558708 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:21:56.580357 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

18:30:03.605505 [3] proc begin: <DistEnv 3/4 nccl>
18:30:09.030366 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
18:30:09.050772 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

18:51:38.436944 [3] proc begin: <DistEnv 3/4 nccl>
18:51:42.930675 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
18:51:42.951576 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:31:22.339377 [3] proc begin: <DistEnv 3/4 nccl>
14:31:30.082837 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
14:31:30.099629 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:31:30.779963 [3] L1 tensor(77137.2891, device='cuda:3', grad_fn=<SumBackward0>) tensor(260.2319, device='cuda:3', grad_fn=<SumBackward0>)
14:31:59.852580 [3] proc begin: <DistEnv 3/4 nccl>
14:32:06.018008 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
14:32:06.038613 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:32:07.136356 [3] L1 tensor(38548.0586, device='cuda:3', grad_fn=<SumBackward0>) tensor(120.5969, device='cuda:3', grad_fn=<SumBackward0>)
16:15:52.536472 [3] proc begin: <DistEnv 3/4 nccl>
16:15:58.200659 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:15:58.225581 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:44:48.330211 [3] proc begin: <DistEnv 3/4 nccl>
16:44:54.106767 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:44:54.130568 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:16:27.780542 [3] proc begin: <DistEnv 3/4 nccl>
21:16:33.872455 [3] proc begin: <DistEnv 3/4 nccl>
21:16:39.513554 [3] proc begin: <DistEnv 3/4 nccl>
21:16:45.445440 [3] proc begin: <DistEnv 3/4 nccl>
21:16:50.903894 [3] proc begin: <DistEnv 3/4 nccl>
21:17:00.903519 [3] proc begin: <DistEnv 3/4 nccl>
21:17:06.544391 [3] proc begin: <DistEnv 3/4 nccl>
21:17:11.580608 [3] proc begin: <DistEnv 3/4 nccl>
21:20:15.231196 [3] proc begin: <DistEnv 3/4 nccl>
21:21:38.741095 [3] proc begin: <DistEnv 3/4 nccl>
21:23:12.546752 [3] proc begin: <DistEnv 3/4 nccl>
21:23:36.198193 [3] graph loaded <COO Graph: e80M_f512_l32_t0.5, |V|: 2000000, |E|: 80000000, masks: 1000000,200000,800000><Local: 3, |V|: 500000, |E|: 1468587>
21:23:36.221063 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1017 MiB |   1033 MiB |   1067 MiB |  51113 KiB |
|       from large pool |   1017 MiB |   1033 MiB |   1067 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1017 MiB |   1033 MiB |   1067 MiB |  51113 KiB |
|       from large pool |   1017 MiB |   1033 MiB |   1067 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1016 MiB |   1031 MiB |   1064 MiB |  48832 KiB |
|       from large pool |   1016 MiB |   1031 MiB |   1064 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1056 MiB |   1056 MiB |   1056 MiB |      0 B   |
|       from large pool |   1054 MiB |   1054 MiB |   1054 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  21190 KiB |  23238 KiB |  61370 KiB |  40179 KiB |
|       from large pool |  21190 KiB |  21190 KiB |  55221 KiB |  34031 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       3    |       3    |       5    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:23:46.481280 [3] Warning: no training nodes in this partition! Backward fake loss.
21:23:48.233536 [3] Warning: no training nodes in this partition! Backward fake loss.
21:23:48.957148 [3] Warning: no training nodes in this partition! Backward fake loss.
21:23:49.680280 [3] Warning: no training nodes in this partition! Backward fake loss.
21:23:50.400661 [3] Warning: no training nodes in this partition! Backward fake loss.
21:23:51.121302 [3] Warning: no training nodes in this partition! Backward fake loss.
21:23:51.840489 [3] Warning: no training nodes in this partition! Backward fake loss.
21:23:52.560874 [3] Warning: no training nodes in this partition! Backward fake loss.
21:23:53.280592 [3] Warning: no training nodes in this partition! Backward fake loss.
21:23:54.000254 [3] Warning: no training nodes in this partition! Backward fake loss.
21:23:54.722355 [3] Warning: no training nodes in this partition! Backward fake loss.
21:23:55.441000 [3] Warning: no training nodes in this partition! Backward fake loss.
21:23:56.159713 [3] Warning: no training nodes in this partition! Backward fake loss.
21:23:56.879249 [3] Warning: no training nodes in this partition! Backward fake loss.
21:23:57.598228 [3] Warning: no training nodes in this partition! Backward fake loss.
21:23:58.319520 [3] Warning: no training nodes in this partition! Backward fake loss.
21:23:59.038643 [3] Warning: no training nodes in this partition! Backward fake loss.
21:23:59.757421 [3] Warning: no training nodes in this partition! Backward fake loss.
21:24:00.477803 [3] Warning: no training nodes in this partition! Backward fake loss.
21:24:01.197902 [3] Warning: no training nodes in this partition! Backward fake loss.
21:24:01.934684 [3] Warning: no training nodes in this partition! Backward fake loss.
21:24:02.675236 [3] Warning: no training nodes in this partition! Backward fake loss.
21:24:03.397496 [3] Warning: no training nodes in this partition! Backward fake loss.
21:24:04.119157 [3] Warning: no training nodes in this partition! Backward fake loss.
21:24:04.840763 [3] Warning: no training nodes in this partition! Backward fake loss.
21:24:05.561872 [3] Warning: no training nodes in this partition! Backward fake loss.
21:24:06.283701 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:14.571314 [3] proc begin: <DistEnv 3/4 nccl>
21:25:15.880800 [3] graph loaded <COO Graph: e80M_f512_l32_t0.5, |V|: 2000000, |E|: 80000000, masks: 1000000,200000,800000><Local: 3, |V|: 500000, |E|: 1468587>
21:25:15.889074 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1017 MiB |   1033 MiB |   1067 MiB |  51113 KiB |
|       from large pool |   1017 MiB |   1033 MiB |   1067 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1017 MiB |   1033 MiB |   1067 MiB |  51113 KiB |
|       from large pool |   1017 MiB |   1033 MiB |   1067 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1016 MiB |   1031 MiB |   1064 MiB |  48832 KiB |
|       from large pool |   1016 MiB |   1031 MiB |   1064 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1056 MiB |   1056 MiB |   1056 MiB |      0 B   |
|       from large pool |   1054 MiB |   1054 MiB |   1054 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  21190 KiB |  23238 KiB |  61370 KiB |  40179 KiB |
|       from large pool |  21190 KiB |  21190 KiB |  55221 KiB |  34031 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       3    |       3    |       5    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:25:28.068594 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:29.861307 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:30.586302 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:31.313611 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:32.041629 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:32.768276 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:33.495626 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:34.222703 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:34.945756 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:35.670221 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:36.392892 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:37.114249 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:37.836383 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:38.560075 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:39.283699 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:40.005716 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:40.727875 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:41.449252 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:42.170430 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:42.892540 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:43.616545 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:44.338144 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:45.059421 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:45.782422 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:46.503561 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:47.224887 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:47.946748 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:48.668864 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:49.390002 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:50.112292 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:50.834353 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:51.556554 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:52.279818 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:53.003002 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:53.727284 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:54.451063 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:55.175463 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:55.901224 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:56.625109 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:57.346809 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:58.069210 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:58.791624 [3] Warning: no training nodes in this partition! Backward fake loss.
21:25:59.513215 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:00.233946 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:00.955284 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:01.683765 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:02.426298 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:03.163064 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:03.888630 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:04.614627 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:05.339301 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:05.691700 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:06.639401 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:06.992573 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:07.941906 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:08.294427 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:09.245274 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:09.596968 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:10.546248 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:10.898499 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:11.847973 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:12.198265 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:13.144885 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:13.495034 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:14.442424 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:14.793001 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:15.738878 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:16.088805 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:17.034034 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:17.384318 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:18.327778 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:18.679167 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:19.625749 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:19.975853 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:20.923338 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:21.273078 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:22.218789 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:22.568332 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:23.515813 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:23.865271 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:24.812207 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:25.161889 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:26.107649 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:26.458617 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:27.404628 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:27.755525 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:28.701722 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:29.053013 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:29.998828 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:30.349894 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:31.294798 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:31.646121 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:32.590937 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:32.943396 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:33.887343 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:34.239623 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:35.184364 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:35.535577 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:36.480907 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:36.831404 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:37.776250 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:38.127962 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:39.072579 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:39.423666 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:40.368419 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:40.720247 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:41.664793 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:42.015679 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:42.960408 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:43.311438 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:44.255917 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:44.607576 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:45.551841 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:45.902868 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:46.847609 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:47.198511 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:48.142236 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:48.493354 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:49.437885 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:49.788435 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:50.734318 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:51.084877 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:52.029542 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:52.381701 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:53.325644 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:53.676127 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:54.619553 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:54.968792 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:55.912811 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:56.262296 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:57.203939 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:57.554120 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:58.498492 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:58.847992 [3] Warning: no training nodes in this partition! Backward fake loss.
21:26:59.794106 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:00.143970 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:01.090080 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:01.439877 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:02.394778 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:02.760201 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:03.717564 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:04.069692 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:05.015849 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:05.367925 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:06.315016 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:06.665541 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:07.613368 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:07.964084 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:08.911168 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:09.262464 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:10.210610 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:10.561877 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:11.510486 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:11.861864 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:12.809216 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:13.160679 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:14.107423 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:14.457461 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:15.403802 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:15.753202 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:16.698722 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:17.048685 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:17.995193 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:18.344935 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:19.291438 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:19.642034 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:20.583761 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:20.933498 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:21.876442 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:22.226158 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:23.169789 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:23.520051 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:24.462755 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:24.812543 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:25.755110 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:26.104632 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:27.047619 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:27.396601 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:28.340887 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:28.690504 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:29.633619 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:29.983298 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:30.927030 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:31.276188 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:32.219506 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:32.569858 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:33.516032 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:33.865866 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:34.813675 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:35.163841 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:36.109605 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:36.459945 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:37.405933 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:37.755653 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:38.701414 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:39.051413 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:39.996569 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:40.346565 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:41.293569 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:41.643043 [3] Warning: no training nodes in this partition! Backward fake loss.
21:27:49.469869 [3] proc begin: <DistEnv 3/4 nccl>
21:28:22.014310 [3] graph loaded <COO Graph: e320M_f512_l32_t0.5, |V|: 2000000, |E|: 320000000, masks: 1000000,200000,800000><Local: 3, |V|: 500000, |E|: 4346637>
21:28:22.020238 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1039 MiB |   1055 MiB |   1089 MiB |  51113 KiB |
|       from large pool |   1039 MiB |   1055 MiB |   1089 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1039 MiB |   1055 MiB |   1089 MiB |  51113 KiB |
|       from large pool |   1039 MiB |   1055 MiB |   1089 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1038 MiB |   1053 MiB |   1086 MiB |  48832 KiB |
|       from large pool |   1038 MiB |   1053 MiB |   1086 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1072 MiB |   1072 MiB |   1072 MiB |      0 B   |
|       from large pool |   1070 MiB |   1070 MiB |   1070 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15089 KiB |  19998 KiB |  49532 KiB |  34442 KiB |
|       from large pool |  15089 KiB |  19998 KiB |  43383 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       7    |       7    |       7    |       0    |
|       from large pool |       6    |       6    |       6    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       5    |       9    |       5    |
|       from large pool |       4    |       4    |       6    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:29:02.195030 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:04.980865 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:06.596239 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:08.204779 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:09.811279 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:11.417906 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:13.022703 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:14.624572 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:16.224904 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:17.826083 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:19.425942 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:21.024809 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:22.623306 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:24.222942 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:25.823793 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:27.422001 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:29.019896 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:30.620105 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:32.222225 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:33.823696 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:35.424855 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:37.025827 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:38.627821 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:40.227635 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:41.826781 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:43.427070 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:45.026408 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:46.626238 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:48.224672 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:49.824166 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:51.423707 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:53.022394 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:54.622156 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:56.221941 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:57.821401 [3] Warning: no training nodes in this partition! Backward fake loss.
21:29:59.420048 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:01.017286 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:02.629620 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:04.238635 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:05.841458 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:07.444290 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:09.047007 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:10.649543 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:12.252967 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:13.857730 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:15.460909 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:17.065726 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:18.669994 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:20.272409 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:21.874702 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:23.478773 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:24.107730 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:26.533846 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:27.160441 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:29.589498 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:30.216440 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:32.641081 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:33.268055 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:35.693321 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:36.320901 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:38.745884 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:39.373416 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:41.799591 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:42.425954 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:44.850885 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:45.479364 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:47.903680 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:48.531371 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:50.957591 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:51.585730 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:54.011577 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:54.640113 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:57.065608 [3] Warning: no training nodes in this partition! Backward fake loss.
21:30:57.692021 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:00.117365 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:00.744238 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:03.185141 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:03.816066 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:06.244461 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:06.872683 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:09.304084 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:09.932099 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:12.361082 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:12.989821 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:15.415028 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:16.043384 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:18.468915 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:19.096999 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:21.525879 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:22.154145 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:24.579110 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:25.205717 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:27.630815 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:28.257591 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:30.683644 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:31.311457 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:33.735744 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:34.362745 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:36.786977 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:37.414431 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:39.840824 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:40.468262 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:42.892375 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:43.518900 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:45.944293 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:46.571316 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:48.997384 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:49.624463 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:52.048712 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:52.675113 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:55.097479 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:55.724269 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:58.149547 [3] Warning: no training nodes in this partition! Backward fake loss.
21:31:58.778728 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:01.200775 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:01.837409 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:04.266917 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:04.894493 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:07.320873 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:07.949790 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:10.374401 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:11.001959 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:13.426509 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:14.053359 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:16.479145 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:17.107328 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:19.532108 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:20.160540 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:22.584321 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:23.212488 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:25.637345 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:26.264391 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:28.689077 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:29.317010 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:31.740984 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:32.367678 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:34.792552 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:35.419801 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:37.843615 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:38.470818 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:40.893944 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:41.521195 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:43.946504 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:44.573552 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:46.996419 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:47.622767 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:50.046796 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:50.673214 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:53.102116 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:53.728874 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:56.152432 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:56.778738 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:59.203177 [3] Warning: no training nodes in this partition! Backward fake loss.
21:32:59.829980 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:02.263167 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:02.905186 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:05.328090 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:05.955283 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:08.380339 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:09.006732 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:11.433279 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:12.062636 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:14.487132 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:15.116459 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:17.541166 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:18.169746 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:20.595692 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:21.223905 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:23.648577 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:24.276886 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:26.702198 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:27.330590 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:29.756674 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:30.385747 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:32.810043 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:33.438128 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:35.862631 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:36.489190 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:38.914628 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:39.542368 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:41.966975 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:42.593437 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:45.016523 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:45.643907 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:48.069506 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:48.696879 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:51.119427 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:51.746385 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:54.171204 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:54.798095 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:57.222799 [3] Warning: no training nodes in this partition! Backward fake loss.
21:33:57.849979 [3] Warning: no training nodes in this partition! Backward fake loss.
21:34:00.275406 [3] Warning: no training nodes in this partition! Backward fake loss.
21:34:00.902766 [3] Warning: no training nodes in this partition! Backward fake loss.
21:34:03.344668 [3] Warning: no training nodes in this partition! Backward fake loss.
21:34:03.971194 [3] Warning: no training nodes in this partition! Backward fake loss.
21:34:06.401386 [3] Warning: no training nodes in this partition! Backward fake loss.
21:34:07.029869 [3] Warning: no training nodes in this partition! Backward fake loss.
21:34:09.458318 [3] Warning: no training nodes in this partition! Backward fake loss.
21:34:10.085058 [3] Warning: no training nodes in this partition! Backward fake loss.
21:34:17.993878 [3] proc begin: <DistEnv 3/4 nccl>
21:34:34.172407 [3] graph loaded <COO Graph: e160M_f256_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 3, |V|: 500000, |E|: 2349774>
21:34:34.186529 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 548605 KiB | 564992 KiB | 599718 KiB |  51113 KiB |
|       from large pool | 548605 KiB | 564989 KiB | 599711 KiB |  51105 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 548605 KiB | 564992 KiB | 599718 KiB |  51113 KiB |
|       from large pool | 548605 KiB | 564989 KiB | 599711 KiB |  51105 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 547654 KiB | 563281 KiB | 596487 KiB |  48832 KiB |
|       from large pool | 547654 KiB | 563279 KiB | 596482 KiB |  48828 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 581632 KiB | 581632 KiB | 581632 KiB |      0 B   |
|       from large pool | 579584 KiB | 579584 KiB | 579584 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  14594 KiB |  20286 KiB |  58216 KiB |  43621 KiB |
|       from large pool |  14594 KiB |  20286 KiB |  52067 KiB |  37473 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       3    |       3    |       5    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:35:05.948598 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:07.847698 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:08.711784 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:09.576371 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:10.437575 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:11.302359 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:12.163358 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:13.023952 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:13.883917 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:14.743974 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:15.604400 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:16.465096 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:17.326377 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:18.186120 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:19.043276 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:19.901899 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:20.760731 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:21.619237 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:22.478261 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:23.337017 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:24.194004 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:25.051494 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:25.909629 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:26.767829 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:27.626745 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:28.484086 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:29.342935 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:30.200215 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:31.057591 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:31.915687 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:32.773698 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:33.631387 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:34.488735 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:35.347010 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:36.204138 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:37.062863 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:37.921120 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:38.779880 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:39.638068 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:40.496654 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:41.354644 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:42.212310 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:43.069693 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:43.929143 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:44.789470 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:45.649994 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:46.510097 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:47.370428 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:48.230341 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:49.089679 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:49.949422 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:50.379557 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:51.520786 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:51.951097 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:53.092013 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:53.522665 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:54.660345 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:55.087843 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:56.225993 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:56.653024 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:57.790567 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:58.218323 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:59.355845 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:59.783028 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:00.920912 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:01.348238 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:02.498853 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:02.938646 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:04.084015 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:04.515243 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:05.658264 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:06.088388 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:07.230698 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:07.660519 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:08.803889 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:09.234018 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:10.376242 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:10.805561 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:11.945945 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:12.374556 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:13.515584 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:13.944302 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:15.085630 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:15.515427 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:16.654306 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:17.082956 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:18.221635 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:18.648921 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:19.788211 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:20.216345 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:21.353171 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:21.779819 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:22.917355 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:23.346118 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:24.484114 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:24.910880 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:26.048313 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:26.475478 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:27.612707 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:28.039569 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:29.176895 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:29.606018 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:30.743683 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:31.171834 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:32.309095 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:32.736438 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:33.873180 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:34.300725 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:35.437622 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:35.865365 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:37.002524 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:37.429246 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:38.568162 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:38.995596 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:40.133053 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:40.561405 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:41.699384 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:42.127884 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:43.266075 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:43.694600 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:44.832079 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:45.260894 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:46.401274 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:46.829856 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:47.967854 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:48.396538 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:49.535184 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:49.962737 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:51.099493 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:51.526810 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:52.665287 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:53.092172 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:54.230433 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:54.658154 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:55.795514 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:56.223134 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:57.360540 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:57.788112 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:58.925644 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:59.353961 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:00.493748 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:00.922527 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:02.073444 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:02.513532 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:03.660070 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:04.090903 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:05.233952 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:05.664785 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:06.813659 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:07.247511 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:08.395117 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:08.826485 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:09.968695 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:10.396650 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:11.539475 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:11.967399 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:13.108788 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:13.536338 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:14.677946 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:15.106013 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:16.246888 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:16.675358 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:17.818888 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:18.246579 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:19.389283 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:19.817008 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:20.955477 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:21.384534 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:22.523251 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:22.951116 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:24.088520 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:24.515031 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:25.652968 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:26.080430 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:27.218050 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:27.643589 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:28.781290 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:29.207665 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:30.345772 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:30.772665 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:31.910676 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:32.337730 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:33.475086 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:33.901841 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:35.039051 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:35.466306 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:36.604220 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:37.031646 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:38.168340 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:38.595375 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:39.732359 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:40.158941 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:41.296310 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:41.723404 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:42.860373 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:43.287666 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:44.424711 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:44.851767 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:45.988396 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:46.415935 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:53.823793 [3] proc begin: <DistEnv 3/4 nccl>
21:38:38.919993 [3] graph loaded <COO Graph: e160M_f1024_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 3, |V|: 500000, |E|: 2349774>
21:38:38.930885 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   2001 MiB |   2017 MiB |   2051 MiB |  51113 KiB |
|       from large pool |   2001 MiB |   2017 MiB |   2051 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   2001 MiB |   2017 MiB |   2051 MiB |  51113 KiB |
|       from large pool |   2001 MiB |   2017 MiB |   2051 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1999 MiB |   2014 MiB |   2047 MiB |  48832 KiB |
|       from large pool |   1999 MiB |   2014 MiB |   2047 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2032 MiB |   2032 MiB |   2032 MiB |      0 B   |
|       from large pool |   2030 MiB |   2030 MiB |   2030 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  12834 KiB |  18526 KiB |  56456 KiB |  43621 KiB |
|       from large pool |  12834 KiB |  18526 KiB |  50307 KiB |  37473 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       3    |       7    |       5    |
|       from large pool |       2    |       2    |       4    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:39:10.990057 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:13.375270 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:14.733789 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:16.092799 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:17.452241 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:18.806688 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:20.161830 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:21.517331 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:22.872676 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:24.228106 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:25.583374 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:26.938283 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:28.292482 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:29.647998 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:31.002545 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:32.358418 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:33.711988 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:35.065593 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:36.418230 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:37.772168 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:39.126088 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:40.480183 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:41.834104 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:43.188093 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:44.541666 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:45.895613 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:47.249009 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:48.604184 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:49.958268 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:51.311719 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:52.665974 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:54.021407 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:55.375259 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:56.727989 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:58.082079 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:59.434881 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:00.788964 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:02.146472 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:03.519227 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:04.876053 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:06.232285 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:07.588001 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:08.945473 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:10.302904 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:11.661055 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:13.018419 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:14.375470 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:15.733462 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:17.091697 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:18.448840 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:19.805264 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:20.286610 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:22.371599 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:22.851823 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:24.934560 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:25.415704 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:27.497518 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:27.978599 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:30.061048 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:30.541116 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:32.620910 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:33.100472 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:35.182207 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:35.663856 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:37.744009 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:38.222970 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:40.303786 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:40.783708 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:42.864035 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:43.344451 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:45.424781 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:45.904235 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:47.985787 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:48.465776 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:50.546539 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:51.027154 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:53.107163 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:53.586919 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:55.667634 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:56.147418 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:58.229463 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:58.709405 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:00.791238 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:01.271153 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:03.366312 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:03.846887 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:05.929254 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:06.409354 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:08.492940 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:08.973579 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:11.054365 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:11.535529 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:13.616333 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:14.096699 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:16.177014 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:16.657356 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:18.737185 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:19.217436 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:21.297365 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:21.777452 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:23.859141 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:24.341140 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:26.422820 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:26.901697 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:28.983382 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:29.462647 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:31.546230 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:32.026260 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:34.108302 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:34.588418 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:36.670409 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:37.150336 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:39.231375 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:39.710521 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:41.791812 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:42.270961 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:44.352190 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:44.832082 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:46.914699 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:47.394231 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:49.477998 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:49.957707 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:52.039835 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:52.519910 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:54.603408 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:55.084615 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:57.166184 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:57.647985 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:59.728832 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:00.209671 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:02.299814 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:02.792164 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:04.874731 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:05.356021 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:07.440057 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:07.921641 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:10.005472 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:10.487477 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:12.570305 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:13.049777 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:15.131258 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:15.610521 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:17.692421 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:18.172147 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:20.252815 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:20.732069 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:22.812251 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:23.291983 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:25.373623 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:25.853609 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:27.934235 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:28.414420 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:30.496245 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:30.975821 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:33.057400 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:33.537536 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:35.617818 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:36.097218 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:38.177214 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:38.656406 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:40.737509 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:41.216248 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:43.296101 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:43.775101 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:45.854631 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:46.333955 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:48.414206 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:48.893294 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:50.973657 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:51.453401 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:53.534772 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:54.013679 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:56.094383 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:56.573193 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:58.653684 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:59.132839 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:01.214081 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:01.697692 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:03.791988 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:04.273031 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:06.355095 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:06.836611 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:08.918303 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:09.398811 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:11.479619 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:11.959957 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:14.042604 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:14.523185 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:16.604574 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:17.084690 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:19.166476 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:19.645828 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:21.726127 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:22.205727 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:24.288051 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:24.767596 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:26.849660 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:27.330712 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:29.413072 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:29.892962 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:37.606720 [3] proc begin: <DistEnv 3/4 nccl>
21:44:12.187986 [3] graph loaded <COO Graph: e160M_f512_l16_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 3, |V|: 500000, |E|: 2349774>
21:44:12.197003 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1023 MiB |   1038 MiB |   1070 MiB |  48832 KiB |
|       from large pool |   1023 MiB |   1038 MiB |   1070 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1056 MiB |   1056 MiB |   1056 MiB |      0 B   |
|       from large pool |   1054 MiB |   1054 MiB |   1054 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  14306 KiB |  19998 KiB |  57928 KiB |  43621 KiB |
|       from large pool |  14306 KiB |  19998 KiB |  51779 KiB |  37473 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       3    |       3    |       5    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:44:30.347445 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:32.327724 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:33.335496 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:34.342029 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:35.345534 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:36.348716 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:37.352493 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:38.356315 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:39.360623 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:40.362596 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:41.363883 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:42.365394 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:43.366554 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:44.368201 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:45.368831 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:46.370348 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:47.371967 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:48.375358 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:49.379324 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:50.383155 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:51.386283 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:52.389406 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:53.393303 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:54.398276 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:55.400801 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:56.404071 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:57.409089 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:58.412009 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:59.415823 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:00.421456 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:01.433926 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:02.454598 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:03.463634 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:04.468795 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:05.476353 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:06.482732 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:07.489387 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:08.496731 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:09.503302 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:10.505593 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:11.509420 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:12.513256 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:13.517534 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:14.520748 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:15.525286 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:16.529151 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:17.532780 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:18.535681 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:19.539408 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:20.543270 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:21.545852 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:21.971690 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:23.409159 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:23.832365 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:25.264175 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:25.688622 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:27.121229 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:27.544858 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:28.978831 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:29.402968 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:30.835210 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:31.258256 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:32.688083 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:33.111351 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:34.540639 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:34.963543 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:36.392320 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:36.815871 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:38.246503 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:38.669749 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:40.099827 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:40.522547 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:41.952444 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:42.374719 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:43.805099 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:44.228049 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:45.657979 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:46.080256 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:47.509982 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:47.933879 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:49.363760 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:49.786978 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:51.218654 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:51.643076 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:53.071467 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:53.493914 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:54.924000 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:55.347077 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:56.778406 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:57.203076 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:58.633627 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:59.057861 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:00.489360 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:00.914478 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:02.355760 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:02.792155 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:04.223925 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:04.647774 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:06.079049 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:06.503865 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:07.935436 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:08.359659 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:09.790224 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:10.214048 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:11.644486 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:12.068213 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:13.498357 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:13.922053 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:15.352301 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:15.774998 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:17.203581 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:17.626515 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:19.057103 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:19.480097 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:20.911426 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:21.335008 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:22.766049 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:23.189854 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:24.623085 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:25.046287 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:26.479801 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:26.902070 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:28.335434 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:28.758909 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:30.191170 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:30.615026 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:32.046077 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:32.469566 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:33.899874 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:34.322449 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:35.752608 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:36.175974 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:37.610155 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:38.033483 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:39.463248 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:39.887374 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:41.321231 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:41.743972 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:43.177876 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:43.601164 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:45.035711 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:45.458575 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:46.893678 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:47.316609 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:48.748494 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:49.172090 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:50.602679 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:51.026507 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:52.458342 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:52.881846 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:54.312830 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:54.736103 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:56.168343 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:56.591184 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:58.021493 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:58.444745 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:59.877130 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:00.301223 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:01.742665 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:02.180243 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:03.614913 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:04.040062 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:05.469807 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:05.894375 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:07.325778 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:07.748990 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:09.180658 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:09.603239 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:11.034687 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:11.459553 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:12.888839 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:13.312104 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:14.743297 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:15.165728 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:16.599077 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:17.022718 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:18.456584 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:18.879342 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:20.313611 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:20.737701 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:22.170745 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:22.594880 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:24.027361 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:24.452754 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:25.886202 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:26.310378 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:27.739679 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:28.163837 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:29.594240 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:30.017920 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:31.449635 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:31.873240 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:33.304313 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:33.727004 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:35.158731 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:35.582075 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:37.014227 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:37.437866 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:38.868874 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:39.292107 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:46.887376 [3] proc begin: <DistEnv 3/4 nccl>
21:48:09.271827 [3] graph loaded <COO Graph: e160M_f512_l64_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 3, |V|: 500000, |E|: 2349774>
21:48:09.281263 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1023 MiB |   1038 MiB |   1070 MiB |  48832 KiB |
|       from large pool |   1023 MiB |   1038 MiB |   1070 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1056 MiB |   1056 MiB |   1056 MiB |      0 B   |
|       from large pool |   1054 MiB |   1054 MiB |   1054 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  14306 KiB |  19998 KiB |  57928 KiB |  43621 KiB |
|       from large pool |  14306 KiB |  19998 KiB |  51779 KiB |  37473 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       3    |       3    |       5    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:48:36.962713 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:38.953315 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:40.034650 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:41.114434 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:42.193221 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:43.272599 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:44.354489 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:45.436089 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:46.516430 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:47.596855 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:48.677026 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:49.757566 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:50.836911 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:51.917719 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:52.997806 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:54.077676 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:55.158405 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:56.238930 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:57.318654 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:58.396519 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:59.473458 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:00.549842 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:01.637201 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:02.730104 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:03.810139 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:04.887120 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:05.965945 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:07.043848 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:08.121383 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:09.198643 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:10.275557 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:11.353840 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:12.429947 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:13.508318 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:14.585017 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:15.660966 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:16.738562 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:17.815200 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:18.892112 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:19.970699 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:21.048622 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:22.126598 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:23.203923 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:24.280858 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:25.357563 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:26.434716 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:27.512905 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:28.590152 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:29.667905 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:30.745200 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:31.822921 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:32.321851 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:33.831007 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:34.329999 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:35.837830 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:36.336569 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:37.844356 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:38.343699 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:39.851477 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:40.350154 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:41.857328 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:42.356068 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:43.864885 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:44.363137 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:45.869622 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:46.367349 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:47.873386 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:48.371658 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:49.877736 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:50.375436 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:51.882320 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:52.380841 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:53.888749 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:54.387562 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:55.895471 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:56.392737 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:57.898648 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:58.398214 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:59.907157 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:00.405150 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:01.923880 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:02.437259 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:03.953902 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:04.452809 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:05.962791 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:06.461547 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:07.972359 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:08.471699 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:09.982823 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:10.481627 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:11.991649 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:12.489574 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:13.997586 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:14.496517 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:16.003116 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:16.501864 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:18.008192 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:18.506745 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:20.013096 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:20.511673 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:22.018325 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:22.516476 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:24.024799 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:24.523333 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:26.029727 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:26.528275 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:28.035605 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:28.533767 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:30.040032 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:30.538510 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:32.045359 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:32.543173 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:34.051326 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:34.549842 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:36.055893 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:36.553721 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:38.059542 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:38.557477 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:40.067482 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:40.565864 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:42.072805 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:42.571154 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:44.078831 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:44.577720 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:46.084826 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:46.583356 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:48.089241 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:48.587071 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:50.098240 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:50.596910 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:52.105006 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:52.603822 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:54.111278 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:54.609633 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:56.116692 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:56.614443 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:58.120095 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:58.618427 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:00.125561 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:00.623940 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:02.154399 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:02.663928 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:04.173299 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:04.672262 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:06.179804 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:06.678726 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:08.187168 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:08.687111 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:10.194483 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:10.692587 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:12.199352 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:12.698259 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:14.205318 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:14.703088 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:16.209346 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:16.707459 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:18.214250 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:18.712421 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:20.220331 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:20.717360 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:22.223452 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:22.722564 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:24.230455 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:24.729739 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:26.237275 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:26.736123 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:28.242617 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:28.741634 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:30.249295 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:30.748647 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:32.256859 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:32.755768 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:34.263771 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:34.761757 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:36.268584 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:36.767335 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:38.273449 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:38.771481 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:40.277894 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:40.775236 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:42.282264 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:42.780812 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:44.288159 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:44.786454 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:46.293296 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:46.791123 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:48.297675 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:48.795586 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:50.302852 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:50.800106 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:52.307199 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:52.804597 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:54.312182 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:54.810592 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:56.317545 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:56.816180 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:58.322744 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:58.822125 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:00.328689 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:00.827227 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:08.227598 [3] proc begin: <DistEnv 3/4 nccl>
21:52:12.509550 [3] graph loaded <COO Graph: e160M_f512_l32_t0.1, |V|: 2000000, |E|: 160000000, masks: 200000,200000,1600000><Local: 3, |V|: 500000, |E|: 2349774>
21:52:12.517223 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1023 MiB |   1038 MiB |   1070 MiB |  48832 KiB |
|       from large pool |   1023 MiB |   1038 MiB |   1070 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1056 MiB |   1056 MiB |   1056 MiB |      0 B   |
|       from large pool |   1054 MiB |   1054 MiB |   1054 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  14306 KiB |  19998 KiB |  57928 KiB |  43621 KiB |
|       from large pool |  14306 KiB |  19998 KiB |  51779 KiB |  37473 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       3    |       3    |       5    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:53:03.437189 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:05.431506 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:06.446437 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:07.463450 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:08.483642 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:09.500879 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:10.520133 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:11.539732 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:12.558735 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:13.579881 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:14.599544 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:15.619384 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:16.634438 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:17.648909 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:18.662799 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:19.677798 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:20.693278 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:21.707708 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:22.722207 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:23.737724 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:24.752533 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:25.767257 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:26.779516 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:27.791523 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:28.805221 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:29.820214 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:30.832743 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:31.845978 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:32.858761 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:33.871886 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:34.884312 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:35.898976 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:36.911902 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:37.924200 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:38.937360 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:39.950187 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:40.964187 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:41.977028 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:42.988736 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:44.002014 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:45.013571 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:46.025212 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:47.037575 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:48.050148 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:49.064266 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:50.076243 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:51.088185 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:52.100622 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:53.115756 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:54.128209 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:55.140426 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:55.576025 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:57.021189 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:57.455642 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:58.899105 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:59.332877 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:00.778031 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:01.211768 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:02.678393 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:03.125260 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:04.572079 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:05.006217 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:06.454193 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:06.888802 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:08.334868 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:08.769993 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:10.217259 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:10.652731 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:12.100719 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:12.535288 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:13.981095 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:14.415806 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:15.860912 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:16.294406 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:17.738328 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:18.171784 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:19.616678 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:20.050880 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:21.495709 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:21.929119 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:23.373068 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:23.807644 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:25.252821 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:25.687550 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:27.132501 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:27.566462 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:29.012570 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:29.447140 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:30.890672 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:31.323611 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:32.767786 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:33.201320 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:34.644143 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:35.076195 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:36.519220 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:36.952441 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:38.395962 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:38.828653 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:40.273418 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:40.706287 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:42.148173 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:42.581055 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:44.024241 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:44.456217 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:45.899353 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:46.332231 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:47.776148 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:48.209471 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:49.653269 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:50.085726 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:51.528817 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:51.963413 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:53.407903 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:53.841940 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:55.287149 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:55.721874 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:57.167268 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:57.600740 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:59.045173 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:59.479475 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:00.923888 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:01.360301 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:02.822571 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:03.258925 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:04.707932 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:05.142871 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:06.591081 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:07.026123 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:08.475037 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:08.912981 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:10.362263 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:10.796806 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:12.243613 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:12.677865 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:14.122441 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:14.557404 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:16.002832 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:16.437615 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:17.883621 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:18.316574 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:19.764797 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:20.198504 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:21.645377 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:22.078776 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:23.525746 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:23.959027 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:25.408367 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:25.843245 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:27.292336 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:27.727277 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:29.181495 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:29.618972 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:31.068504 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:31.503078 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:32.952910 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:33.388295 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:34.833861 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:35.268578 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:36.714620 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:37.149785 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:38.598368 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:39.033225 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:40.477723 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:40.911056 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:42.355539 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:42.790063 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:44.234469 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:44.668692 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:46.112548 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:46.547081 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:47.991939 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:48.425878 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:49.871859 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:50.305796 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:51.748984 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:52.182934 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:53.625940 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:54.058775 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:55.501910 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:55.935639 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:57.379956 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:57.814836 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:59.259352 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:59.694478 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.139285 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.572619 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.037831 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.477928 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:04.925709 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:05.360581 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:06.810745 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:07.245595 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:08.695175 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:09.130807 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:10.579329 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:11.014081 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:12.458832 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:12.894559 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:14.340018 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:14.775691 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:22.174348 [3] proc begin: <DistEnv 3/4 nccl>
21:56:51.867711 [3] graph loaded <COO Graph: e160M_f512_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 3, |V|: 500000, |E|: 2349774>
21:56:51.881486 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1023 MiB |   1038 MiB |   1070 MiB |  48832 KiB |
|       from large pool |   1023 MiB |   1038 MiB |   1070 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1056 MiB |   1056 MiB |   1056 MiB |      0 B   |
|       from large pool |   1054 MiB |   1054 MiB |   1054 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  14306 KiB |  19998 KiB |  57928 KiB |  43621 KiB |
|       from large pool |  14306 KiB |  19998 KiB |  51779 KiB |  37473 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       3    |       3    |       5    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:57:25.728956 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:27.788248 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:28.815472 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:29.840682 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:30.867129 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:31.893214 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:32.920588 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:33.947780 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:34.974282 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:35.999557 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:37.026589 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:38.053670 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:39.084348 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:40.110873 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:41.134966 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:42.158681 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:43.182432 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:44.206665 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:45.228252 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:46.251725 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:47.275353 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:48.297317 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:49.319459 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:50.340501 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:51.362157 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:52.386723 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:53.412271 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:54.437511 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:55.459047 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:56.483297 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:57.508270 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:58.533650 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:59.558603 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:00.583257 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:01.616178 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:02.657731 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:03.682165 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:04.706411 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:05.730194 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:06.752790 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:07.775707 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:08.798940 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:09.821670 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:10.845350 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:11.869319 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:12.892806 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:13.916220 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:14.939053 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:15.965930 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:16.990573 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:18.016676 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:18.464762 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:19.919468 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:20.364780 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:21.819365 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:22.265146 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:23.718529 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:24.163050 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:25.615755 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:26.060571 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:27.512193 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:27.957374 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:29.410843 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:29.856881 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:31.308287 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:31.752807 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:33.204874 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:33.649413 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:35.100015 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:35.545211 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:36.999900 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:37.444672 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:38.898811 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:39.343823 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:40.796597 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:41.242258 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:42.696912 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:43.143501 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:44.598219 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:45.043803 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:46.498376 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:46.943649 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:48.399784 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:48.847181 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:50.301945 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:50.748801 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:52.201063 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:52.646744 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:54.101065 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:54.547277 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:56.001718 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:56.447717 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:57.903328 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:58.347976 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:59.802444 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:00.248508 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:01.701852 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:02.160335 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:03.627491 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:04.073679 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:05.532394 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:05.979030 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:07.436147 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:07.882437 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:09.341244 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:09.787401 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:11.243640 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:11.689588 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:13.144529 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:13.589596 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:15.043004 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:15.489297 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:16.943374 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:17.389040 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:18.843439 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:19.287524 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:20.739537 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:21.183737 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:22.637174 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:23.082428 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:24.534847 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:24.979293 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:26.431421 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:26.875809 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:28.329213 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:28.774733 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:30.228420 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:30.674686 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:32.129358 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:32.574922 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:34.028273 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:34.473550 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:35.931553 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:36.377560 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:37.836233 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:38.281427 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:39.737108 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:40.182550 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:41.636769 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:42.082285 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:43.537182 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:43.982589 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:45.437963 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:45.883700 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:47.339097 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:47.783970 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:49.238244 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:49.682889 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:51.136168 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:51.580725 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:53.034398 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:53.478038 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:54.930500 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:55.374287 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:56.827896 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:57.272441 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:58.726063 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:59.170383 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:00.624056 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:01.067899 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:02.545124 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:02.994702 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:04.446846 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:04.890985 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:06.341293 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:06.785983 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:08.240240 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:08.686497 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:10.137204 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:10.581370 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:12.032268 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:12.475988 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:13.929784 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:14.375131 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:15.826982 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:16.270314 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:17.721802 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:18.166244 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:19.618000 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:20.062197 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:21.513816 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:21.957216 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:23.408916 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:23.852546 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:25.306779 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:25.751963 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:27.205071 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:27.651011 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:29.105566 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:29.548785 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:31.001226 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:31.446263 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:32.899419 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:33.344776 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:34.796871 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:35.240641 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:36.693574 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:37.138868 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:38.591201 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:39.035375 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:46.741394 [3] proc begin: <DistEnv 3/4 nccl>
22:00:47.977613 [3] graph loaded <COO Graph: e160M_f512_l32_t0.8, |V|: 2000000, |E|: 160000000, masks: 1600000,200000,200000><Local: 3, |V|: 500000, |E|: 2349774>
22:00:47.986764 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1023 MiB |   1038 MiB |   1070 MiB |  48832 KiB |
|       from large pool |   1023 MiB |   1038 MiB |   1070 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1056 MiB |   1056 MiB |   1056 MiB |      0 B   |
|       from large pool |   1054 MiB |   1054 MiB |   1054 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  14306 KiB |  19998 KiB |  57928 KiB |  43621 KiB |
|       from large pool |  14306 KiB |  19998 KiB |  51779 KiB |  37473 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       3    |       3    |       5    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:57:16.025112 [3] proc begin: <DistEnv 3/4 nccl>
15:57:30.766984 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
15:57:30.789511 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:59:24.535785 [3] proc begin: <DistEnv 3/4 nccl>
15:59:30.629494 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
15:59:30.648416 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:00:13.862253 [3] proc begin: <DistEnv 3/4 nccl>
16:00:20.212399 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:00:20.233558 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:03:35.382260 [3] proc begin: <DistEnv 3/4 nccl>
16:03:40.743028 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:03:40.760606 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:12:44.280697 [3] proc begin: <DistEnv 3/4 nccl>
16:12:49.294565 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:12:49.310402 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:20:23.786070 [3] proc begin: <DistEnv 3/4 nccl>
16:20:29.501463 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:20:29.532562 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:23:56.724407 [3] proc begin: <DistEnv 3/4 nccl>
16:24:01.274252 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:24:01.294547 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:28:42.407074 [3] proc begin: <DistEnv 3/4 nccl>
16:28:48.253167 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:28:48.273235 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:29:40.494841 [3] proc begin: <DistEnv 3/4 nccl>
16:29:45.896974 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:29:45.913244 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:33:59.012766 [3] proc begin: <DistEnv 3/4 nccl>
16:34:05.258501 [3] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 3, |V|: 58242, |E|: 27666811>
16:34:05.291891 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 357506 KiB | 359328 KiB | 362972 KiB |   5466 KiB |
|       from large pool | 355910 KiB | 357731 KiB | 361372 KiB |   5461 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 356519 KiB | 358339 KiB | 361980 KiB |   5460 KiB |
|       from large pool | 354926 KiB | 356746 KiB | 360386 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 385024 KiB | 385024 KiB | 385024 KiB |      0 B   |
|       from large pool | 382976 KiB | 382976 KiB | 382976 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  27517 KiB |  27745 KiB |  34351 KiB |   6834 KiB |
|       from large pool |  27065 KiB |  27065 KiB |  32527 KiB |   5461 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |       8    |       0    |
|       from large pool |       7    |       7    |       7    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:36:41.229115 [3] proc begin: <DistEnv 3/4 nccl>
16:36:54.851017 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
16:36:54.868495 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:37:04.383422 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:05.882446 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:06.645488 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:07.406330 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:08.165973 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:08.925282 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:09.684428 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:10.444124 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:11.204008 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:11.963214 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:12.725120 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:13.484611 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:14.243714 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:15.001929 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:15.760648 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:16.517857 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:17.275656 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:18.034195 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:18.793083 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:19.550638 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:20.309172 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:21.067354 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:21.824854 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:22.583441 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:23.341449 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:24.101295 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:24.859993 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:25.618734 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:26.376777 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:27.134261 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:27.892619 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:28.650636 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:29.409525 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:30.169431 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:30.926473 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:31.685142 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:32.443069 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:33.201005 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:33.960769 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:34.719462 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:35.478585 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:36.237354 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:36.995993 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:37.755622 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:38.514505 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:39.272734 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:40.031460 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:40.790194 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:41.547295 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:42.304850 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:43.063162 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:43.820850 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:44.578871 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:45.336147 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:46.094001 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:46.853707 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:47.610249 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:48.368727 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:49.125779 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:49.883634 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:50.641121 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:51.399594 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:52.157124 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:52.914585 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:53.673015 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:54.431370 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:55.189749 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:55.947463 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:56.705277 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:57.462883 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:58.222431 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:58.982433 [3] Warning: no training nodes in this partition! Backward fake loss.
16:37:59.739760 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:00.498720 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:01.256750 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:02.035554 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:02.826529 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:03.591274 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:04.351706 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:05.112641 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:05.873208 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:06.632349 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:07.394659 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:08.154764 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:08.914283 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:09.674320 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:10.435734 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:11.196784 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:11.958097 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:12.717185 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:13.477013 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:14.236714 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:14.995642 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:15.755650 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:16.515717 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:17.275372 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:18.036715 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:18.797018 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:19.556882 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:20.317587 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:21.075788 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:21.834117 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:22.593760 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:23.353336 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:24.113218 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:24.872831 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:25.632365 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:26.391900 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:27.151826 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:27.911436 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:28.670679 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:29.428834 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:30.188338 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:30.946302 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:31.704735 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:32.463855 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:33.221049 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:33.979561 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:34.736771 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:35.494373 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:36.252391 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:37.009827 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:37.769377 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:38.527646 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:39.286315 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:40.045583 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:40.802445 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:41.561142 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:42.319075 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:43.077764 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:43.836508 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:44.594374 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:45.352922 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:46.110990 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:46.869091 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:47.627571 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:48.385607 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:49.145857 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:49.904934 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:50.663451 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:51.421572 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:52.179272 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:52.937584 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:53.695524 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:54.453842 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:55.212154 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:55.969560 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:56.727565 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:57.485420 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:58.243455 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:59.002448 [3] Warning: no training nodes in this partition! Backward fake loss.
16:38:59.760141 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:00.518061 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:01.276151 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:02.034229 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:02.822939 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:03.606484 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:04.365384 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:05.123579 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:05.881654 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:06.640213 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:07.398576 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:08.158144 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:08.916340 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:09.673590 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:10.431410 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:11.190063 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:11.948532 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:12.706520 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:13.464681 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:14.223366 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:14.981470 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:15.739725 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:16.498999 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:17.258463 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:18.017792 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:18.776422 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:19.535178 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:20.294404 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:21.053882 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:21.812908 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:22.571215 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:23.328235 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:24.087776 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:24.845482 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:25.603600 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:26.362201 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:27.119991 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:27.879896 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:28.639600 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:29.399158 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:30.157288 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:30.916406 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:31.675875 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:32.435926 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:33.195519 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:33.954850 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:34.714035 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:35.474035 [3] Warning: no training nodes in this partition! Backward fake loss.
16:39:36.232544 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:25.104015 [3] proc begin: <DistEnv 3/4 nccl>
16:45:29.396010 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
16:45:29.418105 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:45:34.685941 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:36.320307 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:37.074629 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:37.830364 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:38.589108 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:39.345679 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:40.101490 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:40.859117 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:41.616664 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:42.374188 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:43.131241 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:43.889949 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:44.646176 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:45.402894 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:46.161058 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:46.918149 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:47.674002 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:48.430166 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:49.187495 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:49.944301 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:50.699490 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:51.455934 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:52.212803 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:52.968994 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:53.725917 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:54.481665 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:55.237401 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:55.993172 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:56.750839 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:57.507632 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:58.262700 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:59.019907 [3] Warning: no training nodes in this partition! Backward fake loss.
16:45:59.775310 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:00.531421 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:01.292704 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:02.082305 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:02.858443 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:03.615807 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:04.372808 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:05.129479 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:05.888451 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:06.645424 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:07.402081 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:08.161606 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:08.920742 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:09.681634 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:10.443686 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:11.204567 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:11.965908 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:12.726337 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:13.487568 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:14.247110 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:15.006870 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:15.765872 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:16.526218 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:17.285429 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:18.044823 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:18.805609 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:19.565277 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:20.323587 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:21.082330 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:21.840688 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:22.599819 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:23.358085 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:24.117742 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:24.876900 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:25.635822 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:26.394818 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:27.153573 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:27.912396 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:28.670513 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:29.429463 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:30.188198 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:30.946399 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:31.705754 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:32.465159 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:33.224960 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:33.982969 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:34.741909 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:35.500629 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:36.259708 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:37.019031 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:37.777815 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:38.537280 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:39.295856 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:40.054881 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:40.813353 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:41.571727 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:42.329853 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:43.088652 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:43.847393 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:44.606864 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:45.368434 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:46.129424 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:46.890918 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:47.651344 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:48.412808 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:49.174195 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:49.933245 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:50.692250 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:51.451198 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:52.208764 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:52.965212 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:53.723049 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:54.480762 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:55.237295 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:55.993586 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:56.749875 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:57.505385 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:58.261674 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.018900 [3] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.777109 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.534114 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:01.291056 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:02.065242 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:02.853631 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:03.622176 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:04.383873 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:05.144696 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:05.906394 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:06.666815 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:07.427808 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:08.187379 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:08.948312 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:09.710388 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:10.470406 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:11.229719 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:11.987067 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:12.744310 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:13.501237 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.258738 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.016413 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.773197 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.530913 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.288452 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.044989 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.801440 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.558611 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.316597 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.073159 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.830078 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.587283 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.344543 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.102729 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.860596 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.616232 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.372219 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.128045 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.884357 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.639445 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.396767 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.152916 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.908876 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.664491 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.420402 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.176384 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.932672 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.688143 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.444074 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.200623 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.956564 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.712535 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.469585 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.227256 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.982272 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.739264 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.496533 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.252758 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.007993 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.763994 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.520229 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:45.276306 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:46.031908 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:46.787698 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:47.543668 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.299309 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.055191 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.811788 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:50.567579 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:51.323869 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:52.079269 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:52.836023 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:53.593046 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:54.349020 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:55.104606 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:55.860575 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:56.616240 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:57.372481 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:58.128288 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:58.885187 [3] Warning: no training nodes in this partition! Backward fake loss.
16:47:59.644412 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:00.403219 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:01.161079 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:01.948969 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:02.733019 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:03.490928 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:04.248639 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:05.005817 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:05.765023 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:06.522882 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:13.723866 [3] proc begin: <DistEnv 3/4 nccl>
16:48:18.192280 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
16:48:18.212711 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:48:24.434115 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:27.223945 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:29.067207 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:30.909675 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:32.749333 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:34.590206 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:36.432948 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:38.275230 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:40.117332 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:41.957955 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:43.805066 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:45.645937 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:47.487984 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:49.329061 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:51.170903 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:53.013734 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:54.854130 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:56.697340 [3] Warning: no training nodes in this partition! Backward fake loss.
16:48:58.538480 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:00.379140 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:02.241404 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:04.119868 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:05.962898 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:07.804436 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:09.645912 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:11.488455 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:13.329706 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:15.169641 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:17.010740 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:18.848502 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:20.685943 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:22.522432 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:24.360188 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:26.193086 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.028084 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:29.864663 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:31.697214 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:33.531258 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:35.365173 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:37.200774 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:39.036375 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:40.870899 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:42.703705 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:44.538506 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:46.372491 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:48.206884 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:50.040361 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:51.873107 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:53.706583 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:55.539373 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:57.373561 [3] Warning: no training nodes in this partition! Backward fake loss.
16:49:59.207923 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:01.042004 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:02.934545 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:04.774715 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:06.613298 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:08.453348 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:10.294190 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:12.135030 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:13.976064 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:15.814593 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:17.654201 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:19.490892 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:21.328836 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:23.164749 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:25.001250 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:26.837708 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:28.674464 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:30.510647 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:32.348647 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:34.187372 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:36.024442 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:37.861911 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:39.701836 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:41.539653 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:43.377980 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:45.215292 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:47.052780 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:48.890585 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:50.729507 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:52.565263 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:54.398159 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:56.237037 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:58.073898 [3] Warning: no training nodes in this partition! Backward fake loss.
16:50:59.912552 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:01.748955 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:03.641064 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:05.480131 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:07.318773 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:09.159889 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:10.997875 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:12.836673 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:14.676948 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:16.515519 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:18.354771 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:20.195173 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:22.033992 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:23.874596 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:25.715834 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:27.556228 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:29.395357 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:31.234891 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:33.073150 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:34.910688 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:36.748434 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:38.585611 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:40.421949 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:42.258625 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:44.094580 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:45.932514 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:47.769727 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:49.606165 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:51.443433 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:53.279701 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:55.114685 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:56.951534 [3] Warning: no training nodes in this partition! Backward fake loss.
16:51:58.787090 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:00.623736 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:02.505572 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:04.350903 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:06.187683 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:08.024866 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:09.862940 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:11.696741 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:13.532721 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:15.370431 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:17.207326 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:19.043031 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:20.882359 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:22.721748 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:24.557704 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:26.390303 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:28.226413 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:30.063135 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:31.899716 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.736117 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:35.571440 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:37.407694 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:39.245505 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:41.086318 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:42.924388 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:44.760234 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.596679 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:48.433708 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:50.268628 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:52.102324 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:53.936399 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:55.770146 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:57.604205 [3] Warning: no training nodes in this partition! Backward fake loss.
16:52:59.438336 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:01.273360 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:03.159433 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:05.002111 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:06.843117 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:08.685743 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:10.523615 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:12.361777 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:14.195897 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:16.028865 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:17.864635 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:19.699540 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:21.532984 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:23.365191 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:25.198891 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:27.033949 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:28.867735 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:30.703464 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.540334 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:34.377744 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:36.213876 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:38.050956 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:39.886283 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:41.721552 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:43.555561 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:45.389083 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:47.221515 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:49.055601 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:50.889595 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:52.724201 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:54.557895 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:56.393773 [3] Warning: no training nodes in this partition! Backward fake loss.
16:53:58.227363 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:00.062450 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:01.916807 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:03.791325 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:05.629536 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:07.466267 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:09.302840 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:11.141758 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:12.978635 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:14.814020 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:16.648305 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:18.482135 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:20.316176 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:22.149158 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:23.982780 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:25.817207 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:27.650764 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:29.485868 [3] Warning: no training nodes in this partition! Backward fake loss.
16:54:31.320707 [3] Warning: no training nodes in this partition! Backward fake loss.
17:15:58.980192 [3] proc begin: <DistEnv 3/4 nccl>
17:16:04.440417 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
17:16:04.463183 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:16:10.694591 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:13.600036 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:15.448716 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:17.289905 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:19.124006 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:20.959006 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:22.791364 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:24.625583 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:26.458664 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:28.289625 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:30.122307 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:31.954240 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:33.787194 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:35.618245 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:37.452359 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:39.284717 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:41.115845 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:42.950435 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.782147 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.613007 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.446004 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.277836 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:52.109913 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:53.941789 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:55.772657 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.605576 [3] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.436816 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.268848 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.148041 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:04.990838 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.823595 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.660591 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.495537 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.327568 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:14.160771 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:15.992518 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:17.824222 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.657546 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:21.489491 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:23.322444 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:25.156696 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:26.988993 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:28.821291 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:30.653959 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:32.487766 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:34.320545 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:36.152258 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:37.985041 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:39.817453 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:41.651369 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:43.486610 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:45.320518 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:47.152759 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:48.984953 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.819019 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.651724 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:54.483965 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:56.316506 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:58.148709 [3] Warning: no training nodes in this partition! Backward fake loss.
17:17:59.981047 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:01.825532 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:03.703641 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:05.535302 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:07.372562 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:09.208656 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:11.044634 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:12.877303 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:14.711751 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:16.545978 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:18.380070 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:20.211299 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:22.044076 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:23.875235 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:25.707629 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:27.540230 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:29.372158 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:31.206467 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:33.041787 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:34.872984 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:36.704666 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:38.536131 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:40.368419 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:42.200145 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:44.034521 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:45.867168 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:47.700117 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:49.533385 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:51.364172 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:53.196345 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:55.028439 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:56.861894 [3] Warning: no training nodes in this partition! Backward fake loss.
17:18:58.694379 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:00.526067 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:02.370535 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:04.251895 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:06.089051 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:07.926612 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.764226 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.599420 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:13.435295 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:15.266372 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:17.096886 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:18.928363 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:20.760378 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:22.591866 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:24.424270 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:26.256978 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:28.087379 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:29.919391 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:31.750305 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:33.582365 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:35.413102 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:37.245126 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:39.077945 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:40.913045 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:42.745278 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:44.581924 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:46.412631 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:48.245947 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:50.078905 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:51.909862 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:53.740394 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:55.573941 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:57.407113 [3] Warning: no training nodes in this partition! Backward fake loss.
17:19:59.237951 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:01.069257 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:02.960533 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:04.799585 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:06.636215 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:08.473287 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:10.310352 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:12.142768 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:13.975276 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:15.807903 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:17.639680 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:19.471431 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:21.303898 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:23.137633 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:24.969462 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:26.802637 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:28.633970 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:30.467597 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:32.299306 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:34.132710 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:35.964421 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:37.796562 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:39.628278 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:41.461785 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:43.295417 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:45.128801 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:46.962230 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:48.795102 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:50.627914 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:52.461078 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:54.293237 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:56.125529 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:57.958146 [3] Warning: no training nodes in this partition! Backward fake loss.
17:20:59.789289 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:01.641222 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:03.515380 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:05.349435 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:07.181107 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:09.012908 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:10.844749 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:12.676436 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:14.509439 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:16.342769 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:18.174088 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:20.007011 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:21.841405 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:23.674192 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:25.506023 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:27.338496 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:29.172118 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:31.003665 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:32.835967 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:34.669845 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:36.503365 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:38.336123 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:40.169129 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:42.001140 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:43.833864 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:45.665455 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:47.498096 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:49.330560 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:51.162144 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:52.996410 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:54.829005 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:56.661473 [3] Warning: no training nodes in this partition! Backward fake loss.
17:21:58.492834 [3] Warning: no training nodes in this partition! Backward fake loss.
17:22:00.326376 [3] Warning: no training nodes in this partition! Backward fake loss.
17:22:02.179706 [3] Warning: no training nodes in this partition! Backward fake loss.
17:22:04.048767 [3] Warning: no training nodes in this partition! Backward fake loss.
17:22:05.883355 [3] Warning: no training nodes in this partition! Backward fake loss.
17:22:07.718592 [3] Warning: no training nodes in this partition! Backward fake loss.
17:22:09.553462 [3] Warning: no training nodes in this partition! Backward fake loss.
17:22:11.389579 [3] Warning: no training nodes in this partition! Backward fake loss.
17:22:13.222261 [3] Warning: no training nodes in this partition! Backward fake loss.
17:22:15.056845 [3] Warning: no training nodes in this partition! Backward fake loss.
17:22:16.889780 [3] Warning: no training nodes in this partition! Backward fake loss.
17:22:24.568663 [3] proc begin: <DistEnv 3/4 nccl>
17:22:28.594341 [3] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 3, |V|: 612258, |E|: 24545059>
17:22:28.612941 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467254 KiB | 486390 KiB | 524662 KiB |  57408 KiB |
|       from large pool | 467254 KiB | 486388 KiB | 524655 KiB |  57400 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466796 KiB | 485930 KiB | 524199 KiB |  57403 KiB |
|       from large pool | 466796 KiB | 485929 KiB | 524195 KiB |  57399 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 507904 KiB | 507904 KiB | 507904 KiB |      0 B   |
|       from large pool | 505856 KiB | 505856 KiB | 505856 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18121 KiB |  21515 KiB |  42661 KiB |  24540 KiB |
|       from large pool |  18121 KiB |  19468 KiB |  36513 KiB |  18391 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      17    |      18    |      20    |       3    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |      16    |       6    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:22:34.968124 [3] Warning: no training nodes in this partition! Backward fake loss.
17:22:39.690300 [3] Warning: no training nodes in this partition! Backward fake loss.
17:22:43.690913 [3] Warning: no training nodes in this partition! Backward fake loss.
17:22:47.690251 [3] Warning: no training nodes in this partition! Backward fake loss.
17:22:51.686049 [3] Warning: no training nodes in this partition! Backward fake loss.
17:22:55.680356 [3] Warning: no training nodes in this partition! Backward fake loss.
17:22:59.673587 [3] Warning: no training nodes in this partition! Backward fake loss.
17:23:03.726981 [3] Warning: no training nodes in this partition! Backward fake loss.
17:23:07.722739 [3] Warning: no training nodes in this partition! Backward fake loss.
17:23:11.719205 [3] Warning: no training nodes in this partition! Backward fake loss.
17:23:15.716673 [3] Warning: no training nodes in this partition! Backward fake loss.
17:23:19.709978 [3] Warning: no training nodes in this partition! Backward fake loss.
17:23:23.703941 [3] Warning: no training nodes in this partition! Backward fake loss.
17:23:27.696775 [3] Warning: no training nodes in this partition! Backward fake loss.
17:23:31.688980 [3] Warning: no training nodes in this partition! Backward fake loss.
17:23:35.683598 [3] Warning: no training nodes in this partition! Backward fake loss.
17:23:39.679997 [3] Warning: no training nodes in this partition! Backward fake loss.
17:23:43.674743 [3] Warning: no training nodes in this partition! Backward fake loss.
17:23:47.667559 [3] Warning: no training nodes in this partition! Backward fake loss.
17:23:51.660427 [3] Warning: no training nodes in this partition! Backward fake loss.
17:23:55.656470 [3] Warning: no training nodes in this partition! Backward fake loss.
17:23:59.649790 [3] Warning: no training nodes in this partition! Backward fake loss.
17:24:03.700579 [3] Warning: no training nodes in this partition! Backward fake loss.
17:24:07.696432 [3] Warning: no training nodes in this partition! Backward fake loss.
17:24:11.689575 [3] Warning: no training nodes in this partition! Backward fake loss.
17:24:15.684996 [3] Warning: no training nodes in this partition! Backward fake loss.
17:24:19.677889 [3] Warning: no training nodes in this partition! Backward fake loss.
17:24:23.673183 [3] Warning: no training nodes in this partition! Backward fake loss.
17:24:27.667823 [3] Warning: no training nodes in this partition! Backward fake loss.
17:24:31.660676 [3] Warning: no training nodes in this partition! Backward fake loss.
17:24:35.654207 [3] Warning: no training nodes in this partition! Backward fake loss.
17:24:39.647986 [3] Warning: no training nodes in this partition! Backward fake loss.
17:24:43.640483 [3] Warning: no training nodes in this partition! Backward fake loss.
17:24:47.634870 [3] Warning: no training nodes in this partition! Backward fake loss.
17:24:51.629400 [3] Warning: no training nodes in this partition! Backward fake loss.
17:24:55.621726 [3] Warning: no training nodes in this partition! Backward fake loss.
17:24:59.616751 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:03.667713 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.658574 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:11.651316 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:15.646028 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:19.637622 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:23.628529 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:27.621571 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:31.614031 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:35.607121 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:39.599407 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:43.594670 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:47.590144 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:51.583218 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:55.576733 [3] Warning: no training nodes in this partition! Backward fake loss.
17:25:59.568268 [3] Warning: no training nodes in this partition! Backward fake loss.
17:26:03.621035 [3] Warning: no training nodes in this partition! Backward fake loss.
17:26:07.618917 [3] Warning: no training nodes in this partition! Backward fake loss.
17:26:11.614223 [3] Warning: no training nodes in this partition! Backward fake loss.
17:26:15.608280 [3] Warning: no training nodes in this partition! Backward fake loss.
17:26:19.602624 [3] Warning: no training nodes in this partition! Backward fake loss.
17:26:23.595734 [3] Warning: no training nodes in this partition! Backward fake loss.
17:26:27.591043 [3] Warning: no training nodes in this partition! Backward fake loss.
17:26:31.586413 [3] Warning: no training nodes in this partition! Backward fake loss.
17:26:35.587962 [3] Warning: no training nodes in this partition! Backward fake loss.
17:26:39.584899 [3] Warning: no training nodes in this partition! Backward fake loss.
17:26:43.580341 [3] Warning: no training nodes in this partition! Backward fake loss.
17:26:47.577887 [3] Warning: no training nodes in this partition! Backward fake loss.
17:26:51.577282 [3] Warning: no training nodes in this partition! Backward fake loss.
17:26:55.575310 [3] Warning: no training nodes in this partition! Backward fake loss.
17:26:59.571449 [3] Warning: no training nodes in this partition! Backward fake loss.
17:27:03.626479 [3] Warning: no training nodes in this partition! Backward fake loss.
17:27:07.623158 [3] Warning: no training nodes in this partition! Backward fake loss.
17:27:11.620540 [3] Warning: no training nodes in this partition! Backward fake loss.
17:27:15.617022 [3] Warning: no training nodes in this partition! Backward fake loss.
17:27:19.715940 [3] Warning: no training nodes in this partition! Backward fake loss.
17:27:23.746070 [3] Warning: no training nodes in this partition! Backward fake loss.
17:27:27.817362 [3] Warning: no training nodes in this partition! Backward fake loss.
17:27:31.850318 [3] Warning: no training nodes in this partition! Backward fake loss.
17:27:35.846789 [3] Warning: no training nodes in this partition! Backward fake loss.
17:27:39.845422 [3] Warning: no training nodes in this partition! Backward fake loss.
17:27:43.846134 [3] Warning: no training nodes in this partition! Backward fake loss.
17:27:47.851004 [3] Warning: no training nodes in this partition! Backward fake loss.
17:27:51.844539 [3] Warning: no training nodes in this partition! Backward fake loss.
17:27:55.837331 [3] Warning: no training nodes in this partition! Backward fake loss.
17:27:59.831042 [3] Warning: no training nodes in this partition! Backward fake loss.
17:28:03.884658 [3] Warning: no training nodes in this partition! Backward fake loss.
17:28:07.884355 [3] Warning: no training nodes in this partition! Backward fake loss.
17:28:11.879959 [3] Warning: no training nodes in this partition! Backward fake loss.
17:28:15.879384 [3] Warning: no training nodes in this partition! Backward fake loss.
17:28:19.884078 [3] Warning: no training nodes in this partition! Backward fake loss.
17:28:23.878334 [3] Warning: no training nodes in this partition! Backward fake loss.
17:28:27.871623 [3] Warning: no training nodes in this partition! Backward fake loss.
17:28:31.866437 [3] Warning: no training nodes in this partition! Backward fake loss.
17:28:35.859412 [3] Warning: no training nodes in this partition! Backward fake loss.
17:28:39.855032 [3] Warning: no training nodes in this partition! Backward fake loss.
17:28:43.846408 [3] Warning: no training nodes in this partition! Backward fake loss.
17:28:47.843275 [3] Warning: no training nodes in this partition! Backward fake loss.
17:28:51.837110 [3] Warning: no training nodes in this partition! Backward fake loss.
17:28:55.831558 [3] Warning: no training nodes in this partition! Backward fake loss.
17:28:59.827678 [3] Warning: no training nodes in this partition! Backward fake loss.
17:29:03.879260 [3] Warning: no training nodes in this partition! Backward fake loss.
17:29:07.874400 [3] Warning: no training nodes in this partition! Backward fake loss.
17:29:11.873008 [3] Warning: no training nodes in this partition! Backward fake loss.
17:29:15.869619 [3] Warning: no training nodes in this partition! Backward fake loss.
17:29:19.865603 [3] Warning: no training nodes in this partition! Backward fake loss.
17:29:23.862533 [3] Warning: no training nodes in this partition! Backward fake loss.
17:29:27.859123 [3] Warning: no training nodes in this partition! Backward fake loss.
17:29:31.853487 [3] Warning: no training nodes in this partition! Backward fake loss.
17:29:35.848335 [3] Warning: no training nodes in this partition! Backward fake loss.
17:29:39.842901 [3] Warning: no training nodes in this partition! Backward fake loss.
17:29:43.837265 [3] Warning: no training nodes in this partition! Backward fake loss.
17:29:47.834343 [3] Warning: no training nodes in this partition! Backward fake loss.
17:29:51.829532 [3] Warning: no training nodes in this partition! Backward fake loss.
17:29:55.825639 [3] Warning: no training nodes in this partition! Backward fake loss.
17:29:59.822575 [3] Warning: no training nodes in this partition! Backward fake loss.
17:30:03.878100 [3] Warning: no training nodes in this partition! Backward fake loss.
17:30:07.869735 [3] Warning: no training nodes in this partition! Backward fake loss.
17:30:11.869467 [3] Warning: no training nodes in this partition! Backward fake loss.
17:30:15.868450 [3] Warning: no training nodes in this partition! Backward fake loss.
17:30:19.861880 [3] Warning: no training nodes in this partition! Backward fake loss.
17:30:23.854900 [3] Warning: no training nodes in this partition! Backward fake loss.
17:30:27.846913 [3] Warning: no training nodes in this partition! Backward fake loss.
17:30:31.839691 [3] Warning: no training nodes in this partition! Backward fake loss.
17:30:35.832575 [3] Warning: no training nodes in this partition! Backward fake loss.
17:30:39.825818 [3] Warning: no training nodes in this partition! Backward fake loss.
17:30:43.819156 [3] Warning: no training nodes in this partition! Backward fake loss.
17:30:47.814869 [3] Warning: no training nodes in this partition! Backward fake loss.
17:30:51.809498 [3] Warning: no training nodes in this partition! Backward fake loss.
17:30:55.801989 [3] Warning: no training nodes in this partition! Backward fake loss.
17:30:59.796217 [3] Warning: no training nodes in this partition! Backward fake loss.
17:31:03.848317 [3] Warning: no training nodes in this partition! Backward fake loss.
17:31:07.840783 [3] Warning: no training nodes in this partition! Backward fake loss.
17:31:11.833835 [3] Warning: no training nodes in this partition! Backward fake loss.
17:31:15.825862 [3] Warning: no training nodes in this partition! Backward fake loss.
17:31:19.817213 [3] Warning: no training nodes in this partition! Backward fake loss.
17:31:23.810235 [3] Warning: no training nodes in this partition! Backward fake loss.
17:31:27.801623 [3] Warning: no training nodes in this partition! Backward fake loss.
17:31:31.795327 [3] Warning: no training nodes in this partition! Backward fake loss.
17:31:35.787149 [3] Warning: no training nodes in this partition! Backward fake loss.
17:31:39.779230 [3] Warning: no training nodes in this partition! Backward fake loss.
17:31:43.772588 [3] Warning: no training nodes in this partition! Backward fake loss.
17:31:47.766648 [3] Warning: no training nodes in this partition! Backward fake loss.
17:31:51.760235 [3] Warning: no training nodes in this partition! Backward fake loss.
17:31:55.753656 [3] Warning: no training nodes in this partition! Backward fake loss.
17:31:59.745750 [3] Warning: no training nodes in this partition! Backward fake loss.
17:32:03.799473 [3] Warning: no training nodes in this partition! Backward fake loss.
17:32:07.795474 [3] Warning: no training nodes in this partition! Backward fake loss.
17:32:11.787301 [3] Warning: no training nodes in this partition! Backward fake loss.
17:32:15.780664 [3] Warning: no training nodes in this partition! Backward fake loss.
17:32:19.772883 [3] Warning: no training nodes in this partition! Backward fake loss.
17:32:23.764175 [3] Warning: no training nodes in this partition! Backward fake loss.
17:32:27.754980 [3] Warning: no training nodes in this partition! Backward fake loss.
17:32:31.748785 [3] Warning: no training nodes in this partition! Backward fake loss.
17:32:35.742568 [3] Warning: no training nodes in this partition! Backward fake loss.
17:32:39.734964 [3] Warning: no training nodes in this partition! Backward fake loss.
17:32:43.725783 [3] Warning: no training nodes in this partition! Backward fake loss.
17:32:47.718549 [3] Warning: no training nodes in this partition! Backward fake loss.
17:32:51.709480 [3] Warning: no training nodes in this partition! Backward fake loss.
17:32:55.701996 [3] Warning: no training nodes in this partition! Backward fake loss.
17:32:59.694705 [3] Warning: no training nodes in this partition! Backward fake loss.
17:33:03.742789 [3] Warning: no training nodes in this partition! Backward fake loss.
17:33:07.743950 [3] Warning: no training nodes in this partition! Backward fake loss.
17:33:11.738267 [3] Warning: no training nodes in this partition! Backward fake loss.
17:33:15.733548 [3] Warning: no training nodes in this partition! Backward fake loss.
17:33:19.729778 [3] Warning: no training nodes in this partition! Backward fake loss.
17:33:23.725447 [3] Warning: no training nodes in this partition! Backward fake loss.
17:33:27.719482 [3] Warning: no training nodes in this partition! Backward fake loss.
17:33:31.711939 [3] Warning: no training nodes in this partition! Backward fake loss.
17:33:35.708061 [3] Warning: no training nodes in this partition! Backward fake loss.
17:33:39.701226 [3] Warning: no training nodes in this partition! Backward fake loss.
17:33:43.696472 [3] Warning: no training nodes in this partition! Backward fake loss.
17:33:47.691193 [3] Warning: no training nodes in this partition! Backward fake loss.
17:33:51.685186 [3] Warning: no training nodes in this partition! Backward fake loss.
17:33:55.680892 [3] Warning: no training nodes in this partition! Backward fake loss.
17:33:59.673085 [3] Warning: no training nodes in this partition! Backward fake loss.
17:34:03.724297 [3] Warning: no training nodes in this partition! Backward fake loss.
17:34:07.719918 [3] Warning: no training nodes in this partition! Backward fake loss.
17:34:11.714116 [3] Warning: no training nodes in this partition! Backward fake loss.
17:34:15.708955 [3] Warning: no training nodes in this partition! Backward fake loss.
17:34:19.702920 [3] Warning: no training nodes in this partition! Backward fake loss.
17:34:23.697379 [3] Warning: no training nodes in this partition! Backward fake loss.
17:34:27.693191 [3] Warning: no training nodes in this partition! Backward fake loss.
17:34:31.688334 [3] Warning: no training nodes in this partition! Backward fake loss.
17:34:35.682293 [3] Warning: no training nodes in this partition! Backward fake loss.
17:34:39.677100 [3] Warning: no training nodes in this partition! Backward fake loss.
17:34:43.673814 [3] Warning: no training nodes in this partition! Backward fake loss.
17:34:47.669308 [3] Warning: no training nodes in this partition! Backward fake loss.
17:34:51.663083 [3] Warning: no training nodes in this partition! Backward fake loss.
17:34:55.657553 [3] Warning: no training nodes in this partition! Backward fake loss.
17:34:59.652832 [3] Warning: no training nodes in this partition! Backward fake loss.
17:35:03.704722 [3] Warning: no training nodes in this partition! Backward fake loss.
17:35:07.700678 [3] Warning: no training nodes in this partition! Backward fake loss.
17:35:11.694874 [3] Warning: no training nodes in this partition! Backward fake loss.
17:35:15.689831 [3] Warning: no training nodes in this partition! Backward fake loss.
17:35:19.684129 [3] Warning: no training nodes in this partition! Backward fake loss.
17:35:23.678273 [3] Warning: no training nodes in this partition! Backward fake loss.
17:35:27.672585 [3] Warning: no training nodes in this partition! Backward fake loss.
17:35:31.668075 [3] Warning: no training nodes in this partition! Backward fake loss.
17:35:35.664178 [3] Warning: no training nodes in this partition! Backward fake loss.
17:35:39.660158 [3] Warning: no training nodes in this partition! Backward fake loss.
17:35:43.653270 [3] Warning: no training nodes in this partition! Backward fake loss.
17:35:47.647165 [3] Warning: no training nodes in this partition! Backward fake loss.
17:35:51.642646 [3] Warning: no training nodes in this partition! Backward fake loss.
