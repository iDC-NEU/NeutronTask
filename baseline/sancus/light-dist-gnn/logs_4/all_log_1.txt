15:46:26.654462 [1] proc begin: <DistEnv 1/4 nccl>
16:08:59.895639 [1] proc begin: <DistEnv 1/4 nccl>
16:10:30.568299 [1] proc begin: <DistEnv 1/4 nccl>
17:06:50.100961 [1] proc begin: <DistEnv 1/4 nccl>
17:06:57.057852 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
17:06:57.061330 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  467269 KB |  505541 KB |  764309 KB |  297040 KB |
|       from large pool |  467269 KB |  505541 KB |  764286 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| Active memory         |  467269 KB |  505541 KB |  764309 KB |  297040 KB |
|       from large pool |  467269 KB |  505541 KB |  764286 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  522240 KB |  522240 KB |  522240 KB |       0 B  |
|       from large pool |  520192 KB |  520192 KB |  520192 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   52922 KB |  216906 KB |  301631 KB |  248708 KB |
|       from large pool |   52922 KB |  216906 KB |  279090 KB |  226168 KB |
|       from small pool |       0 KB |    2047 KB |   22540 KB |   22540 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |      14    |      11    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |      11    |      11    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:07:03.305444 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:04.020896 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:04.564142 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:05.110613 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:05.651775 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:06.193857 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:06.733430 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:07.278023 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:07.819435 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:08.366033 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:08.914605 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:09.512305 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:10.061133 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:10.612448 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:11.158627 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:11.703527 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:12.247421 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:12.789041 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:13.335154 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:13.882805 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:14.429081 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:15.027713 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:15.572435 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:16.120245 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:16.664387 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:17.210689 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:17.755955 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:18.303005 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:18.845212 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:19.392246 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:19.937416 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:20.536759 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:21.081862 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:21.625547 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:22.171297 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:22.710272 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:23.254172 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:23.796658 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:24.339365 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:24.883839 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:25.426448 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:26.023770 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:26.566586 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:27.108615 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:27.646249 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:28.185355 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:28.723265 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:29.264320 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:29.800857 [1] Warning: no training nodes in this partition! Backward fake loss.
17:07:30.341047 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:47.135353 [1] proc begin: <DistEnv 1/4 nccl>
17:17:48.944784 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
17:17:48.945920 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3859 KB |    3881 KB |    3932 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      69 KB |      91 KB |     142 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3859 KB |    3881 KB |    3932 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      69 KB |      91 KB |     142 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18668 KB |   18707 KB |   18808 KB |  142848 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1978 KB |    2045 KB |    2118 KB |  142848 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:17:50.338090 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.507409 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.582919 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.642922 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.709000 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.771969 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.831612 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.893373 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.956563 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.021929 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.084991 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.157633 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.228469 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.299347 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.382721 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.447867 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.507547 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.577461 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.643438 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.712996 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.780058 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.856853 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.923677 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.990921 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.056433 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.129504 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.210860 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.285072 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.350121 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.414921 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.491784 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.559917 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.626864 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.697363 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.783686 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.886689 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.992929 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.068782 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.147352 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.223327 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.298074 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.369815 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.445371 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.519633 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.590005 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.683239 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.788549 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.895278 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.995798 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:54.105970 [1] Warning: no training nodes in this partition! Backward fake loss.
13:29:51.791501 [1] proc begin: <DistEnv 1/4 nccl>
13:30:15.706258 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
13:30:15.710392 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  367349 KB |  369174 KB |  372839 KB |    5489 KB |
|       from large pool |  365753 KB |  367576 KB |  371222 KB |    5469 KB |
|       from small pool |    1596 KB |    1598 KB |    1616 KB |      20 KB |
|---------------------------------------------------------------------------|
| Active memory         |  367349 KB |  369174 KB |  372839 KB |    5489 KB |
|       from large pool |  365753 KB |  367576 KB |  371222 KB |    5469 KB |
|       from small pool |    1596 KB |    1598 KB |    1616 KB |      20 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  393216 KB |  393216 KB |  393216 KB |       0 B  |
|       from large pool |  391168 KB |  391168 KB |  391168 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   25866 KB |   25866 KB |   32724 KB |    6857 KB |
|       from large pool |   25414 KB |   25414 KB |   30883 KB |    5469 KB |
|       from small pool |     452 KB |    1820 KB |    1840 KB |    1388 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      53    |      36    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      40    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      53    |      36    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      40    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:33:33.360141 [1] proc begin: <DistEnv 1/4 nccl>
13:33:42.074231 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
13:33:42.077616 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  367349 KB |  369174 KB |  372839 KB |    5489 KB |
|       from large pool |  365753 KB |  367576 KB |  371222 KB |    5469 KB |
|       from small pool |    1596 KB |    1598 KB |    1616 KB |      20 KB |
|---------------------------------------------------------------------------|
| Active memory         |  367349 KB |  369174 KB |  372839 KB |    5489 KB |
|       from large pool |  365753 KB |  367576 KB |  371222 KB |    5469 KB |
|       from small pool |    1596 KB |    1598 KB |    1616 KB |      20 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  393216 KB |  393216 KB |  393216 KB |       0 B  |
|       from large pool |  391168 KB |  391168 KB |  391168 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   25866 KB |   25866 KB |   32724 KB |    6857 KB |
|       from large pool |   25414 KB |   25414 KB |   30883 KB |    5469 KB |
|       from small pool |     452 KB |    1820 KB |    1840 KB |    1388 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      53    |      36    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      40    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      53    |      36    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      40    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:35:26.576884 [1] proc begin: <DistEnv 1/4 nccl>
13:35:51.531640 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
13:35:51.536229 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  467269 KB |  505541 KB |  764309 KB |  297040 KB |
|       from large pool |  467269 KB |  505541 KB |  764286 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| Active memory         |  467269 KB |  505541 KB |  764309 KB |  297040 KB |
|       from large pool |  467269 KB |  505541 KB |  764286 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  522240 KB |  522240 KB |  522240 KB |       0 B  |
|       from large pool |  520192 KB |  520192 KB |  520192 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   52922 KB |  216906 KB |  301631 KB |  248708 KB |
|       from large pool |   52922 KB |  216906 KB |  279090 KB |  226168 KB |
|       from small pool |       0 KB |    2047 KB |   22540 KB |   22540 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |      14    |      11    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |      11    |      11    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:35:57.958349 [1] Warning: no training nodes in this partition! Backward fake loss.
13:35:58.674512 [1] Warning: no training nodes in this partition! Backward fake loss.
13:35:59.219534 [1] Warning: no training nodes in this partition! Backward fake loss.
13:35:59.760510 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:00.303214 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:00.846222 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:01.390910 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:01.965047 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:02.536429 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:03.101919 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:03.645729 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:04.247406 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:04.794910 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:05.342603 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:05.889609 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:06.438113 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:06.987284 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:07.537420 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:08.090105 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:08.637325 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:09.186358 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:09.782977 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:10.327789 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:10.875418 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:11.421641 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:11.970953 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:12.517598 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:13.067937 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:13.616237 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:14.169659 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:14.717179 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:15.315446 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:15.859996 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:16.403447 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:16.942874 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:17.483248 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:18.026347 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:18.563010 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:19.108106 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:19.648248 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:20.191466 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:20.784703 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:21.333263 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:21.876246 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:22.416959 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:22.962817 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:23.502922 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:24.042673 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:24.583262 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:25.127437 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:25.667279 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:26.260454 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:26.800072 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:27.345729 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:27.888301 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:28.431976 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:28.972994 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:29.517211 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:30.058088 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:30.598099 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:31.142147 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:31.734617 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:32.277345 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:32.819467 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:33.358563 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:33.903177 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:34.447434 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:34.994993 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:35.542537 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:36.096470 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:36.644061 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:37.252930 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:37.797327 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:38.345378 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:38.891970 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:39.435901 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:39.979069 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:40.520565 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:41.060060 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:41.596897 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:42.135757 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:42.728144 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:43.267218 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:43.804698 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:44.344217 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:44.883493 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:45.425608 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:45.963635 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:46.501410 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:47.042842 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:47.581408 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:48.177069 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:48.714066 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:49.253992 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:49.790451 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:50.328498 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:50.865743 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:51.408517 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:51.951543 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:52.492063 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:53.035550 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:53.629720 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:54.171114 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:54.711186 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:55.254691 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:55.793991 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:56.336381 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:56.873754 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:57.414191 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:57.958548 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:58.498754 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:59.095175 [1] Warning: no training nodes in this partition! Backward fake loss.
13:36:59.633740 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:00.178110 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:00.717688 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:01.262237 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:01.802876 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:02.367027 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:02.939991 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:03.506633 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:04.051732 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:04.648628 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:05.198610 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:05.744257 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:06.298818 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:06.849982 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:07.396806 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:07.942923 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:08.488476 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:09.038896 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:09.582570 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:10.179912 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:10.718262 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:11.259690 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:11.795806 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:12.334254 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:12.872879 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:13.415564 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:13.958939 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:14.501921 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:15.043534 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:15.638127 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:16.182653 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:16.722894 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:17.266290 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:17.804395 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:18.345225 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:18.886032 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:19.425649 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:19.967138 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:20.508755 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:21.104362 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:21.644820 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:22.191130 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:22.730398 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:23.274718 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:23.815239 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:24.357880 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:24.900354 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:25.441795 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:25.984767 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:26.578173 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:27.123875 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:27.663927 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:28.204995 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:28.747429 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:29.294418 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:29.840379 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:30.387142 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:30.936664 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:31.485838 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:32.087015 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:32.630475 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:33.178113 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:33.715596 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:34.254947 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:34.793245 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:35.331309 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:35.868166 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:36.405642 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:36.946148 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:37.542949 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:38.084931 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:38.625591 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:39.165348 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:39.703197 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:40.243566 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:40.780009 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:41.319211 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:41.859779 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:42.396514 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:42.986198 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:43.527490 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:44.066595 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:44.605803 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:45.146689 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:45.681993 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:46.221463 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:46.763084 [1] Warning: no training nodes in this partition! Backward fake loss.
13:37:47.302906 [1] Warning: no training nodes in this partition! Backward fake loss.
13:39:46.266492 [1] proc begin: <DistEnv 1/4 nccl>
13:39:48.854716 [1] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 1, |V|: 42336, |E|: 336780>
13:39:48.857840 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   26283 KB |   44155 KB |   51540 KB |   25256 KB |
|       from large pool |   22491 KB |   43659 KB |   47739 KB |   25248 KB |
|       from small pool |    3792 KB |    3794 KB |    3801 KB |       8 KB |
|---------------------------------------------------------------------------|
| Active memory         |   26283 KB |   44155 KB |   51540 KB |   25256 KB |
|       from large pool |   22491 KB |   43659 KB |   47739 KB |   25248 KB |
|       from small pool |    3792 KB |    3794 KB |    3801 KB |       8 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   69632 KB |   69632 KB |   69632 KB |       0 B  |
|       from large pool |   65536 KB |   65536 KB |   65536 KB |       0 B  |
|       from small pool |    4096 KB |    4096 KB |    4096 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   20820 KB |   23428 KB |   29429 KB |    8608 KB |
|       from large pool |   20517 KB |   21877 KB |   25957 KB |    5440 KB |
|       from small pool |     303 KB |    1882 KB |    3472 KB |    3168 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      38    |      21    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      32    |      17    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      38    |      21    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      32    |      17    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       8    |       4    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:55:32.307397 [1] proc begin: <DistEnv 1/4 nccl>
13:55:38.547130 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
13:55:38.550682 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  467269 KB |  505541 KB |  764309 KB |  297040 KB |
|       from large pool |  467269 KB |  505541 KB |  764286 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| Active memory         |  467269 KB |  505541 KB |  764309 KB |  297040 KB |
|       from large pool |  467269 KB |  505541 KB |  764286 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  522240 KB |  522240 KB |  522240 KB |       0 B  |
|       from large pool |  520192 KB |  520192 KB |  520192 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   52922 KB |  216906 KB |  301631 KB |  248708 KB |
|       from large pool |   52922 KB |  216906 KB |  279090 KB |  226168 KB |
|       from small pool |       0 KB |    2047 KB |   22540 KB |   22540 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |      14    |      11    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |      11    |      11    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:55:43.805175 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:44.513912 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:45.050548 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:45.591872 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:46.131723 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:46.674396 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:47.214479 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:47.759506 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:48.297691 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:48.841673 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:49.382884 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:49.976412 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:50.518270 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:51.056639 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:51.595291 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:52.135492 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:52.673129 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:53.208239 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:53.744413 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:54.278912 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:54.817312 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:55.407111 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:55.944424 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:56.481317 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:57.021730 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:57.558766 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:58.094547 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:58.635106 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:59.171493 [1] Warning: no training nodes in this partition! Backward fake loss.
13:55:59.710701 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:00.242423 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:00.835850 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:01.376186 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:01.941766 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:02.507173 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:03.064393 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:03.604372 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:04.144684 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:04.683870 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:05.221749 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:05.761100 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:06.350599 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:06.891253 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:07.427690 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:07.966333 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:08.505984 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:09.042109 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:09.581331 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:10.117699 [1] Warning: no training nodes in this partition! Backward fake loss.
13:56:10.658196 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:09.579432 [1] proc begin: <DistEnv 1/4 nccl>
13:59:11.800062 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
13:59:11.801511 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3859 KB |    3881 KB |    3932 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      69 KB |      91 KB |     142 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3859 KB |    3881 KB |    3932 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      69 KB |      91 KB |     142 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18668 KB |   18707 KB |   18808 KB |  142848 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1978 KB |    2045 KB |    2118 KB |  142848 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:59:13.630231 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:13.851428 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:13.927792 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.032792 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.103190 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.182238 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.254800 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.330428 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.412808 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.516064 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.623998 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.726968 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.830902 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.918521 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.002653 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.098327 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.195543 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.303234 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.396205 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.502092 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.609019 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.717476 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.822157 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.924640 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.023879 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.121622 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.221398 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.322114 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.421543 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.526160 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.632773 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.718007 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.800175 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.872836 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.941755 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.007287 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.080080 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.159816 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.255698 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.352840 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.447462 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.535251 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.605062 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.704760 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.812121 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.912858 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:18.018002 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:18.088410 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:18.159209 [1] Warning: no training nodes in this partition! Backward fake loss.
13:59:18.252467 [1] Warning: no training nodes in this partition! Backward fake loss.
15:17:46.791892 [1] proc begin: <DistEnv 1/4 nccl>
15:17:48.019221 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
15:17:48.020213 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3859 KB |    3881 KB |    3932 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      69 KB |      91 KB |     142 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3859 KB |    3881 KB |    3932 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      69 KB |      91 KB |     142 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18668 KB |   18707 KB |   18808 KB |  142848 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1978 KB |    2045 KB |    2118 KB |  142848 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:18:13.985563 [1] proc begin: <DistEnv 1/4 nccl>
15:18:15.441695 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
15:18:15.442831 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3859 KB |    3881 KB |    3932 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      69 KB |      91 KB |     142 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3859 KB |    3881 KB |    3932 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      69 KB |      91 KB |     142 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18668 KB |   18707 KB |   18808 KB |  142848 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1978 KB |    2045 KB |    2118 KB |  142848 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:18:16.860218 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.001476 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.018312 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.037531 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.055748 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.073131 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.088102 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.103562 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.117093 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.131393 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.144079 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.159690 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.172181 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.183320 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.198221 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.212925 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.225464 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.240601 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.252311 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.267271 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.282201 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.297651 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.313320 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.330205 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.348553 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.364631 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.377930 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.393268 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.406901 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.421634 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.439598 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.460167 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.476718 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.494317 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.512922 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.534768 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.550717 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.562972 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.577892 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.592394 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.605052 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.623939 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.642961 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.665128 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.685760 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.706322 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.726765 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.746213 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.767136 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.785714 [1] Warning: no training nodes in this partition! Backward fake loss.
16:04:46.949508 [1] proc begin: <DistEnv 1/4 nccl>
16:04:51.027458 [1] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 1, |V|: 42336, |E|: 336780>
16:04:51.061459 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  26283 KiB |  44155 KiB |  51536 KiB |  25252 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47739 KiB |  25248 KiB |
|       from small pool |   3792 KiB |   3794 KiB |   3797 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         |  26283 KiB |  44155 KiB |  51536 KiB |  25252 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47739 KiB |  25248 KiB |
|       from small pool |   3792 KiB |   3794 KiB |   3797 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |  26279 KiB |  44155 KiB |  51417 KiB |  25137 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47628 KiB |  25137 KiB |
|       from small pool |   3788 KiB |   3788 KiB |   3789 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  69632 KiB |  69632 KiB |  69632 KiB |      0 B   |
|       from large pool |  65536 KiB |  65536 KiB |  65536 KiB |      0 B   |
|       from small pool |   4096 KiB |   4096 KiB |   4096 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  20820 KiB |  23428 KiB |  29425 KiB |   8604 KiB |
|       from large pool |  20517 KiB |  21877 KiB |  25957 KiB |   5440 KiB |
|       from small pool |    303 KiB |   1882 KiB |   3468 KiB |   3164 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       8    |       4    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:13:51.526254 [1] proc begin: <DistEnv 1/4 nccl>
16:13:51.685533 [1] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 1, |V|: 42336, |E|: 336780>
16:13:51.700890 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  26283 KiB |  44155 KiB |  51536 KiB |  25252 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47739 KiB |  25248 KiB |
|       from small pool |   3792 KiB |   3794 KiB |   3797 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         |  26283 KiB |  44155 KiB |  51536 KiB |  25252 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47739 KiB |  25248 KiB |
|       from small pool |   3792 KiB |   3794 KiB |   3797 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |  26279 KiB |  44155 KiB |  51417 KiB |  25137 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47628 KiB |  25137 KiB |
|       from small pool |   3788 KiB |   3788 KiB |   3789 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  69632 KiB |  69632 KiB |  69632 KiB |      0 B   |
|       from large pool |  65536 KiB |  65536 KiB |  65536 KiB |      0 B   |
|       from small pool |   4096 KiB |   4096 KiB |   4096 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  20820 KiB |  23428 KiB |  29425 KiB |   8604 KiB |
|       from large pool |  20517 KiB |  21877 KiB |  25957 KiB |   5440 KiB |
|       from small pool |    303 KiB |   1882 KiB |   3468 KiB |   3164 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       8    |       4    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:34:42.497948 [1] proc begin: <DistEnv 1/4 nccl>
16:34:42.602109 [1] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 1, |V|: 42336, |E|: 336780>
16:34:42.612915 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  26283 KiB |  44155 KiB |  51536 KiB |  25252 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47739 KiB |  25248 KiB |
|       from small pool |   3792 KiB |   3794 KiB |   3797 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         |  26283 KiB |  44155 KiB |  51536 KiB |  25252 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47739 KiB |  25248 KiB |
|       from small pool |   3792 KiB |   3794 KiB |   3797 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |  26279 KiB |  44155 KiB |  51417 KiB |  25137 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47628 KiB |  25137 KiB |
|       from small pool |   3788 KiB |   3788 KiB |   3789 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  69632 KiB |  69632 KiB |  69632 KiB |      0 B   |
|       from large pool |  65536 KiB |  65536 KiB |  65536 KiB |      0 B   |
|       from small pool |   4096 KiB |   4096 KiB |   4096 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  20820 KiB |  23428 KiB |  29425 KiB |   8604 KiB |
|       from large pool |  20517 KiB |  21877 KiB |  25957 KiB |   5440 KiB |
|       from small pool |    303 KiB |   1882 KiB |   3468 KiB |   3164 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       8    |       4    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:59:21.950010 [1] proc begin: <DistEnv 1/4 nccl>
15:59:25.443574 [1] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 1, |V|: 42336, |E|: 336780>
15:59:25.456952 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  26283 KiB |  44155 KiB |  51536 KiB |  25252 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47739 KiB |  25248 KiB |
|       from small pool |   3792 KiB |   3794 KiB |   3797 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         |  26283 KiB |  44155 KiB |  51536 KiB |  25252 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47739 KiB |  25248 KiB |
|       from small pool |   3792 KiB |   3794 KiB |   3797 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |  26279 KiB |  44155 KiB |  51417 KiB |  25137 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47628 KiB |  25137 KiB |
|       from small pool |   3788 KiB |   3788 KiB |   3789 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  69632 KiB |  69632 KiB |  69632 KiB |      0 B   |
|       from large pool |  65536 KiB |  65536 KiB |  65536 KiB |      0 B   |
|       from small pool |   4096 KiB |   4096 KiB |   4096 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  20820 KiB |  23428 KiB |  29425 KiB |   8604 KiB |
|       from large pool |  20517 KiB |  21877 KiB |  25957 KiB |   5440 KiB |
|       from small pool |    303 KiB |   1882 KiB |   3468 KiB |   3164 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       8    |       4    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:00:28.110430 [1] proc begin: <DistEnv 1/4 nccl>
16:00:28.367058 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
16:00:28.379276 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:00:29.571387 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.340392 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.348323 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.356387 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.366322 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.373156 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.377544 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.381772 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.388531 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.394397 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.398527 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.403976 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.407962 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.411799 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.416059 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.421586 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.425201 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.428911 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.432565 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.436299 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.440119 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.445053 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.449020 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.452902 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.456786 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.460248 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.463989 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.467792 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.473836 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.479219 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.482876 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.487650 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.491507 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.495197 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.499162 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.503380 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.507579 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.511650 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.517806 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.525967 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.531426 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.536937 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.543820 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.550138 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.554517 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.560348 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.566616 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.573392 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.579849 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.588308 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.595206 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.599944 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.604106 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.608103 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.614126 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.621444 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.627076 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.631068 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.635103 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.640254 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.644626 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.652921 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.658389 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.664117 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.670130 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.676445 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.682048 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.686797 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.691567 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.696402 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.701052 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.708321 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.713415 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.718460 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.723265 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.727549 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.733873 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.738432 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.742551 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.746334 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.751040 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.756371 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.760528 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.766089 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.773270 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.779165 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.783993 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.790466 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.794981 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.799141 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.803334 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.810846 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.818232 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.822479 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.829108 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.834411 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.838595 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.842742 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.846831 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.850589 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.854473 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.861251 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.865556 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.869332 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.873080 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.878851 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.883272 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.887152 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.891364 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.895218 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.900586 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.908054 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.913456 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.917467 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.922151 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.928304 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.935615 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.940108 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.943969 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.948147 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.952613 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.957871 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.961764 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.965806 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.969783 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.973984 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.978144 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.982006 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.985786 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.989682 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.993876 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.999303 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.003248 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.007201 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.011138 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.018256 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.022684 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.027207 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.031325 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.035656 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.039575 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.044322 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.048638 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.052673 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.056527 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.060240 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.064156 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.069440 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.075303 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.079241 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.083276 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.088766 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.094698 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.099254 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.104673 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.109825 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.123064 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.129205 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.134423 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.138794 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.151628 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.159900 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.164212 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.168566 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.175315 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.179672 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.183979 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.187903 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.191706 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.195574 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.199404 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.205439 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.211353 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.217766 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.223842 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.228540 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.232913 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.236908 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.240621 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.244391 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.248278 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.253118 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.257248 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.261141 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.265304 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.269359 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.273419 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.277597 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.281689 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.285547 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.289610 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.294824 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.298827 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.304629 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.311604 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.315866 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.319883 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.323917 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.327684 [1] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.331582 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:23.621005 [1] proc begin: <DistEnv 1/4 nccl>
16:05:23.670039 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
16:05:23.679242 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:05:24.919872 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.608842 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.615893 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.622825 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.628810 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.633018 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.636982 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.642632 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.647019 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.650751 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.654479 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.661055 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.666883 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.671412 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.676938 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.681097 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.684959 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.688951 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.692602 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.696084 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.700803 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.707556 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.712862 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.716652 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.720659 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.726791 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.733110 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.737739 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.744296 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.748352 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.752077 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.757138 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.761048 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.764616 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.768574 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.772771 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.778436 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.784040 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.788386 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.792324 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.795982 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.800704 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.805600 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.810897 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.815303 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.819988 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.824164 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.828163 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.833584 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.839039 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.844080 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.849057 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.854747 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.863022 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.868066 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.872160 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.877370 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.883349 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.888104 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.892245 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.896425 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.901519 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.905388 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.909472 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.913412 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.917312 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.921182 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.925231 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.929454 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.933785 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.937625 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.942339 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.946155 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.950102 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.953928 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.959669 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.965751 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.970855 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.977589 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.983519 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.987713 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.992666 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.996578 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.000529 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.006053 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.012862 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.019979 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.026126 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.032014 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.037143 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.041885 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.046683 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.052143 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.057893 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.062474 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.066536 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.070589 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.074383 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.078390 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.082251 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.087189 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.094141 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.098262 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.102168 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.105786 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.111633 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.115962 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.121022 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.125875 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.129926 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.133716 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.144114 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.148789 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.152724 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.156870 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.160846 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.164319 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.168307 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.173659 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.179678 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.186345 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.192464 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.196186 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.200182 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.204123 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.208064 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.211895 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.215709 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.219743 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.223510 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.227472 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.232257 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.236081 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.239791 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.243577 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.247490 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.251303 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.255374 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.259455 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.263237 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.266853 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.271747 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.275759 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.279627 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.283698 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.287264 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.290810 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.294692 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.298923 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.303327 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.306906 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.311352 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.315367 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.319259 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.322915 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.327994 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.333167 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.336719 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.340265 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.344102 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.348036 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.352745 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.356331 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.360076 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.364992 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.368727 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.374578 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.380730 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.386616 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.390596 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.394348 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.399223 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.403093 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.406883 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.412734 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.416556 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.420570 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.424390 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.428385 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.432145 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.438181 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.446149 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.450286 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.454174 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.458009 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.461770 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.465325 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.469080 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.472750 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.478582 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.484142 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.490484 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.494895 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.498983 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.503195 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.508930 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.514594 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.521446 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.526952 [1] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.531005 [1] Warning: no training nodes in this partition! Backward fake loss.
16:11:45.420424 [1] proc begin: <DistEnv 1/4 nccl>
16:11:55.585546 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:11:55.602075 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:46:58.025504 [1] proc begin: <DistEnv 1/4 nccl>
16:46:58.055250 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
16:46:58.065323 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:46:59.282447 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.949677 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.958170 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.963480 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.967459 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.973380 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.978331 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.982286 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.986111 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.990269 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.994504 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.999974 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.004415 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.009025 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.013300 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.017760 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.025493 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.030231 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.036269 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.040790 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.044587 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.049875 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.054124 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.059560 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.065304 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.069645 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.073557 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.077364 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.081346 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.085224 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.089094 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.093739 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.097872 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.101696 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.107523 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.111488 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.115451 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.120714 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.126156 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.133299 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.137951 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.143393 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.147421 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.151031 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.155325 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.159498 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.164348 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.169219 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.175662 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.181409 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.187186 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.192297 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.196425 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.200677 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.204875 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.208515 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.212288 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.216552 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.220616 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.226320 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.231990 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.238969 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.244725 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.250724 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.255753 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.259609 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.265288 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.270650 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.275596 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.279576 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.283486 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.289999 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.295960 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.302725 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.307181 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.311053 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.315130 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.319289 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.323447 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.329375 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.333593 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.338726 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.342671 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.349131 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.353957 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.357832 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.363591 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.367309 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.372914 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.378020 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.382343 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.388568 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.392615 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.396481 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.402088 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.406319 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.410139 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.414031 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.417626 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.421388 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.427408 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.435932 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.441188 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.446354 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.450768 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.454694 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.458427 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.463831 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.470057 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.474756 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.478827 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.483981 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.487715 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.491357 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.494912 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.498952 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.503067 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.506962 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.510869 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.514451 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.518688 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.523686 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.527543 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.531348 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.535130 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.539028 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.542746 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.546574 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.550230 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.553929 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.557814 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.563008 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.567235 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.571113 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.575134 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.579217 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.584915 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.591295 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.595182 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.599190 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.602852 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.607812 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.613627 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.619632 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.624045 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.628189 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.631725 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.635536 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.639440 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.643170 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.646869 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.651469 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.655310 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.658769 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.662215 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.666047 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.669824 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.673937 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.677614 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.681228 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.684882 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.689616 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.696118 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.700311 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.704148 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.708085 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.711600 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.715347 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.719204 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.723001 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.726791 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.731544 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.735277 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.738975 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.742644 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.746134 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.749876 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.753718 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.759320 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.764314 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.768409 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.773969 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.778182 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.782437 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.788613 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.793095 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.797418 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.801440 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.805693 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.810086 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.814181 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.819310 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.823047 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.826769 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.830744 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.834476 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.838351 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.842331 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.846453 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.850343 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:46.836770 [1] proc begin: <DistEnv 1/4 nccl>
16:47:46.915639 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
16:47:46.926227 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:47:48.108795 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.734185 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.741922 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.748248 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.753030 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.757998 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.762472 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.766690 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.774487 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.779080 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.784205 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.792304 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.797992 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.802016 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.806061 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.810248 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.814211 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.818190 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.822205 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.826175 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.830463 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.835764 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.840513 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.844637 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.848683 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.855155 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.862741 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.866724 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.871073 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.876898 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.882720 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.890642 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.899207 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.905149 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.910103 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.913924 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.917923 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.921887 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.925393 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.929209 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.933260 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.939744 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.945562 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.950207 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.954734 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.959513 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.965388 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.969332 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.976038 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.982849 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.986715 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.992006 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.996172 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.002170 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.006171 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.009993 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.014814 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.020401 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.027236 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.031952 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.036408 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.042136 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.045832 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.049477 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.053347 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.057600 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.062548 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.066410 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.070404 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.083776 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.090845 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.099471 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.103708 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.109232 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.114698 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.118834 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.123130 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.127509 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.131664 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.135719 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.141467 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.148687 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.153299 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.157529 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.161712 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.165621 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.171561 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.177153 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.180929 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.184532 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.189119 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.197514 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.203353 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.208596 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.213302 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.218263 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.222365 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.226448 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.230236 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.234313 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.238455 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.245553 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.251067 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.255176 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.258942 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.262901 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.266939 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.270804 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.274838 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.278595 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.282295 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.288619 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.293428 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.297260 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.301231 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.305106 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.308940 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.312768 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.318220 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.322301 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.326464 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.331829 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.337224 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.342682 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.349313 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.356080 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.375689 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.381455 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.386144 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.390741 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.395896 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.402498 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.408815 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.413337 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.418143 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.429684 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.438052 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.443606 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.455224 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.462413 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.467183 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.476342 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.482509 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.487086 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.498785 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.536463 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.558310 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.563282 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.568487 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.576800 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.581434 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.588080 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.593006 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.597705 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.602314 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.607930 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.613122 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.617677 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.622375 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.627886 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.632293 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.638317 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.643086 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.649170 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.653835 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.658397 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.662778 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.667386 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.671948 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.676157 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.680315 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.685884 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.690644 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.695269 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.699861 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.703530 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.707809 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.711498 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.715450 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.719186 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.723922 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.729483 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.733872 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.738091 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.742037 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.746060 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.751876 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.756504 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.760895 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.765130 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.773337 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.779619 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.784343 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.789132 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.792863 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.796624 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.800649 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.808939 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.815299 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.821872 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:24.306887 [1] proc begin: <DistEnv 1/4 nccl>
16:49:24.337162 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
16:49:24.347094 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:49:26.321283 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.164502 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.173786 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.180230 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.184706 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.188802 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.192889 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.196718 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.200448 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.204372 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.208273 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.213211 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.217406 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.221891 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.227511 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.234122 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.238799 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.243176 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.246952 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.252898 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.257289 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.262330 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.266027 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.269681 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.274238 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.280671 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.286393 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.290569 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.294766 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.298421 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.302469 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.309649 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.316967 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.323471 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.328669 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.334373 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.339122 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.345076 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.350876 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.354774 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.360409 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.366071 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.370425 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.377314 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.381924 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.386181 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.390699 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.396734 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.400680 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.404544 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.408398 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.413828 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.418052 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.423570 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.427998 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.432519 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.437450 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.441825 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.446714 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.452606 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.457228 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.462546 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.466452 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.470785 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.477275 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.481722 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.485319 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.489595 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.493760 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.497852 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.501700 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.508220 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.513575 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.517145 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.520567 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.525743 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.529943 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.534055 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.538012 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.541729 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.545346 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.552830 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.558899 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.564276 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.568788 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.573228 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.577078 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.583179 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.589328 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.593657 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.598312 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.603658 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.607963 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.611918 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.615780 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.621026 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.626129 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.630107 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.634298 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.639848 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.645218 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.650103 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.657702 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.663782 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.668080 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.673341 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.678037 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.682041 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.687861 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.692375 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.696799 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.703117 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.709000 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.714999 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.720476 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.725279 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.729955 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.734859 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.739594 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.744511 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.749453 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.755661 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.760578 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.765466 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.769463 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.775260 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.781861 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.786617 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.790418 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.794043 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.798860 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.805709 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.809829 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.815935 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.822905 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.827697 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.832290 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.836577 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.842259 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.848201 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.852412 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.857357 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.861386 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.865284 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.869343 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.873290 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.876959 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.880563 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.884408 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.888529 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.892257 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.897203 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.901538 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.905574 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.909322 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.912933 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.917382 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.923879 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.932393 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.940933 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.946000 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.953083 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.957430 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.961901 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.966413 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.970491 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.975270 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.979324 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.983299 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.987513 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.991381 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.996294 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.001012 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.005141 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.009051 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.012960 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.016965 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.020721 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.025241 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.029212 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.033105 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.041062 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.046134 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.050110 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.053794 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.058109 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.064442 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.070887 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.076206 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.080167 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.084017 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.089208 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.093119 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.096799 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.109008 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.123136 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.128407 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.132819 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.137155 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.141119 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:30.378746 [1] proc begin: <DistEnv 1/4 nccl>
16:52:30.535742 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
16:52:30.545645 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:52:31.712444 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.435752 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.443357 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.449606 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.453879 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.459330 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.463792 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.468475 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.473018 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.478818 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.483543 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.489092 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.493171 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.497402 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.501597 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.505630 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.509571 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.513590 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.517373 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.522628 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.527660 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.533018 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.536925 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.541099 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.544881 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.548612 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.552121 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.555671 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.559454 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.563569 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.567396 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.572656 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.576574 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.580618 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.584829 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.588798 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.593316 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.597540 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.601390 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.605371 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.609422 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.614575 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.620147 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.624357 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.628281 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.634021 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.640108 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.644993 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.649394 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.653041 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.657234 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.662855 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.667125 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.670835 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.674568 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.678432 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.682340 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.686245 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.690417 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.694202 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.699783 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.706863 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.711449 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.715600 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.719805 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.725772 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.729825 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.733632 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.737223 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.740836 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.744685 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.749973 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.754064 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.761511 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.767046 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.771263 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.775658 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.779394 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.783296 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.787478 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.793184 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.800227 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.805203 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.809687 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.813609 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.817388 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.821315 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.825114 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.829983 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.833797 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.837705 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.843409 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.847505 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.851582 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.855641 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.859800 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.863984 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.868280 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.872497 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.876660 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.881022 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.886682 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.890905 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.894896 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.899172 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.903389 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.907644 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.911527 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.915212 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.918898 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.922882 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.928462 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.932596 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.936414 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.940174 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.944260 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.948115 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.952077 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.955851 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.959489 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.963105 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.968542 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.973356 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.977280 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.981219 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.986710 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.992899 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.997211 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.001424 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.005415 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.009217 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.014630 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.018703 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.022344 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.027910 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.031886 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.035822 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.039680 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.043628 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.047448 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.051105 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.056310 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.062303 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.067602 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.071792 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.075911 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.080009 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.083911 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.087829 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.091681 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.097218 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.102821 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.107109 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.110961 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.114843 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.118974 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.122672 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.129627 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.136250 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.141958 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.146953 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.151912 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.158257 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.163270 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.167499 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.172093 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.176127 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.180232 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.184212 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.189420 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.194746 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.200656 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.204539 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.208557 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.212865 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.217054 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.221097 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.226640 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.231863 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.236179 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.241424 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.246593 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.250619 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.256752 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.265648 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.269860 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.274105 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.281918 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.287437 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.291956 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.297817 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.303869 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.307567 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.311717 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.315739 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.319771 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.323745 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.327405 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.331097 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.334948 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:42.185126 [1] proc begin: <DistEnv 1/4 nccl>
16:52:42.239082 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
16:52:42.249036 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:52:44.424782 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.184260 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.190783 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.195891 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.200876 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.207541 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.211647 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.215697 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.219875 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.223709 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.227630 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.232901 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.239329 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.247278 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.253808 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.258551 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.263606 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.268655 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.273810 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.283311 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.290671 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.299135 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.304087 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.309248 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.314409 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.319135 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.323883 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.328930 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.334827 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.340729 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.347830 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.356966 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.363536 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.369772 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.376266 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.380977 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.385284 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.389092 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.393185 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.397254 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.401257 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.406802 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.411117 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.415206 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.419041 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.423083 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.426893 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.430921 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.434829 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.439157 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.443932 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.449348 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.455082 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.459231 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.462998 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.468972 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.475539 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.481198 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.485345 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.489288 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.493627 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.499277 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.503171 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.507619 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.513264 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.519898 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.525872 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.530774 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.536026 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.541109 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.546229 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.552601 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.557353 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.562283 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.567011 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.574166 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.579902 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.585176 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.590119 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.595101 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.601588 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.612257 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.617366 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.621968 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.626533 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.631802 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.635698 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.639888 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.644245 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.648450 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.652429 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.657415 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.661527 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.665365 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.669245 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.673253 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.677377 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.681268 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.685013 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.688863 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.692583 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.698745 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.703035 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.707152 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.711371 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.719493 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.724826 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.731232 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.735434 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.741929 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.746408 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.754405 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.760116 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.763932 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.767853 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.772371 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.776831 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.780949 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.785061 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.789003 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.792692 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.798246 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.802737 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.814434 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.820770 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.824978 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.829342 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.834119 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.840429 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.847398 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.852273 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.857761 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.861844 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.865801 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.869775 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.873577 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.879698 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.886024 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.890962 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.895155 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.901281 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.908917 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.918698 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.923424 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.928963 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.934949 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.939812 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.944264 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.948309 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.953757 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.957923 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.964471 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.968324 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.972710 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.977145 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.981613 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.985861 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.989845 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.994179 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.998330 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.002224 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.007650 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.012182 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.016780 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.020783 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.024523 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.028854 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.032834 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.036656 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.040771 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.044679 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.049970 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.054109 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.059473 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.065958 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.072009 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.075930 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.079815 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.084179 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.088548 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.093219 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.106063 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.115642 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.122316 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.126961 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.131722 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.135889 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.140002 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.143905 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.148060 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.152226 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.165708 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.173676 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.179904 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.190292 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.195496 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.199942 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.205595 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.211454 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.215501 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:29.572106 [1] proc begin: <DistEnv 1/4 nccl>
16:53:29.600504 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
16:53:29.610680 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:53:30.822894 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.545740 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.553944 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.560059 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.565906 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.572121 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.577964 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.584007 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.590576 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.597035 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.603250 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.610173 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.616315 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.622411 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.634080 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.642861 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.650197 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.655782 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.661454 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.668355 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.674089 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.683697 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.690373 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.696118 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.705006 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.710776 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.716572 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.723603 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.729222 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.736004 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.742529 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.748883 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.754721 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.760158 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.765404 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.771003 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.776077 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.781122 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.786198 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.792864 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.798118 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.803836 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.809126 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.817474 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.823258 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.828874 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.834752 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.841987 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.847735 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.853182 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.858715 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.863997 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.869459 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.874777 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.879794 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.884782 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.890563 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.896087 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.901989 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.907854 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.913454 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.918872 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.924300 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.930344 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.937440 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.942840 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.948225 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.954966 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.960509 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.965883 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.972889 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.979807 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.985776 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.991438 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.998363 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.004018 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.009519 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.015198 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.020575 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.025968 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.031380 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.036811 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.042492 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.048022 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.054785 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.061513 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.066996 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.072714 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.078505 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.083734 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.088831 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.093969 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.104444 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.110926 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.116384 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.121877 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.127056 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.135172 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.141217 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.146577 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.151944 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.157381 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.162619 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.167950 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.176699 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.183443 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.190411 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.195655 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.204602 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.213367 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.221422 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.227606 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.233515 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.239292 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.245138 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.251239 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.260158 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.266114 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.272042 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.280120 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.286677 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.293259 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.299613 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.308395 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.313886 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.319633 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.325117 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.330441 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.336039 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.343853 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.349631 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.355619 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.361271 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.366862 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.374145 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.381192 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.386779 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.395784 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.404395 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.412323 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.419982 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.425569 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.431004 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.436787 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.442474 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.449581 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.455801 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.461796 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.467226 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.473098 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.478850 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.484595 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.490568 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.496118 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.502511 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.508599 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.518932 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.525894 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.532491 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.537452 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.542785 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.548163 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.553129 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.558117 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.563182 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.568258 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.580753 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.589100 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.597086 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.603773 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.630861 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.638370 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.644904 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.649910 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.655551 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.660901 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.665922 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.671006 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.676076 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.681035 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.686067 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.691149 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.697045 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.706652 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.713069 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.719154 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.724884 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.730675 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.737219 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.743786 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.749693 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.755707 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.761038 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.766558 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.775632 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.781638 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.787165 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.792830 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.798041 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.806640 [1] Warning: no training nodes in this partition! Backward fake loss.
16:55:22.512489 [1] proc begin: <DistEnv 1/4 nccl>
16:55:29.002166 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:55:29.025607 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:17:57.700909 [1] proc begin: <DistEnv 1/4 nccl>
19:17:57.944148 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
19:17:57.964942 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:17:59.381031 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.193744 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.202813 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.210216 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.216012 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.222388 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.228716 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.234059 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.239875 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.245782 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.251275 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.256774 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.262585 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.268223 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.273372 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.278908 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.288304 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.294273 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.301323 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.306889 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.312707 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.318329 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.325062 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.331871 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.338122 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.343648 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.349712 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.356524 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.362970 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.368641 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.376841 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.383085 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.389321 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.396454 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.405237 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.412693 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.420855 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.427771 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.434946 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.442219 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.448338 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.455553 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.461530 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.469590 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.475670 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.481685 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.487599 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.494857 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.502299 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.509431 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.519108 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.525469 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.531425 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.536962 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.543255 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.551352 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.556839 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.562172 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.567942 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.574125 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.579653 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.585261 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.593342 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.598879 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.604248 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.613218 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.619599 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.629093 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.638606 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.646205 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.654423 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.662529 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.672818 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.685028 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.701400 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.717419 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.728030 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.736244 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.743445 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.750449 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.756401 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.767143 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.777865 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.785009 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.794670 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.801684 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.813639 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.820235 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.826309 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.836427 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.844846 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.852420 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.859474 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.866334 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.873429 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.882599 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.890637 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.898220 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.906464 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.914571 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.922557 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.928962 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.935003 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.942090 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.948794 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.955320 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.961923 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.967843 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.973605 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.980337 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.985917 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.993121 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.999249 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.005298 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.012584 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.018973 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.024979 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.030906 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.036966 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.051660 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.058470 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.065127 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.071946 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.077869 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.084462 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.090253 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.097846 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.105323 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.113682 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.119887 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.125689 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.131364 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.137245 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.143550 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.149407 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.155510 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.163080 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.168656 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.174926 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.181130 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.186511 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.194158 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.201445 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.207040 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.212892 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.218437 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.227714 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.233596 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.241794 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.248777 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.255862 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.261588 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.267135 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.272653 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.278087 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.287544 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.293380 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.299542 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.306099 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.311654 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.317188 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.322596 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.328020 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.333201 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.338618 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.343771 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.349262 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.354600 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.364950 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.371832 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.377745 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.383406 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.388897 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.394481 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.400437 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.406588 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.411970 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.421610 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.428618 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.435313 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.441953 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.447657 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.453559 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.459114 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.464827 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.471147 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.476986 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.483068 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.489150 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.496271 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.503303 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.509332 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.514822 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.520866 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.526990 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.533836 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.539306 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.545403 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.556418 [1] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.566003 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:17.050195 [1] proc begin: <DistEnv 1/4 nccl>
19:23:17.078716 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
19:23:17.088451 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:23:19.162314 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.882014 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.890865 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.896415 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.903790 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.909769 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.919325 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.925777 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.932041 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.937046 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.947581 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.953721 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.959447 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.964609 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.970446 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.976268 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.981376 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.990569 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.996445 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.004159 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.009971 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.016676 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.023626 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.030751 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.037340 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.043582 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.050602 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.057879 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.064846 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.070791 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.079988 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.087377 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.093000 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.099169 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.104864 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.110213 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.115795 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.122783 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.127954 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.133131 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.138464 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.143858 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.148991 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.154210 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.159463 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.167778 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.175919 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.182545 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.188482 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.193758 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.199474 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.205795 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.211489 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.217048 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.223058 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.230060 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.235327 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.243241 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.248735 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.253959 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.259771 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.265226 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.270655 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.276026 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.281412 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.286877 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.292440 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.301408 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.308568 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.314293 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.320111 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.325735 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.331318 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.336707 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.341778 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.351512 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.358713 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.366134 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.371977 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.377571 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.384149 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.390001 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.396916 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.403755 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.409339 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.414734 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.421502 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.430813 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.436557 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.442201 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.447970 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.453779 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.459197 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.464895 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.470706 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.476419 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.482061 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.487697 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.492889 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.498190 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.504345 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.509898 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.515169 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.522223 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.529186 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.534707 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.540707 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.548589 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.554116 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.559290 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.564542 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.573264 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.580603 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.587730 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.595356 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.602670 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.609926 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.616302 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.621711 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.630560 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.640786 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.646762 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.653229 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.659202 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.664920 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.670339 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.677649 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.684212 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.690983 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.696648 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.702452 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.707952 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.715584 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.721368 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.726873 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.736049 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.741917 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.747620 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.753124 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.758610 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.768424 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.774921 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.780345 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.785839 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.791153 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.798938 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.808385 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.821251 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.831121 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.836676 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.842035 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.847474 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.852988 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.858811 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.863974 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.869340 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.875835 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.882755 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.887827 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.893286 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.898365 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.904292 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.909759 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.915073 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.920359 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.926463 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.932331 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.937284 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.946758 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.952455 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.958017 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.967213 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.977695 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.987244 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.993255 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.999289 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.017160 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.025601 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.035476 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.043898 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.058605 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.065767 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.071887 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.077309 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.082789 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.089653 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.096644 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.105185 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.111879 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.117436 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.122760 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.128112 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.133678 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.140767 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.149714 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.155575 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.160893 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.166196 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.174565 [1] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.180228 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:45.801770 [1] proc begin: <DistEnv 1/4 nccl>
20:00:45.878784 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
20:00:45.888162 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:00:47.232100 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.952490 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.961357 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.972032 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.977787 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.983886 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.989422 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.995262 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.001008 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.006360 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.012091 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.017985 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.023284 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.028857 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.034837 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.040740 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.046070 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.051564 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.057062 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.062440 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.067912 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.073442 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.082796 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.089596 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.096712 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.103465 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.112010 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.117253 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.122877 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.128532 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.133965 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.139827 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.145581 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.151947 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.157394 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.163171 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.168740 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.174280 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.180527 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.186768 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.192385 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.198572 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.204906 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.210756 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.217245 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.223491 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.229537 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.235283 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.242081 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.248072 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.253574 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.259403 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.266472 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.273453 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.280159 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.285596 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.291324 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.296673 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.302058 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.307620 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.312956 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.318070 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.323103 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.328252 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.333302 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.338759 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.344767 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.351673 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.357128 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.362241 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.367754 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.378182 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.386293 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.392472 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.398597 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.404400 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.414156 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.421911 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.428039 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.433995 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.439522 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.444936 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.450537 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.456001 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.464879 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.471657 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.477957 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.483219 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.488592 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.495273 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.502023 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.507933 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.513766 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.519278 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.525004 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.530608 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.535935 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.541963 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.552083 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.558375 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.565046 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.570811 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.576771 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.582273 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.589697 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.597176 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.604062 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.611294 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.617051 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.622823 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.630508 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.637613 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.642858 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.649496 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.655696 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.661861 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.668961 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.676395 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.682871 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.690177 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.696500 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.703129 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.709086 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.714875 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.720958 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.729135 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.735559 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.741724 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.747985 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.753753 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.759864 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.765289 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.770895 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.778989 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.784619 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.789944 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.795998 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.804733 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.810231 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.820083 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.827265 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.833158 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.838913 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.844551 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.850342 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.858886 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.865177 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.871062 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.876257 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.881733 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.887039 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.895053 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.908581 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.916264 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.923078 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.928703 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.934313 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.943114 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.949689 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.957675 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.963709 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.970926 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.976314 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.981528 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.988292 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.997940 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.006089 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.011814 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.017436 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.026910 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.032971 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.041081 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.049653 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.056650 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.063002 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.068336 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.075292 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.080891 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.086606 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.092161 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.097800 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.103331 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.108863 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.114524 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.119787 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.126202 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.132527 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.144174 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.150577 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.157087 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.163491 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.171273 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.178789 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.185323 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.191530 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.198711 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.205284 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.211937 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.218783 [1] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.225482 [1] Warning: no training nodes in this partition! Backward fake loss.
20:48:25.134046 [1] proc begin: <DistEnv 1/4 nccl>
20:48:40.049441 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
20:48:40.067233 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:26:54.513565 [1] proc begin: <DistEnv 1/4 nccl>
20:27:08.989514 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
20:27:09.017305 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:01:27.797245 [1] proc begin: <DistEnv 1/4 nccl>
17:01:28.055870 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
17:01:28.081169 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:01:29.983835 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.251547 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.333770 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.425993 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.581719 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.713710 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.887381 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.948914 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.058600 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.230064 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.355953 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.491138 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.570578 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.743522 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.856669 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.027780 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.087337 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.223853 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.377181 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.493663 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.633601 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.716498 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.886844 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.962209 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.110598 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.177619 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.252236 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.326905 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.375030 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.382492 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.389325 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.397292 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.405622 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.418599 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.428447 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.435486 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.442653 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.449116 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.460002 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.487032 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.501596 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.509236 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.517253 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.525086 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.533180 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.545933 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.558772 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.572988 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.584960 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.593765 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.604261 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.617850 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.637304 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.688877 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.911736 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:35.113460 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:35.272521 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:35.488569 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:35.652279 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:35.865653 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:36.004219 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:36.223835 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:36.434336 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:36.677491 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:36.838602 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:37.102830 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:37.308062 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:37.497809 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:37.656993 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:37.889701 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.057654 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.234312 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.435006 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.604726 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.750222 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.947187 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:39.141316 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:39.420625 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:39.625994 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:39.845179 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:39.982886 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:40.232416 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:40.420152 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:40.677236 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:40.855936 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.076022 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.222687 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.396356 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.521527 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.752049 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.918106 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.085294 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.233290 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.414097 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.571217 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.729725 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.911085 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.122334 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.233051 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.416371 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.564363 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.740376 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.897239 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.030478 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.177848 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.362131 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.542610 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.601730 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.733553 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.901438 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.021177 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.152994 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.236127 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.426352 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.575475 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.727627 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.802899 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.977204 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.087713 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.259020 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.314318 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.489953 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.620851 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.810931 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.865156 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.032106 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.175096 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.260062 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.396758 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.460237 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.542958 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.620594 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.693856 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.871284 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.942052 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.119795 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.202283 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.346453 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.505669 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.623281 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.768532 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.840398 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.024815 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.154781 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.308074 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.367290 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.444105 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.520777 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.600125 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.698600 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.785777 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.880785 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.040088 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.181325 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.361041 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.424468 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.586961 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.753259 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.927830 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.005397 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.104179 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.261937 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.394455 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.543219 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.598181 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.780254 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.900499 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.911663 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.991850 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.073380 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.090034 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.097849 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.108351 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.116849 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.135342 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.150614 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.165677 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.207292 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.335570 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.343613 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.452950 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.615364 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.802109 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.953893 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.145184 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.267908 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.489557 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.608062 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.757361 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.806733 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.972622 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.126970 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.251541 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.371204 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.480090 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.653474 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.794965 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.940835 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:55.022704 [1] Warning: no training nodes in this partition! Backward fake loss.
17:01:55.151391 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:41.879330 [1] proc begin: <DistEnv 1/4 nccl>
17:16:41.997256 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
17:16:42.029366 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:16:43.553648 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.637232 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.738084 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.758488 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.776468 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.798215 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.814584 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.828667 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.840914 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.853479 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.863536 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.916677 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:45.187449 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:45.394561 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:45.643613 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:45.888225 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.082425 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.321765 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.554403 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.788477 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.993250 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:47.186861 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:47.386599 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:47.508878 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:47.733097 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:47.933777 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.134910 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.358325 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.495217 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.769818 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.966925 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:49.185251 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:49.339318 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:49.528545 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:49.669146 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:49.918861 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.112157 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.352424 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.537620 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.607903 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.625464 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.649361 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.697228 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.884071 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:51.170112 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:51.423172 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:51.700920 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:51.930907 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:52.230049 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:52.525364 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:52.775566 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:53.119810 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:53.406405 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:53.644395 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:53.966126 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:54.163943 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:54.466374 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:54.711057 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:54.973575 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:55.249934 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:55.583861 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:55.835025 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:56.085029 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:56.382029 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:56.571578 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:56.761002 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:56.920307 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.094268 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.245778 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.451587 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.679262 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.835087 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:58.033322 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:58.238944 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:58.391776 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:58.634856 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:58.851228 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:58.996479 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.231216 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.434394 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.594788 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.790988 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.990404 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:00.119073 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:00.346835 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:00.549596 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:00.702312 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:00.930104 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.155436 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.267242 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.469999 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.642500 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.833238 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.011590 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.243865 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.443576 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.543393 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.648950 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.816554 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.097459 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.258415 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.345679 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.364536 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.386239 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.413350 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.436507 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.460958 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.621413 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.656271 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.778558 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.815764 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.836624 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.859191 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.875233 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.891241 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.914448 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.962400 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:04.155608 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:04.433576 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:04.640996 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:04.864236 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.074465 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.291252 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.383009 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.613631 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.799958 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.972311 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.109607 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.370152 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.544694 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.733341 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.961650 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.177586 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.197475 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.214213 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.242750 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.266263 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.403595 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.662203 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.878366 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.113163 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.320378 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.532477 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.783654 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.990121 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.112271 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.214854 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.292880 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.369441 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.486393 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.683672 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.915026 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.126157 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.338571 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.565436 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.746595 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.901733 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:11.129530 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:11.338780 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:11.499789 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:11.679353 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:11.929172 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.010391 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.158722 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.386465 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.601677 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.650510 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.681197 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.728614 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.848165 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.878909 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.907000 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:13.133917 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:13.407960 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:13.682470 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:13.870301 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:14.185388 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:14.524901 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:14.789662 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:15.095724 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:15.399643 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:15.624842 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:15.879027 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:16.151366 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:16.432911 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:16.630616 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:16.873188 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:17.146414 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:17.406832 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:17.691947 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:17.937918 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:18.158828 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:18.350606 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:18.575348 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:18.803245 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.010290 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.190793 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.432856 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.559371 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.580735 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:46.959916 [1] proc begin: <DistEnv 1/4 nccl>
22:21:47.480596 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
22:21:47.504241 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:21:49.147237 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.049216 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.063379 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.072673 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.082441 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.091923 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.104700 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.113989 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.123312 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.134385 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.143899 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.162819 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.173841 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.185542 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.195225 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.205295 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.218239 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.232357 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.244822 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.253910 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.264155 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.274485 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.285540 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.299652 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.313747 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.324672 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.334931 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.344242 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.353808 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.363079 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.372598 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.382057 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.391276 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.400888 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.414645 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.432439 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.446479 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.455636 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.470421 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.482747 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.492321 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.502202 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.511732 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.520892 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.530259 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.539720 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.549471 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.566527 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.577708 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.588456 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.601871 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.612803 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.624718 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.636302 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.648723 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.658812 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.669226 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.680059 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.690202 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.700763 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.711153 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.722353 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.732927 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.743296 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.754185 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.764547 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.774928 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.785283 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.795765 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.806272 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.816512 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.828732 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.844302 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.854295 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.866838 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.877801 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.889006 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.899971 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.911207 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.921438 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.933720 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.945451 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.956166 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.972259 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.989058 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.009449 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.023078 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.032472 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.041796 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.058263 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.070752 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.079834 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.089429 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.098885 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.108375 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.121662 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.130971 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.144408 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.164966 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.178754 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.187788 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.197918 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.207386 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.216933 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.228433 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.237378 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.246688 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.258265 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.267369 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.276747 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.286349 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.296244 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.306763 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.316119 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.325823 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.334721 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.345795 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.357818 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.368616 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.378052 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.387336 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.398001 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.408923 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.418145 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.427428 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.436481 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.446353 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.455741 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.465265 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.474506 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.487843 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.498466 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.509559 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.519699 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.528736 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.538000 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.546917 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.558646 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.570183 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.579214 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.588499 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.598168 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.607377 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.616527 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.627879 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.637913 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.647203 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.656748 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.665965 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.675476 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.684830 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.694177 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.703842 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.712978 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.722195 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.731465 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.740643 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.750193 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.759420 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.768734 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.778035 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.787290 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.796560 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.805926 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.815353 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.824417 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.833777 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.843047 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.852185 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.861458 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.870705 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.880099 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.889834 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.899463 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.908542 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.917806 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.927109 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.936408 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.945820 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.955574 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.965213 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.995446 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.014114 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.027544 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.036928 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.048058 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.060273 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.070382 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.080475 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.090898 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.100897 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.111923 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.122340 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.132305 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.144260 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.154676 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.164463 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.175050 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.184896 [1] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.194903 [1] Warning: no training nodes in this partition! Backward fake loss.
22:22:57.560953 [1] proc begin: <DistEnv 1/4 nccl>
22:22:57.660695 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
22:22:57.673662 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:22:59.112787 [1] Warning: no training nodes in this partition! Backward fake loss.
22:22:59.966823 [1] Warning: no training nodes in this partition! Backward fake loss.
22:22:59.982814 [1] Warning: no training nodes in this partition! Backward fake loss.
22:22:59.996615 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.007901 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.017942 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.027592 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.036852 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.049818 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.061788 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.070844 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.080168 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.089749 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.102025 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.111472 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.120708 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.130112 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.139255 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.148847 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.158566 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.168252 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.180071 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.190816 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.200499 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.209920 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.219329 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.229793 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.239184 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.248407 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.257850 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.267269 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.276678 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.288300 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.301108 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.311101 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.320423 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.329953 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.339283 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.348477 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.357810 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.367752 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.377228 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.389054 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.402240 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.416804 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.428299 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.441930 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.453929 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.467123 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.477736 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.490788 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.503301 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.513089 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.525812 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.539573 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.550826 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.568442 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.580757 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.596956 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.609247 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.621400 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.637920 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.651135 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.660097 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.673897 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.684333 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.693093 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.702095 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.711994 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.721051 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.731031 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.739933 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.748981 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.759120 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.768545 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.777852 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.786637 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.796403 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.808723 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.818406 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.827430 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.836363 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.845463 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.854416 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.863309 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.872165 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.883848 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.894604 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.903273 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.919181 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.929268 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.938954 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.947929 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.957045 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.966183 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.976309 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.988325 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.998561 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.007442 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.016478 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.025544 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.034609 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.043943 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.053043 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.061877 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.070983 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.080123 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.089125 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.098954 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.108799 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.117944 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.126887 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.135851 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.145109 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.154133 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.164478 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.174043 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.183521 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.192405 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.201894 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.210719 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.219658 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.228844 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.239352 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.251279 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.263097 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.272430 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.282166 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.291364 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.301063 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.312223 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.321323 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.330911 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.340184 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.349750 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.359036 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.368796 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.378013 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.388308 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.399854 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.411393 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.422114 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.437465 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.446954 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.456367 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.465876 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.475183 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.484663 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.496824 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.505880 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.515445 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.524849 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.534393 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.543484 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.552958 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.574239 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.586062 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.602572 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.614739 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.624236 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.633969 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.648978 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.660984 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.670177 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.680881 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.700449 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.713153 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.725396 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.734550 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.743758 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.753321 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.763093 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.772554 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.781783 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.794239 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.803435 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.815425 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.824556 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.833989 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.845690 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.854637 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.863732 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.873047 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.882153 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.891186 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.900402 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.909640 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.918735 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.931011 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.943029 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.952133 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.961704 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.974984 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.985861 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.996831 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:02.007693 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:02.017552 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:02.027555 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:02.036717 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:02.046449 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:49.973739 [1] proc begin: <DistEnv 1/4 nccl>
22:23:50.076516 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
22:23:50.088597 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:23:51.500855 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.223981 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.237678 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.247325 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.256690 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.269786 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.282791 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.291991 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.301413 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.310888 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.322366 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.331510 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.340482 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.349563 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.358807 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.374185 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.384263 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.393046 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.402027 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.412765 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.421910 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.432498 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.443417 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.452695 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.461759 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.470829 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.479785 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.488950 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.500535 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.509329 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.526663 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.536213 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.545317 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.555634 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.566849 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.575891 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.585074 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.594010 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.602819 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.611773 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.620964 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.633331 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.644018 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.654038 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.664362 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.673557 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.682692 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.691637 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.704222 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.715170 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.728666 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.737839 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.747372 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.756767 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.767012 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.776487 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.791539 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.802377 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.811069 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.820462 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.829795 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.838735 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.847880 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.857133 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.871124 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.885395 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.895348 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.904488 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.915021 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.924283 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.938838 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.948379 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.957812 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.978226 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.995905 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.007439 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.017712 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.029166 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.039125 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.048214 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.057484 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.066740 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.075982 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.085103 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.094616 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.104255 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.115960 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.127295 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.141185 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.156178 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.168735 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.180003 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.192367 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.202578 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.211996 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.221080 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.230178 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.239260 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.250899 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.260298 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.269744 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.278779 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.287950 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.297200 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.306258 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.315254 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.324359 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.333681 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.345881 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.355398 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.368363 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.380483 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.389858 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.399044 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.411807 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.420539 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.432911 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.444179 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.453249 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.463146 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.477924 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.489407 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.502206 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.516454 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.526389 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.535629 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.545076 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.555098 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.564499 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.576192 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.585758 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.595234 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.604443 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.613822 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.623000 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.632318 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.641796 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.651057 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.660156 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.669842 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.680129 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.693847 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.706564 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.715624 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.725326 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.734815 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.744213 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.756339 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.769523 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.779497 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.789221 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.798835 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.808175 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.817987 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.829293 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.838553 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.850603 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.860172 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.872785 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.882342 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.893062 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.905736 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.915209 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.924676 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.934334 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.947255 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.956293 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.966972 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.992582 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.001831 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.011569 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.020776 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.030031 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.040019 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.050097 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.060568 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.071078 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.080804 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.092071 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.102364 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.113020 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.123694 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.134183 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.144029 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.152799 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.162956 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.171629 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.180793 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.190118 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.199093 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.209917 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.220024 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.232793 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.241790 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.251369 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.262944 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.275337 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.288346 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.296976 [1] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.306177 [1] Warning: no training nodes in this partition! Backward fake loss.
22:24:49.254055 [1] proc begin: <DistEnv 1/4 nccl>
22:25:03.018446 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
22:25:03.035124 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:26:06.055262 [1] proc begin: <DistEnv 1/4 nccl>
22:26:12.512064 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
22:26:12.528382 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:27:25.117070 [1] proc begin: <DistEnv 1/4 nccl>
22:27:31.921991 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
22:27:31.938628 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:28:35.120449 [1] proc begin: <DistEnv 1/4 nccl>
22:28:41.860647 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
22:28:41.876564 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:33:09.650727 [1] proc begin: <DistEnv 1/4 nccl>
22:33:28.605488 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
22:33:28.617050 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:33:36.330974 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:37.354076 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:37.535918 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:37.717103 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:37.897935 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.079193 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.259547 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.440414 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.620067 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.799991 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.980586 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:39.160273 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:39.339589 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:39.522684 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:39.702897 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:39.882877 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.062571 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.242962 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.423517 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.603455 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.784962 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.965063 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:41.145330 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:41.330244 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:41.511511 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:41.692528 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:41.872219 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.052818 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.232610 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.415196 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.595203 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.776041 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.956701 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:43.137611 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:43.318462 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:43.500596 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:43.684377 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:43.865699 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.046324 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.226892 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.407212 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.587990 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.780152 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.965979 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:45.146313 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:45.327091 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:45.507573 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:45.687772 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:45.868281 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.049987 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.230259 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.410800 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.592448 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.774553 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.956578 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:47.136860 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:47.316950 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:47.496719 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:47.678403 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:47.858327 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.039202 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.219715 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.399388 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.579663 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.760848 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.941801 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:49.121592 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:49.302163 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:49.483090 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:49.663259 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:49.844118 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.025786 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.206950 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.387799 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.570009 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.750118 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.930233 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:51.111513 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:51.292638 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:51.473264 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:51.653735 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:51.834512 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.014237 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.195636 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.375491 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.556691 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.737496 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.918368 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:53.099743 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:53.281258 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:53.461250 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:53.642066 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:53.822700 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.003253 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.183108 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.363706 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.544168 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.723913 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.903774 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.084606 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.265013 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.446931 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.627749 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.807857 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.988281 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:56.169101 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:56.349615 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:56.529740 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:56.710181 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:56.891872 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.072572 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.252636 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.433360 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.613443 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.793269 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.973804 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:58.154232 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:58.334148 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:58.515501 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:58.695931 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:58.875583 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.055646 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.235191 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.416144 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.596151 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.776719 [1] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.956925 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:00.137096 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:00.316588 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:00.497439 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:00.677334 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:00.856962 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.036846 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.216913 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.398732 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.579186 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.760241 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.950921 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:02.139322 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:02.328838 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:02.517642 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:02.705537 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:02.893489 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.082342 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.268701 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.450237 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.630852 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.811081 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.991950 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:04.172752 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:04.353607 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:04.534536 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:04.715415 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:04.897807 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.079411 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.260087 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.441664 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.622338 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.802817 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.983959 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:06.164459 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:06.347386 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:06.528453 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:06.709833 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:06.890986 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.071869 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.253119 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.434437 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.615673 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.797115 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.978440 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:08.159789 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:08.341233 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:08.522501 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:08.704866 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:08.887060 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.068327 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.250242 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.431636 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.613580 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.794337 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.975066 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:10.156227 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:10.337618 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:10.518171 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:10.698793 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:10.879181 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.060815 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.241916 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.422913 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.604197 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.785435 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.966241 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:12.148353 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:12.329380 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:12.509970 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:12.689963 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:12.870302 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:13.050014 [1] Warning: no training nodes in this partition! Backward fake loss.
22:34:13.230216 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:03.156610 [1] proc begin: <DistEnv 1/4 nccl>
22:35:08.703756 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
22:35:08.716617 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:35:13.869093 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:15.433660 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:16.202409 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:16.971864 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:17.741384 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:18.510365 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:19.278096 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:20.051708 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:20.823314 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:21.591768 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:22.359174 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:23.127475 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:23.895355 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:24.664306 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:25.436077 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:26.205783 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:26.978226 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:27.748176 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:28.519082 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:29.288917 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:30.058957 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:30.830779 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:31.600814 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:32.373018 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:33.141979 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:33.910995 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:34.678822 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:35.449052 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:36.217107 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:36.986310 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:37.755199 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:38.524600 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:39.293207 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:40.062259 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:40.832704 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:41.603137 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:42.374317 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:43.143817 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:43.912974 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:44.683236 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:45.453467 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:46.225421 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:46.995874 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:47.766241 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:48.535117 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:49.304322 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:50.074102 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:50.843989 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:51.615412 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:52.385069 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:53.155655 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:53.926290 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:54.696223 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:55.466985 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:56.236908 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:57.006795 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:57.776799 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:58.547882 [1] Warning: no training nodes in this partition! Backward fake loss.
22:35:59.317330 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:00.087039 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:00.857662 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:01.627969 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:02.428564 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:03.233056 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:04.003313 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:04.773730 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:05.544363 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:06.315590 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:07.086529 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:07.857094 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:08.627166 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:09.397531 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:10.168990 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:10.940906 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:11.711663 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:12.481585 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:13.251421 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:14.020353 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:14.790617 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:15.562090 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:16.332252 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:17.101652 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:17.872833 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:18.643332 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:19.414542 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:20.185977 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:20.956882 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:21.726646 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:22.494492 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:23.266787 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:24.030868 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:24.798642 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:25.569526 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:26.337491 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:27.106196 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:27.875865 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:28.645362 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:29.413418 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:30.183460 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:30.968669 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:31.740102 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:32.511920 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:33.282663 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:34.051161 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:34.819215 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:35.588224 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:36.357036 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:37.125559 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:37.894808 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:38.664295 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:39.434144 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:40.203997 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:40.973903 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:41.742171 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:42.510833 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:43.278296 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:44.048189 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:44.816680 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:45.585569 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:46.354326 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:47.122418 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:47.891442 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:48.660184 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:49.429100 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:50.196777 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:50.965900 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:51.733406 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:52.500203 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:53.266641 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:54.033820 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:54.802280 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:55.576443 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:56.344254 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:57.110846 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:57.877930 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:58.644870 [1] Warning: no training nodes in this partition! Backward fake loss.
22:36:59.412042 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:00.180404 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:00.950900 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:01.737794 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:02.542780 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:03.327194 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:04.097193 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:04.866430 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:05.635900 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:06.405882 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:07.175246 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:07.946695 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:08.715145 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:09.484500 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:10.253477 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:11.024350 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:11.794093 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:12.564008 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:13.333881 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:14.104121 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:14.873840 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:15.643953 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:16.414297 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:17.183476 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:17.953920 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:18.724655 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:19.494490 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:20.264100 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:21.034365 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:21.803941 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:22.572793 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:23.342101 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:24.109578 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:24.879007 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:25.647696 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:26.415664 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:27.184185 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:27.952513 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:28.720181 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:29.487729 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:30.255501 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:31.022940 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:31.790746 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:32.560097 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:33.327552 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:34.094524 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:34.862192 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:35.630200 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:36.398676 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:37.167199 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:37.934484 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:38.704184 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:39.474418 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:40.242422 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:41.010998 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:41.777962 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:42.546250 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:43.314213 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:44.081711 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:44.849123 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:45.617535 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:46.384418 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:47.153024 [1] Warning: no training nodes in this partition! Backward fake loss.
22:37:47.919330 [1] Warning: no training nodes in this partition! Backward fake loss.
22:38:43.742515 [1] proc begin: <DistEnv 1/4 nccl>
22:38:48.988508 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
22:38:48.997805 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:38:54.357618 [1] Warning: no training nodes in this partition! Backward fake loss.
22:38:55.891859 [1] Warning: no training nodes in this partition! Backward fake loss.
22:38:56.659300 [1] Warning: no training nodes in this partition! Backward fake loss.
22:38:57.426006 [1] Warning: no training nodes in this partition! Backward fake loss.
22:38:58.194892 [1] Warning: no training nodes in this partition! Backward fake loss.
22:38:58.964516 [1] Warning: no training nodes in this partition! Backward fake loss.
22:38:59.732907 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:00.501525 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:01.270344 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:02.068951 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:02.871900 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:03.639987 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:04.406706 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:05.174566 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:05.943622 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:06.713441 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:07.481637 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:08.251925 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:09.022968 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:09.792219 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:10.564057 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:11.334477 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:12.105282 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:12.874738 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:13.645602 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:14.415285 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:15.185424 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:15.954385 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:16.723549 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:17.490534 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:18.258005 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:19.024856 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:19.792509 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:20.561502 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:21.329186 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:22.096610 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:22.863867 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:23.631489 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:24.399370 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:25.168062 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:25.937893 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:26.707939 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:27.477071 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:28.245489 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:29.015893 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:29.784933 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:30.555355 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:31.324375 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:32.093785 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:32.863579 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:33.634331 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:34.404059 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:35.174585 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:35.945798 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:36.715152 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:37.484349 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:38.254342 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:39.024112 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:39.793696 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:40.563235 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:41.332491 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:42.102248 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:42.873073 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:43.642838 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:44.412711 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:45.182602 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:45.952490 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:46.723278 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:47.493263 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:48.262610 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:49.031843 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:49.802321 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:50.572537 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:51.341998 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:52.111408 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:52.881915 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:53.650209 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:54.419852 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:55.189732 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:55.960157 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:56.730634 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:57.500115 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:58.270161 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:59.039467 [1] Warning: no training nodes in this partition! Backward fake loss.
22:39:59.809024 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:00.578572 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:01.348382 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:02.125431 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:02.926787 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:03.723972 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:04.491568 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:05.262194 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:06.031572 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:06.801322 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:07.571546 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:08.343028 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:09.112184 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:09.882685 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:10.652648 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:11.422426 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:12.192353 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:12.962323 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:13.732204 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:14.502040 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:15.271697 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:16.042547 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:16.812410 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:17.581555 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:18.350946 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:19.121057 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:19.890439 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:20.658616 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:21.426245 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:22.193920 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:22.960472 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:23.728948 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:24.497232 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:25.265453 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:26.033094 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:26.801794 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:27.569088 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:28.336684 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:29.105021 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:29.873163 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:30.642620 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:31.410380 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:32.178569 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:32.947768 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:33.718215 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:34.486886 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:35.256746 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:36.025315 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:36.795396 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:37.566361 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:38.335883 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:39.105542 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:39.875616 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:40.646348 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:41.416324 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:42.184525 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:42.952488 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:43.719213 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:44.486444 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:45.254251 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:46.022155 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:46.790221 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:47.558096 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:48.325754 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:49.094352 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:49.862842 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:50.632319 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:51.400896 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:52.169256 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:52.936918 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:53.703538 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:54.471266 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:55.238403 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:56.006461 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:56.773742 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:57.541556 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:58.308738 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:59.073698 [1] Warning: no training nodes in this partition! Backward fake loss.
22:40:59.840202 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:00.606296 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:01.372931 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:02.166031 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:02.968572 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:03.739607 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:04.508992 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:05.277129 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:06.045827 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:06.814178 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:07.582620 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:08.350864 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:09.119606 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:09.888254 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:10.657103 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:11.426500 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:12.194185 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:12.963179 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:13.731007 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:14.499305 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:15.266413 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:16.035005 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:16.803434 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:17.572266 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:18.339694 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:19.107519 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:19.878272 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:20.647488 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:21.415027 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:22.182207 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:22.949933 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:23.717647 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:24.484064 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:25.252566 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:26.021093 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:26.787303 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:27.553793 [1] Warning: no training nodes in this partition! Backward fake loss.
22:41:28.320103 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:23.309910 [1] proc begin: <DistEnv 1/4 nccl>
22:42:28.294426 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
22:42:28.304294 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:42:32.424318 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:33.867084 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:34.634928 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:35.402634 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:36.169230 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:36.935488 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:37.702973 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:38.469530 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:39.240699 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:40.005865 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:40.774302 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:41.541196 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:42.308796 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:43.076799 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:43.845600 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:44.614012 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:45.381973 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:46.149713 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:46.917510 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:47.684330 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:48.451734 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:49.218619 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:49.986418 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:50.753477 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:51.521835 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:52.289528 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:53.057135 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:53.825410 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:54.594678 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:55.362838 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:56.129736 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:56.897622 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:57.665415 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:58.431945 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:59.199510 [1] Warning: no training nodes in this partition! Backward fake loss.
22:42:59.968348 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:00.736599 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:01.504431 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:02.300138 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:03.103148 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:03.875954 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:04.645601 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:05.414895 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:06.185257 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:06.953222 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:07.723006 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:08.492932 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:09.263221 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:10.031889 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:10.800798 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:11.570835 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:12.340314 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:13.109586 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:13.879193 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:14.648358 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:15.418347 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:16.187034 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:16.955591 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:17.723615 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:18.491980 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:19.259788 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:20.027986 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:20.796822 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:21.564055 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:22.332088 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:23.100893 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:23.869487 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:24.638534 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:25.407433 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:26.176257 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:26.945547 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:27.714660 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:28.484027 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:29.253077 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:30.023409 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:30.792933 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:31.560927 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:32.344264 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:33.112868 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:33.882647 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:34.650595 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:35.418447 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:36.185445 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:36.953676 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:37.721635 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:38.488679 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:39.256893 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:40.025681 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:40.793718 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:41.564669 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:42.329769 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:43.098299 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:43.865318 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:44.632479 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:45.399687 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:46.166862 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:46.934088 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:47.699848 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:48.466616 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:49.234130 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:50.000585 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:50.767579 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:51.533541 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:52.299971 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:53.066930 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:53.833442 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:54.599720 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:55.366956 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:56.135781 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:56.903512 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:57.670571 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:58.436749 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:59.203263 [1] Warning: no training nodes in this partition! Backward fake loss.
22:43:59.969885 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:00.737235 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:01.512889 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:02.313946 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:03.103850 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:03.876099 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:04.645265 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:05.413913 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:06.182955 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:06.952670 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:07.720612 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:08.488950 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:09.257209 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:10.026856 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:10.795604 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:11.565404 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:12.333894 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:13.104278 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:13.872420 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:14.640875 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:15.409334 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:16.177244 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:16.945532 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:17.713023 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:18.482409 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:19.250720 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:20.020597 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:20.789677 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:21.556627 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:22.323975 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:23.091787 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:23.859903 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:24.626592 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:25.393053 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:26.161263 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:26.927916 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:27.695772 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:28.464330 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:29.231198 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:29.999708 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:30.766384 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:31.533844 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:32.302029 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:33.069514 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:33.840703 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:34.609012 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:35.377112 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:36.144953 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:36.914070 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:37.681742 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:38.450393 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:39.218022 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:39.986285 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:40.755021 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:41.522538 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:42.290678 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:43.059171 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:43.828348 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:44.597226 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:45.365830 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:46.134040 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:46.903066 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:47.671351 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:48.439026 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:49.205794 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:49.973376 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:50.740424 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:51.508496 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:52.275616 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:53.043119 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:53.811242 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:54.579173 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:55.347555 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:56.117216 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:56.885819 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:57.655376 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:58.423872 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:59.192648 [1] Warning: no training nodes in this partition! Backward fake loss.
22:44:59.960722 [1] Warning: no training nodes in this partition! Backward fake loss.
22:45:00.730224 [1] Warning: no training nodes in this partition! Backward fake loss.
22:45:01.499085 [1] Warning: no training nodes in this partition! Backward fake loss.
22:45:02.284859 [1] Warning: no training nodes in this partition! Backward fake loss.
22:45:03.088489 [1] Warning: no training nodes in this partition! Backward fake loss.
22:45:03.868020 [1] Warning: no training nodes in this partition! Backward fake loss.
22:45:04.638785 [1] Warning: no training nodes in this partition! Backward fake loss.
22:45:05.409525 [1] Warning: no training nodes in this partition! Backward fake loss.
22:45:06.180592 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:05.824749 [1] proc begin: <DistEnv 1/4 nccl>
22:47:11.151389 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
22:47:11.162469 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:47:16.646361 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:18.276654 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:19.045412 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:19.815021 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:20.585466 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:21.356879 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:22.127298 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:22.896907 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:23.667893 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:24.437984 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:25.206599 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:25.976551 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:26.747682 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:27.518602 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:28.289995 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:29.059679 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:29.828988 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:30.598591 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:31.368993 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:32.138877 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:32.909273 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:33.680205 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:34.450906 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:35.222118 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:35.991716 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:36.761039 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:37.528846 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:38.296246 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:39.064622 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:39.832183 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:40.599780 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:41.365802 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:42.134002 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:42.901275 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:43.667937 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:44.434380 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:45.200764 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:45.969177 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:46.735266 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:47.502952 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:48.268656 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:49.036285 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:49.803122 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:50.572997 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:51.340894 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:52.109022 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:52.877852 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:53.645849 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:54.413070 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:55.180754 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:55.949321 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:56.716807 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:57.484961 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:58.251710 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:59.018764 [1] Warning: no training nodes in this partition! Backward fake loss.
22:47:59.787547 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:00.556031 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:01.324412 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:02.122574 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:02.924993 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:03.694423 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:04.464293 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:05.234595 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:06.005327 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:06.774920 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:07.545354 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:08.315174 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:09.086294 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:09.856582 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:10.627080 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:11.396225 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:12.165379 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:12.937623 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:13.707929 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:14.477859 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:15.244798 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:16.013097 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:16.780617 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:17.548803 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:18.317019 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:19.084747 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:19.852677 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:20.622471 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:21.390019 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:22.157349 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:22.926271 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:23.694017 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:24.461918 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:25.229879 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:26.003373 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:26.768953 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:27.536725 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:28.304065 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:29.072234 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:29.839639 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:30.607354 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:31.375222 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:32.142903 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:32.910737 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:33.679368 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:34.449152 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:35.217453 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:35.985402 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:36.753348 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:37.520977 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:38.289212 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:39.057394 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:39.823928 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:40.593378 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:41.360066 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:42.127494 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:42.893833 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:43.661341 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:44.428341 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:45.195173 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:45.962068 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:46.729628 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:47.496847 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:48.264262 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:49.032749 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:49.801275 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:50.570080 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:51.337200 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:52.106006 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:52.874772 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:53.642749 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:54.411201 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:55.179185 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:55.947693 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:56.715509 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:57.483623 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:58.252752 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:59.021875 [1] Warning: no training nodes in this partition! Backward fake loss.
22:48:59.789804 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:00.558830 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:01.327339 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:02.100098 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:02.902364 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:03.694762 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:04.464131 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:05.231499 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:05.999498 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:06.768853 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:07.536787 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:08.304878 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:09.073490 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:09.841069 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:10.611263 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:11.378222 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:12.145987 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:12.913958 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:13.682209 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:14.450613 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:15.218817 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:15.986567 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:16.754130 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:17.522104 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:18.292198 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:19.060107 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:19.827712 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:20.595532 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:21.363046 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:22.131481 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:22.900106 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:23.667809 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:24.435476 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:25.202667 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:25.970310 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:26.738462 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:27.506321 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:28.273597 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:29.041071 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:29.809347 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:30.576683 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:31.345002 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:32.112335 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:32.879562 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:33.646003 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:34.414253 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:35.182204 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:35.950267 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:36.717505 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:37.484637 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:38.250857 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:39.018270 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:39.784787 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:40.552006 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:41.319004 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:42.085795 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:42.852058 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:43.618612 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:44.387515 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:45.155642 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:45.923765 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:46.692302 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:47.461133 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:48.230229 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:48.997303 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:49.764870 [1] Warning: no training nodes in this partition! Backward fake loss.
22:49:50.533060 [1] Warning: no training nodes in this partition! Backward fake loss.
00:51:54.817511 [1] proc begin: <DistEnv 1/4 nccl>
00:52:03.289427 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
00:52:03.307129 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

00:53:37.783796 [1] proc begin: <DistEnv 1/4 nccl>
00:53:43.796442 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
00:53:43.812790 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

00:55:14.202454 [1] proc begin: <DistEnv 1/4 nccl>
00:55:21.713666 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
00:55:21.727587 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:06:54.139828 [1] proc begin: <DistEnv 1/4 nccl>
02:06:54.201209 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
02:06:54.217791 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:06:55.751684 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.543224 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.562536 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.578509 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.590413 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.599770 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.609884 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.619219 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.629043 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.638206 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.647509 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.657035 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.666361 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.675793 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.685583 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.695061 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.704809 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.714439 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.723776 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.733268 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.742740 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.752460 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.763809 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.772879 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.782669 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.792362 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.802333 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.811720 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.821369 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.832062 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.841043 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.852452 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.861908 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.874855 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.887839 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.900026 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.909980 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.922891 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.933416 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.943944 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.953217 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.963358 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.972820 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.982387 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.991560 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.001044 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.010431 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.019644 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.029114 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.041277 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.050061 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.059658 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.069671 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.078743 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.087843 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.097309 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.106703 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.124151 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.136988 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.150443 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.159750 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.181528 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.194100 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.207438 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.217165 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.226774 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.236147 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.251093 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.260520 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.270121 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.280061 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.291710 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.304304 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.313351 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.322657 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.332329 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.341862 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.351152 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.360571 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.371849 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.380646 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.389982 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.402135 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.411604 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.420847 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.430053 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.439464 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.448648 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.457909 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.471836 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.489857 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.502067 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.513012 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.525772 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.536844 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.548496 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.564499 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.577449 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.587508 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.599807 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.609330 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.618831 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.628273 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.637534 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.646737 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.655963 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.665444 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.674794 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.684114 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.695897 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.705310 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.714828 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.723974 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.733109 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.742456 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.751659 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.760848 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.770094 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.779169 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.788456 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.797624 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.806659 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.815733 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.824771 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.837136 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.850922 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.863651 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.872854 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.882255 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.894773 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.904072 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.915436 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.928873 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.940997 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.950159 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.959516 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.971851 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.980726 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.989900 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.001894 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.010944 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.020088 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.029517 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.038562 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.047533 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.059677 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.069271 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.078466 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.087632 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.096889 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.106127 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.130004 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.148572 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.160222 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.177508 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.187084 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.196299 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.205528 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.214565 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.223813 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.233191 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.249731 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.267222 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.280838 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.293349 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.305320 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.314561 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.323986 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.334981 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.344801 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.354457 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.363497 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.372549 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.382028 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.391118 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.400406 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.409581 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.418760 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.427964 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.437156 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.446318 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.455416 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.464422 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.473781 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.484397 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.494373 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.503318 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.513719 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.530300 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.542174 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.553167 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.562667 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.572005 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.581412 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.590604 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.599834 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.608846 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.621147 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.630306 [1] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.639521 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:42.952845 [1] proc begin: <DistEnv 1/4 nccl>
02:07:43.005235 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
02:07:43.018132 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:07:44.587371 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.408628 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.426512 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.437230 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.447258 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.456959 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.468531 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.478229 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.488686 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.500510 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.510082 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.519616 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.528858 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.540987 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.550411 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.560809 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.571761 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.582501 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.591888 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.603524 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.612813 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.622716 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.636690 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.647186 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.656934 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.668568 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.677937 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.687358 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.696591 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.705851 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.715340 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.724669 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.734816 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.744557 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.754271 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.763970 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.773770 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.783431 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.794112 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.803795 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.815385 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.826383 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.836080 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.855164 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.868454 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.877813 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.890640 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.900034 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.909351 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.918762 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.929365 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.943876 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.959201 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.970250 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.979481 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.988692 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.000192 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.016473 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.028071 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.038353 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.048581 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.060558 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.074300 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.088864 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.100827 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.112551 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.122110 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.131419 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.143777 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.154110 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.165468 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.175714 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.186204 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.197015 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.207551 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.218013 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.228785 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.239390 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.249289 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.259131 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.269873 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.279661 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.290426 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.301509 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.312955 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.324260 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.333700 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.342917 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.356236 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.380956 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.396280 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.406896 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.416546 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.426056 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.435655 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.445444 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.455719 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.465044 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.474670 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.483903 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.493297 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.502922 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.515995 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.525114 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.534943 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.546011 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.555169 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.564347 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.573759 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.585505 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.596472 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.609585 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.620713 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.629859 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.639192 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.648906 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.660109 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.669284 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.681199 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.690590 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.699534 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.709180 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.718632 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.730892 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.740427 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.754068 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.767250 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.778091 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.789412 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.801479 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.813511 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.824477 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.840073 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.854301 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.865866 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.877283 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.891100 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.901874 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.913116 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.932171 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.952117 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.974213 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.984199 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.993177 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.002175 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.025850 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.038411 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.047647 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.056876 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.066170 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.081535 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.091631 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.103750 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.112753 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.121766 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.130681 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.139606 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.151632 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.163026 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.171958 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.187398 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.200356 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.210999 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.220612 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.231766 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.241294 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.250692 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.259996 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.269189 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.278275 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.287613 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.296711 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.306025 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.315021 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.326526 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.338213 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.351194 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.362930 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.372108 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.381762 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.391045 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.400188 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.409666 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.418971 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.428257 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.437834 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.447130 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.456283 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.465899 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.475364 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.484774 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.495032 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.504213 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.513504 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.522951 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.532337 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.541904 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.551183 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.560727 [1] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.570577 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:51.771804 [1] proc begin: <DistEnv 1/4 nccl>
02:08:51.840950 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
02:08:51.851529 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:08:53.397212 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.200611 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.215174 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.226789 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.236373 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.249272 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.258624 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.268303 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.280753 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.290219 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.299658 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.309489 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.319118 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.328876 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.341968 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.351063 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.360721 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.370387 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.381062 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.390735 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.402540 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.411959 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.421516 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.431268 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.440966 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.450191 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.459678 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.469178 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.478616 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.489019 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.498796 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.510315 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.519543 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.528975 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.539452 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.548975 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.563494 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.581989 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.594624 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.605250 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.616163 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.625371 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.634616 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.643956 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.653394 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.662909 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.678154 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.688794 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.698526 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.707949 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.717615 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.727048 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.737391 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.753783 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.775405 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.789505 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.801814 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.812547 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.822174 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.831473 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.840839 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.850160 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.859263 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.868411 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.877581 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.886762 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.899874 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.911477 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.921277 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.930609 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.939872 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.950054 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.959615 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.971244 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.981481 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.990812 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.999873 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.009375 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.018479 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.027655 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.037163 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.046830 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.056007 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.065610 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.074750 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.083863 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.093105 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.102223 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.111587 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.121996 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.131717 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.141278 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.151507 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.162110 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.171560 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.180706 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.189916 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.198910 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.208147 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.217677 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.226860 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.235989 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.245096 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.261127 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.275109 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.284220 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.294063 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.303651 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.314849 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.324105 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.337969 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.350656 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.360480 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.369567 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.378783 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.388088 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.397506 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.406534 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.415857 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.425402 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.434612 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.443978 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.453864 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.463329 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.473097 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.485467 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.498292 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.507235 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.516651 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.525959 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.535326 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.544964 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.554351 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.564840 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.577332 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.590246 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.599575 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.610596 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.620089 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.629962 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.639292 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.648603 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.657850 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.671632 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.681998 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.691513 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.701149 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.712913 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.725303 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.734601 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.753499 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.772225 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.793806 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.805374 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.816522 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.826636 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.835684 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.845473 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.855597 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.865895 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.875107 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.884431 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.894083 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.909449 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.920632 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.929921 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.939573 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.948953 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.962951 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.973418 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.984562 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.997081 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.006303 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.016387 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.029263 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.039695 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.049206 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.062711 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.073905 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.086532 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.099251 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.108548 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.118150 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.127361 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.136573 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.146232 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.155744 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.165361 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.174719 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.184204 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.193974 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.203204 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.212637 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.221983 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.231683 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.241212 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.250396 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.259858 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.269701 [1] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.279291 [1] Warning: no training nodes in this partition! Backward fake loss.
02:18:49.214563 [1] proc begin: <DistEnv 1/4 nccl>
02:18:53.727177 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
02:18:53.735052 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:18:59.105985 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:00.202926 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:00.486246 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:00.767373 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:01.046933 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:01.327858 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:01.620791 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:01.913218 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:02.205734 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:02.498547 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:02.788867 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:03.072847 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:03.354005 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:03.634598 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:03.914895 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:04.195865 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:04.476995 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:04.757794 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:05.038319 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:05.319592 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:05.602425 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:05.884027 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:06.165475 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:06.446559 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:06.729801 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:07.011152 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:07.292053 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:07.573470 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:07.854608 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:08.136241 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:08.418029 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:08.699401 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:08.980898 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:09.262569 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:09.545531 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:09.827661 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:10.110381 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:10.392411 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:10.672695 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:10.954212 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:11.235723 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:11.516037 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:11.797619 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:12.079151 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:12.359824 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:12.641439 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:12.923354 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:13.203967 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:13.484836 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:13.766167 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:14.047230 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:14.328505 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:14.610324 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:14.892259 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:15.174342 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:15.456396 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:15.737897 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:16.019189 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:16.300396 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:16.581907 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:16.862477 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:17.143780 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:17.424449 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:17.705395 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:17.986348 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:18.267024 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:18.548896 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:18.831850 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:19.113184 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:19.394438 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:19.675927 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:19.957599 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:20.238057 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:20.519766 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:20.801232 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:21.082538 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:21.363063 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:21.643660 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:21.925121 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:22.207388 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:22.488753 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:22.769800 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:23.050790 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:23.331899 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:23.613601 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:23.894468 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:24.175972 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:24.457283 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:24.738527 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:25.019540 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:25.300187 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:25.581334 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:25.862873 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:26.145009 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:26.426028 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:26.707047 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:26.989169 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:27.271523 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:27.552297 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:27.833070 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:28.114558 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:28.396471 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:28.677328 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:28.959176 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:29.240615 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:29.521700 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:29.803358 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:30.084385 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:30.366422 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:30.647688 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:30.930290 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:31.211220 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:31.491729 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:31.773704 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:32.054727 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:32.337189 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:32.618340 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:32.898938 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:33.181553 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:33.462160 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:33.743744 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:34.024991 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:34.305826 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:34.587090 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:34.868599 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:35.149600 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:35.430169 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:35.711070 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:35.992325 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:36.273225 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:36.555395 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:36.836206 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:37.117829 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:37.399840 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:37.680530 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:37.961768 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:38.244210 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:38.525645 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:38.807861 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:39.089605 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:39.371335 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:39.652526 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:39.933615 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:40.215900 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:40.498086 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:40.779722 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:41.062214 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:41.342867 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:41.623952 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:41.904550 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:42.185284 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:42.465780 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:42.746194 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:43.026556 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:43.308316 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:43.589512 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:43.870281 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:44.150927 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:44.431876 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:44.713232 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:44.994441 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:45.276490 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:45.556761 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:45.837930 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:46.119065 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:46.401172 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:46.683368 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:46.965205 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:47.247183 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:47.528581 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:47.809914 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:48.090357 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:48.371393 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:48.652934 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:48.933237 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:49.214815 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:49.496541 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:49.777943 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:50.058874 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:50.340692 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:50.621860 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:50.903267 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:51.184585 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:51.465736 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:51.746858 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:52.028078 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:52.309277 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:52.589935 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:52.870030 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:53.151118 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:53.432216 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:53.712999 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:53.993325 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:54.274368 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:54.555621 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:54.837998 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:55.118812 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:55.400243 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:55.680566 [1] Warning: no training nodes in this partition! Backward fake loss.
02:19:55.963518 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:06.331740 [1] proc begin: <DistEnv 1/4 nccl>
02:22:12.109777 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
02:22:12.119536 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:22:17.539524 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:18.852231 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:19.334250 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:19.814103 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:20.294119 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:20.774476 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:21.256600 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:21.738166 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:22.219599 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:22.699371 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:23.179666 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:23.660999 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:24.142808 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:24.627194 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:25.109656 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:25.591313 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:26.073424 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:26.555204 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:27.036683 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:27.517247 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:27.996211 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:28.476904 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:28.958823 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:29.440948 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:29.923080 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:30.404710 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:30.887746 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:31.369957 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:31.853121 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:32.335235 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:32.817387 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:33.299772 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:33.783474 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:34.266944 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:34.749501 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:35.231570 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:35.714090 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:36.196565 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:36.677326 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:37.160018 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:37.641733 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:38.123227 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:38.606073 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:39.088605 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:39.570397 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:40.051208 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:40.529534 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:41.009590 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:41.488623 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:41.967768 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:42.448131 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:42.926584 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:43.405533 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:43.884676 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:44.363593 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:44.842254 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:45.320549 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:45.799278 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:46.277027 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:46.755982 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:47.234443 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:47.713863 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:48.192104 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:48.670113 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:49.148726 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:49.627636 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:50.106243 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:50.585257 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:51.064820 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:51.543128 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:52.021535 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:52.500471 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:52.979482 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:53.457576 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:53.936480 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:54.415682 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:54.893954 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:55.373203 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:55.852757 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:56.332002 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:56.810812 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:57.289506 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:57.768009 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:58.246632 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:58.725507 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:59.204715 [1] Warning: no training nodes in this partition! Backward fake loss.
02:22:59.683245 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:00.162805 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:00.641423 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:01.120917 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:01.615088 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:02.116901 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:02.618869 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:03.104618 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:03.584802 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:04.064083 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:04.543215 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:05.022124 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:05.501108 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:05.980298 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:06.459353 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:06.938352 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:07.418085 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:07.897188 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:08.377075 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:08.855545 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:09.333968 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:09.813297 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:10.292474 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:10.771458 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:11.251226 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:11.729728 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:12.208292 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:12.687549 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:13.167772 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:13.646900 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:14.127901 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:14.607752 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:15.086858 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:15.566930 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:16.046732 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:16.526037 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:17.003683 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:17.480672 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:17.958354 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:18.435938 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:18.913500 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:19.390553 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:19.868303 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:20.346010 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:20.822919 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:21.301711 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:21.779570 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:22.257506 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:22.735116 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:23.212579 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:23.689891 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:24.167882 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:24.646074 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:25.125113 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:25.604408 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:26.083450 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:26.562378 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:27.040613 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:27.518440 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:27.997635 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:28.476057 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:28.955217 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:29.434227 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:29.912541 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:30.391163 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:30.869811 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:31.348663 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:31.826968 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:32.305720 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:32.783792 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:33.261970 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:33.743126 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:34.222895 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:34.702174 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:35.182321 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:35.661472 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:36.141530 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:36.620975 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:37.100820 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:37.579761 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:38.059276 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:38.538544 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:39.018534 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:39.496227 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:39.974899 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:40.452572 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:40.931028 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:41.409655 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:41.887073 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:42.364492 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:42.842248 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:43.320866 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:43.800458 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:44.279468 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:44.759333 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:45.238825 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:45.717592 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:46.197172 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:46.676370 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:47.155613 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:47.635842 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:48.115947 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:48.598756 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:49.079027 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:49.560493 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:50.042241 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:50.522380 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:51.002744 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:51.485061 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:51.966433 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:52.447737 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:52.929082 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:53.410763 [1] Warning: no training nodes in this partition! Backward fake loss.
02:23:53.892636 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:12.821256 [1] proc begin: <DistEnv 1/4 nccl>
02:25:17.797444 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
02:25:17.807389 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:25:23.320283 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:24.797679 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:25.688949 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:26.580307 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:27.464756 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:28.352318 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:29.239431 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:30.125432 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:31.010382 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:31.895497 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:32.780692 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:33.664659 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:34.549964 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:35.433958 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:36.318441 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:37.200182 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:38.082639 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:38.964193 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:39.849024 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:40.731812 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:41.615483 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:42.498243 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:43.379088 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:44.262816 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:45.144851 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:46.028839 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:46.910348 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:47.794426 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:48.676351 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:49.557706 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:50.441020 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:51.322542 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:52.205965 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:53.089675 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:53.970400 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:54.852028 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:55.732916 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:56.612581 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:57.493481 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:58.376681 [1] Warning: no training nodes in this partition! Backward fake loss.
02:25:59.259449 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:00.142782 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:01.025870 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:01.916789 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:02.837624 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:03.733191 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:04.617264 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:05.501878 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:06.385368 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:07.270555 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:08.156190 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:09.042261 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:09.928540 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:10.814359 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:11.701913 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:12.589067 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:13.473561 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:14.359872 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:15.243580 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:16.128990 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:17.014273 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:17.901761 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:18.787263 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:19.673802 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:20.558351 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:21.444791 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:22.331868 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:23.218019 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:24.103896 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:24.989697 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:25.874792 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:26.758814 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:27.643698 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:28.526960 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:29.413805 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:30.299234 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:31.185508 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:32.069013 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:32.955233 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:33.839135 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:34.723991 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:35.608165 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:36.491220 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:37.376026 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:38.259215 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:39.141732 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:40.024576 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:40.908475 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:41.792764 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:42.677048 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:43.561816 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:44.444907 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:45.328172 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:46.212146 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:47.096865 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:47.981356 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:48.866304 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:49.750301 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:50.633525 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:51.518837 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:52.402800 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:53.286846 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:54.171433 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:55.055404 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:55.939372 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:56.823786 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:57.709310 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:58.594052 [1] Warning: no training nodes in this partition! Backward fake loss.
02:26:59.478079 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:00.360802 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:01.241112 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:02.160281 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:03.069610 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:03.954250 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:04.836136 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:05.719436 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:06.605917 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:07.490541 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:08.375065 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:09.260895 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:10.145833 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:11.028207 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:11.912136 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:12.795558 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:13.679786 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:14.563181 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:15.448109 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:16.332614 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:17.217672 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:18.101650 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:18.986992 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:19.871989 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:20.756607 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:21.640586 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:22.525860 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:23.410314 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:24.295216 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:25.178190 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:26.062495 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:26.946501 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:27.830549 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:28.713182 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:29.596032 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:30.479993 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:31.362789 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:32.245423 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:33.128202 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:34.011101 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:34.895522 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:35.778270 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:36.660458 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:37.545079 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:38.428132 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:39.311441 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:40.195215 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:41.077822 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:41.961532 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:42.841946 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:43.722908 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:44.607415 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:45.489721 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:46.372169 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:47.256066 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:48.139813 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:49.020716 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:49.903882 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:50.789123 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:51.670217 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:52.552277 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:53.435013 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:54.318822 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:55.200847 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:56.080950 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:56.961197 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:57.842996 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:58.725451 [1] Warning: no training nodes in this partition! Backward fake loss.
02:27:59.607209 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:00.488982 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:01.372085 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:02.272895 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:03.192596 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:04.079920 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:04.965549 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:05.850654 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:06.736956 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:07.622806 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:08.508472 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:09.393877 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:10.281573 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:11.167905 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:12.052636 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:12.936892 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:13.822560 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:14.707558 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:15.592154 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:16.478417 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:17.362357 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:18.247325 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:19.131194 [1] Warning: no training nodes in this partition! Backward fake loss.
02:28:20.017900 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:32.542747 [1] proc begin: <DistEnv 1/4 nccl>
02:29:32.640266 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
02:29:32.652937 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:29:34.096893 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.838783 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.855388 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.869104 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.879715 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.890577 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.901005 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.913958 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.925084 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.937525 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.954007 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.970014 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.982427 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.991709 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.001173 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.010677 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.019580 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.032814 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.043090 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.053354 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.063483 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.073094 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.083263 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.094253 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.104647 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.115131 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.126083 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.136641 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.147047 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.157478 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.167754 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.178214 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.188359 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.198827 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.210478 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.222876 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.233249 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.244387 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.253484 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.262805 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.271962 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.282891 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.291992 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.301383 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.310634 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.325922 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.335262 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.344983 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.354420 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.370781 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.382013 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.390960 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.405371 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.414525 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.423814 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.434877 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.450126 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.462464 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.477229 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.489856 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.499195 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.508358 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.523585 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.533863 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.549172 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.565836 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.580932 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.590747 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.602822 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.611655 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.621818 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.630467 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.639366 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.648004 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.657651 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.667852 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.679123 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.688663 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.700484 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.709523 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.718564 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.727676 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.738327 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.746874 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.755569 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.766854 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.776395 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.785553 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.797273 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.808957 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.821306 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.830329 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.839313 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.849149 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.859114 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.868531 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.878315 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.887684 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.896876 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.908506 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.922270 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.934977 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.943979 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.953441 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.963643 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.974230 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.982993 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.992291 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.001462 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.011753 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.024050 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.032995 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.041986 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.051044 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.061306 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.073114 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.085929 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.097768 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.109974 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.123698 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.133018 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.144271 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.155353 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.164774 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.174152 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.183694 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.196209 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.206618 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.215792 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.225440 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.236206 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.245480 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.254628 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.263643 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.272853 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.282120 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.291566 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.301304 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.311760 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.324705 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.335927 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.347980 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.362113 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.371608 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.380775 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.390262 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.399379 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.408896 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.418548 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.427680 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.436762 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.445991 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.458579 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.468857 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.484476 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.496254 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.506417 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.515474 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.534626 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.545279 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.569502 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.581014 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.592975 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.602530 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.611549 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.620748 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.631853 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.641027 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.650167 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.659468 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.668637 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.677591 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.686498 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.695890 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.705417 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.714397 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.723373 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.732344 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.743652 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.754060 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.763272 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.775817 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.792309 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.804716 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.814440 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.823449 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.835018 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.849390 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.860420 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.869942 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.878979 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.888358 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.899988 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.911855 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.920570 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.929835 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.938919 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.948234 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.958887 [1] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.968509 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:11.160531 [1] proc begin: <DistEnv 1/4 nccl>
02:30:11.195018 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
02:30:11.209923 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:30:12.798795 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.569753 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.586239 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.598920 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.610976 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.620281 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.633068 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.646189 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.659598 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.668614 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.678031 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.687521 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.699359 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.710086 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.719574 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.729035 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.738467 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.747667 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.757487 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.770471 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.781017 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.795448 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.805779 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.817985 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.827682 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.837234 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.846832 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.856155 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.865906 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.875530 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.884976 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.897539 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.906968 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.916413 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.926771 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.936375 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.946244 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.958610 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.971169 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.983571 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.996752 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.009822 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.021112 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.030754 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.040147 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.050659 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.060433 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.072472 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.082670 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.092634 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.102472 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.112129 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.121808 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.131191 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.140791 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.150630 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.164935 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.184065 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.196006 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.206521 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.216260 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.225908 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.239428 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.249438 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.258962 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.274427 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.285647 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.295960 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.305652 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.316527 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.325729 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.335238 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.344797 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.359900 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.370970 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.380310 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.389844 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.400840 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.411886 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.421129 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.430787 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.441215 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.452939 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.463684 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.474472 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.485074 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.494905 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.504558 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.514130 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.532478 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.548562 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.558202 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.567774 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.577129 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.586439 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.595442 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.605320 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.619415 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.629987 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.639446 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.649233 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.660372 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.670194 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.679788 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.689071 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.701167 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.710167 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.720695 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.730166 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.739412 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.752224 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.761821 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.773716 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.782844 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.792474 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.802061 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.811300 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.821010 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.830233 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.839605 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.850437 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.867552 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.879840 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.889460 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.899377 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.908902 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.918461 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.927629 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.937023 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.946502 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.955941 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.965708 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.980000 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.991664 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.001593 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.011122 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.020695 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.030318 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.039587 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.048796 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.058679 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.068035 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.077668 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.086836 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.098226 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.107258 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.116720 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.128578 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.140332 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.150057 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.159445 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.182044 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.192294 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.204680 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.214510 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.223916 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.233156 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.257078 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.279530 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.291639 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.301110 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.310483 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.319693 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.329086 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.338341 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.350280 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.359363 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.368989 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.378171 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.387407 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.396578 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.406151 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.415267 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.426656 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.435678 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.445222 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.457393 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.469671 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.478861 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.494372 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.509861 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.523124 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.536343 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.546460 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.555868 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.565317 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.574883 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.585450 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.597152 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.608930 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.618279 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.627432 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.638153 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.648640 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.658084 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.667227 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.678101 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.687085 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.696443 [1] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.705866 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:49.174236 [1] proc begin: <DistEnv 1/4 nccl>
02:31:49.264062 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
02:31:49.277187 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:31:50.789390 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.571365 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.587170 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.596867 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.608771 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.625594 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.636274 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.646529 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.656084 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.665719 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.675069 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.687618 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.696794 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.709234 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.718324 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.727768 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.737202 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.746567 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.755998 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.765727 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.777523 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.787068 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.796812 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.806300 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.816740 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.827266 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.836393 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.846534 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.855795 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.867476 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.876589 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.885970 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.898715 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.910537 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.923720 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.933615 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.944556 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.957083 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.966908 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.977250 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.986927 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.996858 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.008668 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.019665 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.028938 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.039586 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.048964 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.058424 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.067737 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.076954 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.086349 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.095708 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.105464 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.122519 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.134511 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.143815 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.153377 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.165691 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.178917 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.190269 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.199695 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.209596 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.221816 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.231097 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.240205 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.249505 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.258587 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.267698 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.281087 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.291022 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.300858 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.310531 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.319582 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.328980 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.343355 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.353034 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.362258 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.377978 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.387637 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.397008 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.406384 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.415722 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.424944 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.434459 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.445335 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.457087 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.467627 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.477191 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.490645 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.520909 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.537449 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.551075 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.560222 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.570450 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.579857 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.589332 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.605180 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.615720 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.627292 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.636987 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.649416 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.658917 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.668214 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.677556 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.686861 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.699010 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.708495 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.717775 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.733097 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.742046 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.751091 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.763176 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.776042 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.785494 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.794677 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.804251 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.813919 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.823279 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.832422 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.841861 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.850680 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.866756 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.877422 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.888430 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.899216 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.909037 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.919428 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.929437 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.939704 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.950372 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.961066 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.974009 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.983427 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.992290 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.001363 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.010398 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.019407 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.028733 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.038145 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.047095 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.058040 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.070634 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.079741 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.090642 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.104808 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.119482 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.130049 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.139154 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.148260 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.157466 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.184031 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.195944 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.209523 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.222380 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.234819 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.243709 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.255345 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.264539 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.274024 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.289157 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.299096 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.308660 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.319701 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.330792 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.342193 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.351156 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.360500 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.369546 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.382555 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.395602 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.406492 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.415370 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.427360 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.436463 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.445734 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.454858 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.463980 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.473656 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.482835 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.492090 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.504657 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.513838 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.523029 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.532033 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.541299 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.553772 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.566686 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.583196 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.595733 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.605970 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.615199 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.624359 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.633703 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.642891 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.652070 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.661209 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.671294 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.681485 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.690924 [1] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.700254 [1] Warning: no training nodes in this partition! Backward fake loss.
02:32:59.932479 [1] proc begin: <DistEnv 1/4 nccl>
02:33:06.498201 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
02:33:06.517147 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:33:51.444522 [1] proc begin: <DistEnv 1/4 nccl>
02:33:57.265340 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
02:33:57.286470 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:35:06.681677 [1] proc begin: <DistEnv 1/4 nccl>
02:35:13.580211 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
02:35:13.597293 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:36:39.162155 [1] proc begin: <DistEnv 1/4 nccl>
02:36:39.240079 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
02:36:39.252713 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:36:40.792106 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.690408 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.726127 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.762124 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.790700 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.820414 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.848671 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.878447 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.907015 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.937222 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.962269 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.995899 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.025409 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.058391 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.079428 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.111285 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.135661 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.167590 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.197022 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.225097 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.254862 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.275042 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.305545 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.336289 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.355259 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.389234 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.412035 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.440833 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.473599 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.500473 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.529067 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.557444 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.586596 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.615246 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.642588 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.670299 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.701107 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.733767 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.767492 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.799298 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.830448 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.862613 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.894470 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.924318 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.957872 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.985510 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.019774 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.050508 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.081442 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.111167 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.154787 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.198779 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.228344 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.257700 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.291737 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.321757 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.343028 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.373424 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.413412 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.441929 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.469921 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.498197 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.528179 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.554055 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.582899 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.613762 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.643553 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.674389 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.704406 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.733145 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.757634 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.791797 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.812988 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.841086 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.871098 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.904788 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.935731 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.963513 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.985389 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.016008 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.045854 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.076245 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.101821 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.136691 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.188493 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.217488 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.241272 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.271779 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.305852 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.331247 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.361041 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.384651 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.427165 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.460521 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.489607 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.520358 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.543363 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.572600 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.601615 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.628813 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.658271 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.688363 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.716576 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.744916 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.777135 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.808638 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.838175 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.861548 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.891791 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.921103 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.950305 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.978669 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.013028 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.036080 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.065052 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.094343 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.124410 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.163109 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.210399 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.240578 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.268934 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.299598 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.329024 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.359218 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.388715 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.423382 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.452663 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.480832 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.509007 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.538477 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.568415 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.598244 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.628426 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.657568 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.680948 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.711209 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.740091 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.767883 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.797093 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.824585 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.850334 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.877026 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.897752 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.926227 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.957168 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.980864 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.011599 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.038861 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.068221 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.097332 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.127954 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.169001 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.203971 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.231317 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.257076 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.287690 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.321603 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.348859 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.375873 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.405340 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.443003 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.463293 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.493986 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.523943 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.551402 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.579317 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.611018 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.640716 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.664248 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.695013 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.723978 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.750852 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.781312 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.812441 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.846432 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.866641 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.900853 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.932296 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.961503 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.991931 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.019084 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.047423 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.074689 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.103911 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.133221 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.164749 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.218763 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.249760 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.277611 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.305283 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.334040 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.365627 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.394265 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.433122 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.458124 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.488698 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.517893 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.546675 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.577910 [1] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.606908 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:14.210136 [1] proc begin: <DistEnv 1/4 nccl>
02:37:14.302332 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
02:37:14.314982 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:37:15.669025 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.436530 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.463540 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.487283 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.508157 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.529542 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.555154 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.580389 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.604897 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.630016 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.659061 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.686641 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.711814 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.735621 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.759171 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.778911 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.800908 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.827280 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.850119 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.874627 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.901602 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.926596 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.948863 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.971788 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.989072 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.001969 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.016064 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.029717 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.052052 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.074670 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.097747 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.124140 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.144975 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.167257 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.195802 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.217540 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.243232 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.259736 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.283568 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.306709 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.325688 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.350375 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.371380 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.387542 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.410496 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.428239 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.450565 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.469712 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.494309 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.515634 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.538344 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.561334 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.586909 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.609972 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.632100 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.654325 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.676087 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.698700 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.718052 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.739787 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.762443 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.786285 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.810967 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.832756 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.853917 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.877205 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.898608 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.924267 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.947323 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.963955 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.981264 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.005753 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.030019 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.054223 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.077384 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.099457 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.122306 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.142827 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.163448 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.185387 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.219175 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.240377 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.258955 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.279590 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.301127 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.322382 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.338631 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.359072 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.378575 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.395849 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.419738 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.444644 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.470395 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.494676 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.518748 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.537919 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.562407 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.586312 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.611344 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.636280 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.658581 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.676136 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.698604 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.716219 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.738770 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.759186 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.782913 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.802989 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.825417 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.846701 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.870227 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.891853 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.914830 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.938015 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.962211 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.981707 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.006329 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.027045 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.047443 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.068028 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.089354 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.110938 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.135123 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.157925 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.182300 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.209972 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.241715 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.264440 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.281133 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.300010 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.313638 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.325272 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.338655 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.360697 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.383192 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.398755 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.410432 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.424617 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.448459 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.470287 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.491308 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.515685 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.537719 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.562401 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.586122 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.607949 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.630967 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.652802 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.677507 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.697891 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.715609 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.738364 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.762231 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.781433 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.804232 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.827122 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.850861 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.870791 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.888923 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.910582 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.934621 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.954803 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.979180 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.999601 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.022259 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.042053 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.066225 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.087696 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.111645 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.134153 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.153046 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.178069 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.202885 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.225910 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.255442 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.278283 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.302835 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.325926 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.349486 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.378185 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.406157 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.430789 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.457842 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.486032 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.509814 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.534179 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.554179 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.579229 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.601714 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.624867 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.650058 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.674889 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.698009 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.721176 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.736024 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.753458 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.771232 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.794271 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.815187 [1] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.838591 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:16.488422 [1] proc begin: <DistEnv 1/4 nccl>
02:38:16.525064 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
02:38:16.538530 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:38:18.146528 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.006996 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.028466 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.044316 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.061203 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.073099 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.083633 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.093616 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.103458 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.113441 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.123669 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.133786 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.145486 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.155406 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.169292 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.187277 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.206994 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.223910 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.247646 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.266017 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.286684 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.303084 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.323169 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.341914 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.360360 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.373489 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.383927 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.394077 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.403589 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.414154 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.424176 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.437440 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.456665 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.475925 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.500035 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.522360 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.539613 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.560134 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.578867 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.598315 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.615586 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.627293 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.637522 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.647046 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.656310 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.666437 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.676063 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.687245 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.697046 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.707648 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.718030 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.728715 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.742735 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.752217 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.763012 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.774261 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.788134 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.797901 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.808250 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.819296 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.835085 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.854204 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.871458 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.890852 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.910945 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.930208 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.946887 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.958559 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.976617 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.997623 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.008882 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.022724 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.032343 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.042220 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.052418 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.062724 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.072646 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.082487 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.097105 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.115562 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.136780 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.155819 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.174293 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.192548 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.216760 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.232218 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.241936 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.252025 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.263359 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.273336 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.283479 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.293374 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.304486 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.316450 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.327571 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.336932 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.355552 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.367225 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.377389 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.387334 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.397488 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.411510 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.421085 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.431191 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.448589 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.462050 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.472560 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.483106 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.499525 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.517828 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.536846 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.557134 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.575759 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.588752 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.608046 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.628167 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.648652 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.668529 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.692630 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.711004 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.730232 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.747918 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.768923 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.787745 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.806223 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.833523 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.852604 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.872000 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.890197 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.910959 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.922173 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.931548 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.941859 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.954650 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.968119 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.987488 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.008795 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.027051 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.046389 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.063914 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.084022 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.103212 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.122482 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.138492 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.148059 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.162105 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.172424 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.185349 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.198052 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.208417 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.218325 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.227950 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.238017 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.247863 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.258250 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.271281 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.290425 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.307169 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.318025 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.327817 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.337614 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.347563 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.357900 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.368370 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.378663 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.389126 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.399619 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.411059 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.422011 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.447616 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.467179 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.485814 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.507200 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.527776 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.541127 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.560300 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.574355 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.584575 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.594699 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.605032 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.616419 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.631269 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.641070 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.651477 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.665297 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.674769 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.684533 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.702862 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.716378 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.725958 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.736720 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.748170 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.757720 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.769945 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.779482 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.792741 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.802445 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.812118 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.827052 [1] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.837284 [1] Warning: no training nodes in this partition! Backward fake loss.
09:41:28.297816 [1] proc begin: <DistEnv 1/4 nccl>
09:42:00.190565 [1] proc begin: <DistEnv 1/4 nccl>
09:42:07.199123 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:42:07.216447 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:43:33.009093 [1] proc begin: <DistEnv 1/4 nccl>
09:43:40.258060 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:43:40.274649 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:45:34.983280 [1] proc begin: <DistEnv 1/4 nccl>
09:45:41.874583 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:45:41.891946 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:48:52.103521 [1] proc begin: <DistEnv 1/4 nccl>
09:48:57.635750 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:48:57.650507 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:49:45.665608 [1] proc begin: <DistEnv 1/4 nccl>
09:49:52.461655 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:49:52.479303 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:50:38.523118 [1] proc begin: <DistEnv 1/4 nccl>
09:50:44.207777 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:50:44.224203 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:52:42.371974 [1] proc begin: <DistEnv 1/4 nccl>
09:52:47.222711 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
09:52:47.231351 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:52:52.654152 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:53.664001 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:53.943334 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:54.223136 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:54.502068 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:54.780546 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:55.058477 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:55.340709 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:55.619666 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:55.898236 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:56.176640 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:56.455721 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:56.735463 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:57.013559 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:57.291858 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:57.571315 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:57.850223 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:58.128434 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:58.407822 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:58.686270 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:58.964756 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:59.244118 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:59.523478 [1] Warning: no training nodes in this partition! Backward fake loss.
09:52:59.802070 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:00.080726 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:00.359395 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:00.639423 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:00.918822 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:01.197766 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:01.476721 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:01.761574 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:02.053149 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:02.344804 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:02.637434 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:02.927601 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:03.215247 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:03.495092 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:03.775137 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:04.054929 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:04.334874 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:04.614226 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:04.893611 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:05.173486 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:05.451467 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:05.729708 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:06.008648 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:06.288242 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:06.569333 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:06.850270 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:07.130787 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:07.412214 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:07.692981 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:07.973563 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:08.253863 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:08.534604 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:08.814333 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:09.095811 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:09.376987 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:09.657178 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:09.937457 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:10.218519 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:10.498716 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:10.779018 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:11.060111 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:11.340417 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:11.620576 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:11.901169 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:12.183188 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:12.465087 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:12.745319 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:13.025665 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:13.305893 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:13.585836 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:13.866354 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:14.146690 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:14.426827 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:14.707901 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:14.988680 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:15.269747 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:15.549249 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:15.830483 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:16.111549 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:16.392472 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:16.672477 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:16.952946 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:17.233771 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:17.514779 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:17.796407 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:18.076615 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:18.357672 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:18.637432 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:18.917405 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:19.198610 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:19.479333 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:19.760303 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:20.040230 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:20.320400 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:20.600978 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:20.882176 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:21.161977 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:21.441693 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:21.721867 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:22.001580 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:22.282667 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:22.563036 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:22.843640 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:23.124134 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:23.404345 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:23.684541 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:23.964435 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:24.244409 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:24.524542 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:24.804469 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:25.084236 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:25.364543 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:25.644786 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:25.925275 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:26.205335 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:26.485534 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:26.767001 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:27.048673 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:27.329671 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:27.610148 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:27.890314 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:28.169475 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:28.448087 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:28.726226 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:29.004094 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:29.281987 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:29.560693 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:29.839262 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:30.117721 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:30.396049 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:30.674808 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:30.953178 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:31.231942 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:31.510404 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:31.790334 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:32.069561 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:32.349310 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:32.628877 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:32.908422 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:33.189134 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:33.468438 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:33.747410 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:34.026844 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:34.306277 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:34.585456 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:34.864791 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:35.144613 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:35.423247 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:35.702499 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:35.980957 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:36.260170 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:36.539507 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:36.818475 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:37.097367 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:37.377410 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:37.657624 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:37.938186 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:38.216661 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:38.495029 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:38.773693 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:39.052757 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:39.332234 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:39.610959 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:39.890856 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:40.169307 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:40.448068 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:40.727133 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:41.006122 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:41.284366 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:41.562705 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:41.841685 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:42.119935 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:42.399171 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:42.677860 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:42.956550 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:43.235491 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:43.514278 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:43.793136 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:44.071624 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:44.350051 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:44.629084 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:44.907338 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:45.186066 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:45.465054 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:45.743707 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:46.022615 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:46.301567 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:46.580624 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:46.859485 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:47.137639 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:47.416739 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:47.696204 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:47.974784 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:48.253323 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:48.533184 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:48.812361 [1] Warning: no training nodes in this partition! Backward fake loss.
09:53:49.090687 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:13.917646 [1] proc begin: <DistEnv 1/4 nccl>
09:54:18.914872 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
09:54:18.924570 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:54:22.804295 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:23.886565 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:24.168300 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:24.449462 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:24.733451 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:25.014440 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:25.295594 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:25.576723 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:25.858308 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:26.139248 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:26.420084 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:26.700398 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:26.981673 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:27.264173 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:27.545455 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:27.826868 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:28.108345 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:28.389837 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:28.671239 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:28.951997 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:29.233276 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:29.513531 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:29.795093 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:30.076090 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:30.356453 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:30.637724 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:30.919060 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:31.200466 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:31.480394 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:31.760915 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:32.041900 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:32.322883 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:32.603314 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:32.883699 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:33.165127 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:33.445520 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:33.726000 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:34.006792 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:34.286511 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:34.567605 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:34.848612 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:35.129199 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:35.409193 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:35.689418 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:35.970092 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:36.250470 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:36.530227 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:36.810012 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:37.090165 [1] Warning: no training nodes in this partition! Backward fake loss.
09:54:37.370970 [1] Warning: no training nodes in this partition! Backward fake loss.
09:55:31.600900 [1] proc begin: <DistEnv 1/4 nccl>
09:55:44.175569 [1] proc begin: <DistEnv 1/4 nccl>
09:55:48.228982 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
09:55:48.240925 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:55:53.223209 [1] Warning: no training nodes in this partition! Backward fake loss.
09:55:54.481987 [1] Warning: no training nodes in this partition! Backward fake loss.
09:55:54.960413 [1] Warning: no training nodes in this partition! Backward fake loss.
09:55:55.438917 [1] Warning: no training nodes in this partition! Backward fake loss.
09:55:55.918294 [1] Warning: no training nodes in this partition! Backward fake loss.
09:55:56.396643 [1] Warning: no training nodes in this partition! Backward fake loss.
09:55:56.878300 [1] Warning: no training nodes in this partition! Backward fake loss.
09:55:57.358491 [1] Warning: no training nodes in this partition! Backward fake loss.
09:55:57.841815 [1] Warning: no training nodes in this partition! Backward fake loss.
09:55:58.321688 [1] Warning: no training nodes in this partition! Backward fake loss.
09:55:58.803488 [1] Warning: no training nodes in this partition! Backward fake loss.
09:55:59.284597 [1] Warning: no training nodes in this partition! Backward fake loss.
09:55:59.767012 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:00.247215 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:00.726555 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:01.204624 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:01.683596 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:02.163436 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:02.666378 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:03.170235 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:03.672368 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:04.153229 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:04.634446 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:05.114715 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:05.595319 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:06.079215 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:06.559392 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:07.040519 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:07.522668 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:08.006187 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:08.488960 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:08.971025 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:09.452090 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:09.932266 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:10.412706 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:10.894874 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:11.377116 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:11.858666 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:12.340916 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:12.823332 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:13.304276 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:13.785276 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:14.265242 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:14.747499 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:15.228057 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:15.708538 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:16.190812 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:16.670286 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:17.150784 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:17.630127 [1] Warning: no training nodes in this partition! Backward fake loss.
09:56:52.881953 [1] proc begin: <DistEnv 1/4 nccl>
09:56:59.350506 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:56:59.366573 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:58:03.893452 [1] proc begin: <DistEnv 1/4 nccl>
09:58:10.533989 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:58:10.554886 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:00:01.539676 [1] proc begin: <DistEnv 1/4 nccl>
10:00:08.452810 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
10:00:08.474597 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:00:20.912774 [1] proc begin: <DistEnv 1/4 nccl>
10:00:27.120358 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
10:00:27.137297 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:02:53.110518 [1] proc begin: <DistEnv 1/4 nccl>
10:02:59.038854 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
10:02:59.056583 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:04:12.738241 [1] proc begin: <DistEnv 1/4 nccl>
10:04:19.005666 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
10:04:19.022615 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:25:34.447474 [1] proc begin: <DistEnv 1/4 nccl>
10:25:39.764275 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
10:25:39.772600 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:25:44.599641 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:46.181974 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:46.954244 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:47.727169 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:48.496813 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:49.265737 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:50.035469 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:50.806923 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:51.577879 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:52.350178 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:53.122310 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:53.890656 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:54.662991 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:55.433581 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:56.204155 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:56.974339 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:57.743416 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:58.512290 [1] Warning: no training nodes in this partition! Backward fake loss.
10:25:59.281194 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:00.050762 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:00.818821 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:01.588410 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:02.390700 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:03.188468 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:03.956927 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:04.724865 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:05.493622 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:06.262013 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:07.031415 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:07.801111 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:08.570207 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:09.338039 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:10.107578 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:10.875734 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:11.645227 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:12.413485 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:13.182134 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:13.952233 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:14.722103 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:15.493894 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:16.263130 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:17.033688 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:17.803728 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:18.574027 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:19.344072 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:20.113950 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:20.883798 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:21.653713 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:22.424091 [1] Warning: no training nodes in this partition! Backward fake loss.
10:26:23.194679 [1] Warning: no training nodes in this partition! Backward fake loss.
10:27:50.550465 [1] proc begin: <DistEnv 1/4 nccl>
10:27:55.692475 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
10:27:55.700597 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:28:01.543997 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:04.194816 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:06.072267 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:07.949704 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:09.825373 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:11.698136 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:13.566370 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:15.433969 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:17.299080 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:19.164281 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:21.029538 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:22.896053 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:24.760787 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:26.625626 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:28.489610 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:30.353297 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:32.218551 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:34.081863 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:35.946381 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:37.810983 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:39.675907 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:41.538271 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:43.404836 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:45.268357 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:47.134061 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:48.998672 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:50.862332 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:52.728200 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:54.592796 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:56.461714 [1] Warning: no training nodes in this partition! Backward fake loss.
10:28:58.327445 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:00.193829 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:02.089688 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:03.987323 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:05.850726 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:07.719090 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:09.588382 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:11.455056 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:13.322155 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:15.185815 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:17.049035 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:18.912424 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:20.776410 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:22.641632 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:24.506025 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:26.371040 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:28.236074 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:30.099842 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:31.962837 [1] Warning: no training nodes in this partition! Backward fake loss.
10:29:33.828056 [1] Warning: no training nodes in this partition! Backward fake loss.
10:31:28.888050 [1] proc begin: <DistEnv 1/4 nccl>
10:31:33.804300 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
10:31:33.815083 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:31:40.350381 [1] Warning: no training nodes in this partition! Backward fake loss.
10:31:45.282122 [1] Warning: no training nodes in this partition! Backward fake loss.
10:31:49.354479 [1] Warning: no training nodes in this partition! Backward fake loss.
10:31:53.424729 [1] Warning: no training nodes in this partition! Backward fake loss.
10:31:57.491380 [1] Warning: no training nodes in this partition! Backward fake loss.
10:32:01.557000 [1] Warning: no training nodes in this partition! Backward fake loss.
10:32:05.691655 [1] Warning: no training nodes in this partition! Backward fake loss.
10:32:09.760892 [1] Warning: no training nodes in this partition! Backward fake loss.
10:32:13.830723 [1] Warning: no training nodes in this partition! Backward fake loss.
10:32:17.901118 [1] Warning: no training nodes in this partition! Backward fake loss.
10:32:21.967116 [1] Warning: no training nodes in this partition! Backward fake loss.
10:32:26.038700 [1] Warning: no training nodes in this partition! Backward fake loss.
10:32:30.103643 [1] Warning: no training nodes in this partition! Backward fake loss.
10:32:34.169052 [1] Warning: no training nodes in this partition! Backward fake loss.
10:32:38.235318 [1] Warning: no training nodes in this partition! Backward fake loss.
10:32:42.303039 [1] Warning: no training nodes in this partition! Backward fake loss.
10:32:46.370437 [1] Warning: no training nodes in this partition! Backward fake loss.
10:32:50.436806 [1] Warning: no training nodes in this partition! Backward fake loss.
10:32:54.503282 [1] Warning: no training nodes in this partition! Backward fake loss.
10:32:58.570982 [1] Warning: no training nodes in this partition! Backward fake loss.
10:33:02.694270 [1] Warning: no training nodes in this partition! Backward fake loss.
10:33:06.773004 [1] Warning: no training nodes in this partition! Backward fake loss.
10:33:10.843414 [1] Warning: no training nodes in this partition! Backward fake loss.
10:33:14.913184 [1] Warning: no training nodes in this partition! Backward fake loss.
10:33:18.982216 [1] Warning: no training nodes in this partition! Backward fake loss.
10:33:23.048529 [1] Warning: no training nodes in this partition! Backward fake loss.
10:33:27.115827 [1] Warning: no training nodes in this partition! Backward fake loss.
10:33:31.181938 [1] Warning: no training nodes in this partition! Backward fake loss.
10:33:35.247262 [1] Warning: no training nodes in this partition! Backward fake loss.
10:33:39.313237 [1] Warning: no training nodes in this partition! Backward fake loss.
10:33:43.379630 [1] Warning: no training nodes in this partition! Backward fake loss.
10:33:47.452609 [1] Warning: no training nodes in this partition! Backward fake loss.
10:33:51.519954 [1] Warning: no training nodes in this partition! Backward fake loss.
10:33:55.585642 [1] Warning: no training nodes in this partition! Backward fake loss.
10:33:59.654851 [1] Warning: no training nodes in this partition! Backward fake loss.
10:34:03.805932 [1] Warning: no training nodes in this partition! Backward fake loss.
10:34:07.882412 [1] Warning: no training nodes in this partition! Backward fake loss.
10:34:11.954054 [1] Warning: no training nodes in this partition! Backward fake loss.
10:34:16.024156 [1] Warning: no training nodes in this partition! Backward fake loss.
10:34:20.091851 [1] Warning: no training nodes in this partition! Backward fake loss.
10:34:24.159990 [1] Warning: no training nodes in this partition! Backward fake loss.
10:34:28.226989 [1] Warning: no training nodes in this partition! Backward fake loss.
10:34:32.294854 [1] Warning: no training nodes in this partition! Backward fake loss.
10:34:36.360798 [1] Warning: no training nodes in this partition! Backward fake loss.
10:34:40.429736 [1] Warning: no training nodes in this partition! Backward fake loss.
10:34:44.500393 [1] Warning: no training nodes in this partition! Backward fake loss.
10:34:48.565856 [1] Warning: no training nodes in this partition! Backward fake loss.
10:34:52.632694 [1] Warning: no training nodes in this partition! Backward fake loss.
10:34:56.697651 [1] Warning: no training nodes in this partition! Backward fake loss.
10:35:00.763364 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:42.905882 [1] proc begin: <DistEnv 1/4 nccl>
10:51:47.493338 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
10:51:47.501275 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:52:49.380833 [1] proc begin: <DistEnv 1/4 nccl>
10:52:54.322148 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
10:52:54.330672 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:53:50.034404 [1] proc begin: <DistEnv 1/4 nccl>
10:53:55.498604 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
10:53:55.507325 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:54:42.951093 [1] proc begin: <DistEnv 1/4 nccl>
10:54:47.859116 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
10:54:47.866932 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:56:02.129843 [1] proc begin: <DistEnv 1/4 nccl>
10:56:08.072648 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
10:56:08.086275 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:56:13.144136 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:14.579090 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:15.287751 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:15.993399 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:16.701436 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:17.408152 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:18.115852 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:18.823471 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:19.530126 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:20.238617 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:20.945351 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:21.652494 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:22.360269 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:23.068454 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:23.773726 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:24.479075 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:25.184572 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:25.889647 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:26.595876 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:27.300336 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:28.006590 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:28.714796 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:29.422151 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:30.128601 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:30.834861 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:31.540080 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:32.248622 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:32.954565 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:33.660937 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:34.367593 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:35.076311 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:35.783301 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:36.490875 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:37.197605 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:37.905841 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:38.613021 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:39.320738 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:40.028500 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:40.734766 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:41.441236 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:42.147561 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:42.854208 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:43.559273 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:44.264774 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:44.970303 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:45.676265 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:46.381317 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:47.089289 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:47.797228 [1] Warning: no training nodes in this partition! Backward fake loss.
10:56:48.502806 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:01.211116 [1] proc begin: <DistEnv 1/4 nccl>
10:58:06.643353 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
10:58:06.653213 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:00:00.493085 [1] proc begin: <DistEnv 1/4 nccl>
11:00:05.299432 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
11:00:05.307587 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:00:56.930128 [1] proc begin: <DistEnv 1/4 nccl>
11:01:03.057881 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
11:01:03.065531 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:02:09.723410 [1] proc begin: <DistEnv 1/4 nccl>
11:02:14.816681 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
11:02:14.824856 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:08:18.099517 [1] proc begin: <DistEnv 1/4 nccl>
11:08:23.230182 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
11:08:23.245490 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:09:49.662656 [1] proc begin: <DistEnv 1/4 nccl>
11:10:07.549850 [1] proc begin: <DistEnv 1/4 nccl>
11:10:11.996940 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
11:10:12.004613 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:20:41.989951 [1] proc begin: <DistEnv 1/4 nccl>
11:20:46.730059 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
11:20:46.737467 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:21:08.479763 [1] proc begin: <DistEnv 1/4 nccl>
11:21:13.440012 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
11:21:13.449939 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:22:41.702206 [1] proc begin: <DistEnv 1/4 nccl>
11:22:46.798186 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
11:22:46.807570 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:23:04.987902 [1] proc begin: <DistEnv 1/4 nccl>
11:23:10.363996 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
11:23:10.373294 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:23:41.287675 [1] proc begin: <DistEnv 1/4 nccl>
11:23:45.468124 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
11:23:45.475865 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:23:51.476193 [1] Warning: no training nodes in this partition! Backward fake loss.
11:23:54.064234 [1] Warning: no training nodes in this partition! Backward fake loss.
11:23:55.885283 [1] Warning: no training nodes in this partition! Backward fake loss.
11:23:57.707215 [1] Warning: no training nodes in this partition! Backward fake loss.
11:23:59.525713 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:01.342515 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:03.225460 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:05.045516 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:06.864348 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:08.690071 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:10.514064 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:12.336190 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:14.158034 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:15.978595 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:17.797547 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:19.616101 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:21.437079 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:23.256289 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:25.077732 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:26.898786 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:28.717440 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:30.534695 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:32.354303 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:34.174751 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:35.995426 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:37.631703 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:39.267602 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:40.900275 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:42.533619 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:44.169671 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:45.802636 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:47.435714 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:49.070085 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:50.706432 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:52.340467 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:53.975247 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:55.610463 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:57.246019 [1] Warning: no training nodes in this partition! Backward fake loss.
11:24:58.883461 [1] Warning: no training nodes in this partition! Backward fake loss.
11:25:00.518106 [1] Warning: no training nodes in this partition! Backward fake loss.
11:25:02.156414 [1] Warning: no training nodes in this partition! Backward fake loss.
11:25:03.849304 [1] Warning: no training nodes in this partition! Backward fake loss.
11:25:05.489448 [1] Warning: no training nodes in this partition! Backward fake loss.
11:25:07.130424 [1] Warning: no training nodes in this partition! Backward fake loss.
11:25:08.774323 [1] Warning: no training nodes in this partition! Backward fake loss.
11:25:10.417337 [1] Warning: no training nodes in this partition! Backward fake loss.
11:25:12.061014 [1] Warning: no training nodes in this partition! Backward fake loss.
11:25:13.702059 [1] Warning: no training nodes in this partition! Backward fake loss.
11:25:15.337706 [1] Warning: no training nodes in this partition! Backward fake loss.
11:25:16.972354 [1] Warning: no training nodes in this partition! Backward fake loss.
14:25:06.515387 [1] proc begin: <DistEnv 1/4 nccl>
14:25:11.069395 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:25:11.076810 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:25:18.069767 [1] Warning: no training nodes in this partition! Backward fake loss.
14:25:47.550016 [1] proc begin: <DistEnv 1/4 nccl>
14:25:52.521753 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:25:52.529821 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:25:59.484301 [1] Warning: no training nodes in this partition! Backward fake loss.
14:26:28.154211 [1] proc begin: <DistEnv 1/4 nccl>
14:26:33.927480 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:26:33.942122 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:26:38.160673 [1] Warning: no training nodes in this partition! Backward fake loss.
14:26:39.572462 [1] Warning: no training nodes in this partition! Backward fake loss.
14:26:40.277936 [1] Warning: no training nodes in this partition! Backward fake loss.
14:26:40.985514 [1] Warning: no training nodes in this partition! Backward fake loss.
14:26:49.994062 [1] proc begin: <DistEnv 1/4 nccl>
14:26:54.815627 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:26:54.824145 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:27:00.718777 [1] Warning: no training nodes in this partition! Backward fake loss.
14:27:02.286632 [1] Warning: no training nodes in this partition! Backward fake loss.
14:27:03.027459 [1] Warning: no training nodes in this partition! Backward fake loss.
14:27:03.739771 [1] Warning: no training nodes in this partition! Backward fake loss.
14:27:04.449297 [1] Warning: no training nodes in this partition! Backward fake loss.
14:27:05.159267 [1] Warning: no training nodes in this partition! Backward fake loss.
14:27:05.868894 [1] Warning: no training nodes in this partition! Backward fake loss.
14:27:06.580254 [1] Warning: no training nodes in this partition! Backward fake loss.
14:27:07.290891 [1] Warning: no training nodes in this partition! Backward fake loss.
14:27:08.000969 [1] Warning: no training nodes in this partition! Backward fake loss.
14:27:08.712700 [1] Warning: no training nodes in this partition! Backward fake loss.
14:27:09.424238 [1] Warning: no training nodes in this partition! Backward fake loss.
14:27:17.592401 [1] proc begin: <DistEnv 1/4 nccl>
14:27:24.790658 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
14:27:24.806711 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:06:35.907133 [1] proc begin: <DistEnv 1/4 nccl>
16:06:41.631466 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:06:41.649497 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:09:53.447474 [1] proc begin: <DistEnv 1/4 nccl>
16:09:58.973406 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:09:58.989976 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:11:41.317968 [1] proc begin: <DistEnv 1/4 nccl>
16:11:46.972829 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:11:46.986442 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:13:23.106417 [1] proc begin: <DistEnv 1/4 nccl>
16:13:23.196021 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
16:13:23.208591 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:13:24.507946 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.238821 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.252553 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.260587 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.268930 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.282647 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.291136 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.299462 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.314600 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.322489 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.333088 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.343778 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.354798 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.362499 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.379423 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.390294 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.398769 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.406400 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.415290 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.423436 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.431087 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.439046 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.447385 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.455571 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.465918 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.474116 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.487525 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.497227 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.506128 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.515033 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.524724 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.533868 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.542937 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.555133 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.564191 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.573529 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.582680 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.594520 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.603225 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.612512 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.623426 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.633026 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.642063 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.652711 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.667406 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.677868 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.687457 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.695334 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.703378 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.711257 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.719010 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.725376 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.733769 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.740523 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.749556 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.758059 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.770502 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.778086 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.787791 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.795357 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.805554 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.813415 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.823541 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.831283 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.841275 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.851373 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.862620 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.870240 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.879955 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.887909 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.896937 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.904585 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.914538 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.922184 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.931755 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.939536 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.948893 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.956685 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.966495 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.975294 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.987343 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.996477 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.008595 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.016238 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.026435 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.033750 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.051230 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.062928 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.072386 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.080271 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.089971 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.097331 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.113687 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.123312 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.143126 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.153874 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.166179 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.174115 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.183848 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.191729 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.201000 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.208528 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.218211 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.225724 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.238676 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.247001 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.257701 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.264894 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.275303 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.284836 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.299328 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.310280 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.322205 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.330573 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.341326 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.352595 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.363563 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.371499 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.381887 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.389216 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.399253 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.407027 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.416699 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.424580 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.437936 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.447085 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.458408 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.467146 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.481004 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.491186 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.504363 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.513745 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.524094 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.533456 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.557621 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.572691 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.582918 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.591341 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.604435 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.614957 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.624615 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.632261 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.642623 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.650197 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.660334 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.667894 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.678492 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.686299 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.697357 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.706495 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.719807 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.727969 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.740900 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.748161 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.758595 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.766776 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.780863 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.801508 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.812604 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.820665 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.834329 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.843590 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.854398 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.861918 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.873306 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.881050 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.895025 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.904326 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.913961 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.921860 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.932571 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.941197 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.951249 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.962971 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.974761 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.982692 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.994309 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.003042 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.015911 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.024174 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.034436 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.044680 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.057255 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.064963 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.075673 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.085935 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.095465 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.102977 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.118498 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.125283 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.139750 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.155805 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.167177 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.173781 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.182143 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.188180 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.196410 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.202782 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.211538 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.218124 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:41.600034 [1] proc begin: <DistEnv 1/4 nccl>
16:13:41.657634 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
16:13:41.670566 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:13:43.191444 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:43.976003 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:43.999842 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.022047 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.043421 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.067506 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.087313 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.106929 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.124276 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.147001 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.167803 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.190646 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.214551 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.234527 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.258313 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.277560 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.302951 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.326354 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.347216 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.371552 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.394644 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.417463 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.442119 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.466163 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.487008 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.508586 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.531210 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.554332 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.582352 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.603922 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.621800 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.646793 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.667645 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.694086 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.721032 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.748067 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.770991 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.794202 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.818781 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.839280 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.858702 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.879068 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.899212 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.915708 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.938250 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.970511 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.995059 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.017794 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.043449 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.067226 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.091322 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.117410 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.145564 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.170486 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.189894 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.211690 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.234497 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.255308 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.278670 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.295988 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.315886 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.334518 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.352263 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.375160 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.391981 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.414733 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.435273 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.453950 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.477668 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.502600 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.525778 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.546110 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.581504 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.597776 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.629417 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.652313 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.672112 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.694482 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.713925 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.740919 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.762522 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.788526 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.810706 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.830981 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.854219 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.872787 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.896210 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.918427 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.947231 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.970661 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.992210 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.014399 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.042206 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.062857 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.089722 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.110268 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.134330 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.158783 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.182059 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.205475 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.226958 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.252975 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.272610 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.295471 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.322253 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.340963 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.363352 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.385798 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.405016 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.430913 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.454518 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.481738 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.510909 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.538342 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.564205 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.590587 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.616705 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.644512 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.667791 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.688308 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.711896 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.737647 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.762351 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.783774 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.806646 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.827296 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.850821 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.869079 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.891891 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.912959 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.934013 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.958182 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.981029 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.005929 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.025787 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.049899 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.074991 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.098250 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.122715 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.146130 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.172154 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.198036 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.223082 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.247021 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.268738 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.290051 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.312658 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.338513 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.360779 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.378272 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.399917 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.422542 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.446602 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.470872 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.495195 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.518706 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.541236 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.562779 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.593675 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.621675 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.649887 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.679464 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.710192 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.733942 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.754970 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.777273 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.797827 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.817219 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.838878 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.862367 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.882163 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.905907 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.927457 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.949862 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.970609 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.991025 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.010248 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.033485 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.054038 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.075217 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.098909 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.116249 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.138683 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.158570 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.178220 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.201601 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.225274 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.238532 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.250859 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.264123 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.276666 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.293760 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.309963 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.327195 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.350364 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.374528 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.398812 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.422678 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.446937 [1] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.471097 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:06.533403 [1] proc begin: <DistEnv 1/4 nccl>
16:14:06.569309 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
16:14:06.581847 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:14:08.019704 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:08.860399 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:08.901110 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:08.933974 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:08.970013 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.006584 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.039681 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.072690 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.105043 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.132165 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.161267 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.192919 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.227430 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.260510 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.287864 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.319787 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.352554 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.386312 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.421891 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.456337 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.488308 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.523416 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.561410 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.591433 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.624262 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.659942 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.691484 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.724846 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.755100 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.787398 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.820999 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.854930 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.885467 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.920144 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.951148 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.982363 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.016298 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.047975 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.083158 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.120136 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.156473 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.193263 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.225100 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.264153 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.300533 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.337310 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.372025 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.419248 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.462367 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.507899 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.550320 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.585829 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.623460 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.656798 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.692750 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.727233 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.754504 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.782054 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.810911 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.843333 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.876109 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.910678 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.941765 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.968120 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.994787 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.024428 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.056891 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.092410 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.123687 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.155224 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.192808 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.226662 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.259034 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.287323 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.320816 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.352168 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.382208 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.420495 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.454816 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.488681 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.526734 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.570991 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.603927 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.635938 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.668357 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.699883 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.743020 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.774466 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.807827 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.838796 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.872119 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.898404 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.931519 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.958571 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.982592 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.015721 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.047389 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.079180 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.113406 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.149012 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.190335 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.223919 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.256057 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.288049 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.324172 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.356273 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.392208 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.430874 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.463276 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.500373 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.543179 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.588234 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.621542 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.655158 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.683620 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.716521 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.747756 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.783694 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.820985 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.856927 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.894898 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.924987 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.962521 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.995715 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.032973 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.070403 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.105212 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.136033 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.172711 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.206894 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.237819 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.270774 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.304319 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.334868 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.365007 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.397315 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.434993 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.463142 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.494886 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.537487 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.580345 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.611269 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.645360 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.672168 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.703440 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.735687 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.767711 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.795587 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.827376 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.861549 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.895500 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.927817 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.954104 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.986521 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.016176 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.047938 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.080035 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.115856 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.153507 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.189873 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.227106 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.264482 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.301819 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.337403 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.375265 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.407766 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.446869 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.478516 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.513503 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.551175 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.597294 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.628124 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.656349 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.689412 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.726802 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.762712 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.795843 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.831264 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.863079 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.897984 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.930834 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.962634 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.997639 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.030213 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.066421 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.103776 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.139249 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.173453 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.209521 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.248902 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.286678 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.323086 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.357767 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.394205 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.430245 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.477011 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.512392 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.557263 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.603667 [1] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.635674 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:25.128407 [1] proc begin: <DistEnv 1/4 nccl>
16:15:29.996142 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
16:15:30.003880 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:15:34.526118 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:36.058753 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:36.768409 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:37.477975 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:38.185700 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:38.893737 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:39.602248 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:40.313210 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:41.021799 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:41.730665 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:42.438188 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:43.146576 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:43.855011 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:44.562552 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:45.271156 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:45.979820 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:46.688918 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:47.397590 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:48.104787 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:48.814788 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:49.521925 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:50.231183 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:50.938956 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:51.646904 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:52.355281 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:53.062705 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:53.769739 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:54.477109 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:55.185262 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:55.895183 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:56.603514 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:57.311703 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:58.019740 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:58.729108 [1] Warning: no training nodes in this partition! Backward fake loss.
16:15:59.437684 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:00.145603 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:00.852848 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:01.574620 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:02.314331 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:03.037760 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:03.746416 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:04.453822 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:05.161258 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:05.870772 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:06.579240 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:07.287479 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:07.994965 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:08.701883 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:09.411213 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:10.118973 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:10.827345 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:11.296262 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:12.053986 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:12.523498 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:13.280913 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:13.748100 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:14.504664 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:14.972369 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:15.730360 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:16.197893 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:16.956688 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:17.425292 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:18.183822 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:18.652800 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:19.410331 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:19.878795 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:20.637115 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:21.104833 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:21.861401 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:22.328941 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:23.086055 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:23.554082 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:24.310619 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:24.778412 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:25.536449 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:26.004004 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:26.761303 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:27.228782 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:27.985775 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:28.453609 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:29.210895 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:29.678918 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:30.436420 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:30.904656 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:31.662146 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:32.130639 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:32.890512 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:33.359792 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:34.118397 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:34.587032 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:35.343862 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:35.812832 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:36.569689 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:37.037784 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:37.794757 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:38.263067 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:39.021847 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:39.491234 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:40.248771 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:40.718952 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:41.476826 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:41.946253 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:42.704850 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:43.174504 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:43.931487 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:44.401269 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:45.158986 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:45.627752 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:46.383564 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:46.852680 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:47.610666 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:48.079401 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:48.836422 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:49.304346 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:50.062173 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:50.530477 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:51.288584 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:51.756827 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:52.517385 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:52.989468 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:53.748676 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:54.216982 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:54.973356 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:55.441588 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:56.198506 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:56.666997 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:57.424972 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:57.892597 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:58.649622 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:59.118226 [1] Warning: no training nodes in this partition! Backward fake loss.
16:16:59.875260 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:00.344159 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:01.101293 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:01.570083 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:02.347596 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:02.836016 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:03.611829 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:04.080848 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:04.838086 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:05.307882 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:06.064178 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:06.533249 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:07.290838 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:07.759811 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:08.516564 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:08.985269 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:09.742567 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:10.211084 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:10.966842 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:11.433931 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:12.191433 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:12.658646 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:13.413725 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:13.881188 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:14.637527 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:15.105905 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:15.861637 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:16.329504 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:17.085887 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:17.553491 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:18.309125 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:18.776353 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:19.531291 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:19.999826 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:20.755833 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:21.223040 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:21.979177 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:22.446936 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:23.203196 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:23.671565 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:24.427477 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:24.895142 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:25.651368 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:26.118979 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:26.874423 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:27.342435 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:28.096977 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:28.564687 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:29.321076 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:29.788198 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:30.544222 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:31.011946 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:31.768857 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:32.238378 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:32.993993 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:33.461272 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:34.217719 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:34.684775 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:35.440793 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:35.908642 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:36.664541 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:37.132844 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:37.889360 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:38.356559 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:39.111839 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:39.578901 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:40.335268 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:40.802638 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:41.558202 [1] Warning: no training nodes in this partition! Backward fake loss.
16:17:42.025400 [1] Warning: no training nodes in this partition! Backward fake loss.
09:20:39.611653 [1] proc begin: <DistEnv 1/4 nccl>
09:20:50.566512 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:20:50.600061 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:22:47.476164 [1] proc begin: <DistEnv 1/4 nccl>
09:22:54.984497 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:22:55.026928 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:25:19.156106 [1] proc begin: <DistEnv 1/4 nccl>
09:25:26.498775 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:25:26.527986 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:26:04.947417 [1] proc begin: <DistEnv 1/4 nccl>
09:26:11.211746 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:26:11.249418 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:26:53.521397 [1] proc begin: <DistEnv 1/4 nccl>
09:27:00.428728 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:27:00.463900 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:27:26.275814 [1] proc begin: <DistEnv 1/4 nccl>
09:27:32.517415 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:27:32.549723 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:13:20.392828 [1] proc begin: <DistEnv 1/4 nccl>
14:13:37.544508 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:13:37.553064 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:13:49.380575 [1] Warning: no training nodes in this partition! Backward fake loss.
14:14:53.239356 [1] proc begin: <DistEnv 1/4 nccl>
14:15:00.637752 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:15:00.640696 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  467269 KB |  505541 KB |  764309 KB |  297040 KB |
|       from large pool |  467269 KB |  505541 KB |  764286 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| Active memory         |  467269 KB |  505541 KB |  764309 KB |  297040 KB |
|       from large pool |  467269 KB |  505541 KB |  764286 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  522240 KB |  522240 KB |  522240 KB |       0 B  |
|       from large pool |  520192 KB |  520192 KB |  520192 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   52922 KB |  216906 KB |  301631 KB |  248708 KB |
|       from large pool |   52922 KB |  216906 KB |  279090 KB |  226168 KB |
|       from small pool |       0 KB |    2047 KB |   22540 KB |   22540 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |      14    |      11    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |      11    |      11    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:15:08.054916 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:10.029736 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:11.890352 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:13.746642 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:15.599052 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:17.457419 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:19.305331 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:21.155801 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:23.007641 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:24.859948 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:26.707659 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:28.558071 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:30.412177 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:32.266780 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:34.126201 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:35.973168 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:37.828969 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:39.683765 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:41.536217 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:43.389107 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:45.242050 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:47.092108 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:48.941041 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:50.796359 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:52.645267 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:54.496809 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:56.351123 [1] Warning: no training nodes in this partition! Backward fake loss.
14:15:58.210059 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:00.067211 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:01.935905 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:03.846437 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:05.699742 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:07.557359 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:09.413483 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:11.271433 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:13.128168 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:14.980232 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:16.830716 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:18.680183 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:20.535016 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:22.387884 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:24.236595 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:26.089606 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:27.943837 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:29.796623 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:31.647133 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:33.501342 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:35.355610 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:37.209631 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:39.063714 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:40.921641 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:42.593595 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:44.450426 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:46.118756 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:47.974968 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:49.644159 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:51.503294 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:53.171665 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:55.022222 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:56.682404 [1] Warning: no training nodes in this partition! Backward fake loss.
14:16:58.531444 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:00.197436 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:02.082895 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:03.789187 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:05.646911 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:07.321635 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:09.178701 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:10.850257 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:12.712557 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:14.388583 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:16.247419 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:17.919263 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:19.783513 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:21.458871 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:23.312954 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:24.980533 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:26.838735 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:28.511629 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:30.369324 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:32.038131 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:33.893333 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:35.566924 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:37.430469 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:39.101084 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:40.957232 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:42.627341 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:44.484516 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:46.158198 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:48.012623 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:49.680232 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:51.534348 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:53.205605 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:55.058175 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:56.731155 [1] Warning: no training nodes in this partition! Backward fake loss.
14:17:58.590375 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:00.262639 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:02.127602 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:03.857251 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:05.720919 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:07.391836 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:09.250593 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:10.926243 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:12.785306 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:14.453496 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:16.312280 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:17.982321 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:19.838761 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:21.512864 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:23.371500 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:25.038517 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:26.894582 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:28.563687 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:30.421106 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:32.091886 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:33.945732 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:35.619625 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:37.476435 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:39.148713 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:41.007038 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:42.676148 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:44.535537 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:46.202949 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:48.064646 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:49.731913 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:51.585466 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:53.250475 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:55.107687 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:56.777236 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:58.641350 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:00.318779 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:02.202298 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:03.919539 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:05.776905 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:07.454290 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:09.313419 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:10.983317 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:12.838251 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:14.517426 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:16.377644 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:18.052719 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:19.910039 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:21.584786 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:23.443666 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:25.113276 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:26.969706 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:28.641809 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:30.495738 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:32.167597 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:34.024541 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:35.693828 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:37.549449 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:39.216758 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:41.072938 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:42.746301 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:44.608484 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:46.278396 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:48.142186 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:49.809634 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:51.664281 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:53.330995 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:55.184478 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:56.854223 [1] Warning: no training nodes in this partition! Backward fake loss.
14:19:58.708877 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:00.385444 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:02.289792 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:03.982073 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:05.842048 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:07.516761 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:09.377041 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:11.050640 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:12.906699 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:14.578004 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:16.436523 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:18.108500 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:19.962933 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:21.628723 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:23.480808 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:25.146837 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:27.001993 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:28.670890 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:30.525790 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:32.192765 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:34.050405 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:35.719393 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:37.580887 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:39.250210 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:41.104109 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:42.774180 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:44.629244 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:46.298979 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:48.153626 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:49.820654 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:51.673054 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:53.350127 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:55.209830 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:56.881107 [1] Warning: no training nodes in this partition! Backward fake loss.
14:20:58.740250 [1] Warning: no training nodes in this partition! Backward fake loss.
14:21:00.419379 [1] Warning: no training nodes in this partition! Backward fake loss.
14:21:02.297659 [1] Warning: no training nodes in this partition! Backward fake loss.
14:21:04.022475 [1] Warning: no training nodes in this partition! Backward fake loss.
14:22:14.901886 [1] proc begin: <DistEnv 1/4 nccl>
14:22:19.227804 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:22:19.234858 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:22:24.461922 [1] Warning: no training nodes in this partition! Backward fake loss.
14:22:27.008611 [1] Warning: no training nodes in this partition! Backward fake loss.
14:22:28.837426 [1] Warning: no training nodes in this partition! Backward fake loss.
14:22:30.670201 [1] Warning: no training nodes in this partition! Backward fake loss.
14:22:32.502109 [1] Warning: no training nodes in this partition! Backward fake loss.
14:25:50.294816 [1] proc begin: <DistEnv 1/4 nccl>
14:25:54.796412 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:25:54.804409 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:26:01.872277 [1] Warning: no training nodes in this partition! Backward fake loss.
14:26:25.640589 [1] proc begin: <DistEnv 1/4 nccl>
14:26:32.508181 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:26:32.511818 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  467269 KB |  505541 KB |  764309 KB |  297040 KB |
|       from large pool |  467269 KB |  505541 KB |  764286 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| Active memory         |  467269 KB |  505541 KB |  764309 KB |  297040 KB |
|       from large pool |  467269 KB |  505541 KB |  764286 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  522240 KB |  522240 KB |  522240 KB |       0 B  |
|       from large pool |  520192 KB |  520192 KB |  520192 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   52922 KB |  216906 KB |  301631 KB |  248708 KB |
|       from large pool |   52922 KB |  216906 KB |  279090 KB |  226168 KB |
|       from small pool |       0 KB |    2047 KB |   22540 KB |   22540 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |      14    |      11    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |      11    |      11    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:00:25.428263 [1] proc begin: <DistEnv 1/4 nccl>
15:00:25.589071 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
15:00:25.598905 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:00:26.827229 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.561184 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.572621 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.589893 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.600494 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.609059 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.616357 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.624259 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.631735 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.643421 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.651674 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.659240 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.666516 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.673991 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.681435 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.691104 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.699149 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.707274 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.714326 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.721315 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.730490 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.737877 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.747898 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.756265 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.766250 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.775206 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.783956 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.791288 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.798743 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.806268 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.814622 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.822610 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.831301 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.841733 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.850904 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.858645 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.870483 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.878003 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.885514 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.892461 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.900264 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.908169 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.915646 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.922672 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.930246 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.937899 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.945307 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.953952 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.961159 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.968020 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:56.003043 [1] proc begin: <DistEnv 1/4 nccl>
15:01:56.057099 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
15:01:56.066597 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:01:58.212676 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.925506 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.936063 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.943579 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.951410 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.958945 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.966176 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.973001 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.980364 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.988866 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.996290 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.003964 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.011679 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.019418 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.029870 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.038722 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.049216 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.058419 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.066142 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.078536 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.087011 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.094418 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.102048 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.109515 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.116332 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.125780 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.135753 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.146234 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.155977 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.163369 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.171219 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.178336 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.185332 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.192636 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.200294 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.207121 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.214156 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.221406 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.228649 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.236889 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.244239 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.251762 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.258947 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.266608 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.274370 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.280889 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.288340 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.296099 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.303384 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.310816 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:49.222956 [1] proc begin: <DistEnv 1/4 nccl>
15:02:49.284055 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
15:02:49.300899 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:02:50.736261 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.470465 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.494318 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.515461 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.537559 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.557062 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.583992 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.607377 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.625441 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.646773 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.665394 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.686535 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.709484 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.728873 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.749279 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.762102 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.782034 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.797923 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.819537 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.840997 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.859102 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.876291 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.898501 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.915415 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.930091 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.939981 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.951114 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.966879 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.984309 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.005803 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.024398 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.045670 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.066030 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.085439 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.105502 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.126153 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.147831 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.168238 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.182921 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.193252 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.204993 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.227133 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.243079 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.255313 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.266229 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.279139 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.290591 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.305340 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.316614 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.328232 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:09.267135 [1] proc begin: <DistEnv 1/4 nccl>
21:53:23.482046 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
21:53:23.504177 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:06:22.595674 [1] proc begin: <DistEnv 1/4 nccl>
22:06:45.477641 [1] proc begin: <DistEnv 1/4 nccl>
22:06:51.819667 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
22:06:51.840242 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:08:01.996929 [1] proc begin: <DistEnv 1/4 nccl>
22:08:07.777317 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
22:08:07.794406 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:09:31.288224 [1] proc begin: <DistEnv 1/4 nccl>
22:09:38.092447 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
22:09:38.109135 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:09:52.700319 [1] proc begin: <DistEnv 1/4 nccl>
22:09:57.227467 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
22:09:57.246152 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:10:56.410359 [1] proc begin: <DistEnv 1/4 nccl>
22:11:03.500607 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
22:11:03.518721 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:12:18.023568 [1] proc begin: <DistEnv 1/4 nccl>
22:12:23.647207 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
22:12:23.663723 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:12:59.076329 [1] proc begin: <DistEnv 1/4 nccl>
22:13:05.176120 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
22:13:05.193463 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:51:33.230097 [1] proc begin: <DistEnv 1/4 nccl>
14:51:33.328181 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
14:51:33.335882 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:52:06.414369 [1] proc begin: <DistEnv 1/4 nccl>
14:52:06.476648 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
14:52:06.485984 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:52:48.891636 [1] proc begin: <DistEnv 1/4 nccl>
14:52:48.956280 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
14:52:48.972960 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:53:06.960032 [1] proc begin: <DistEnv 1/4 nccl>
14:53:07.030529 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
14:53:07.039857 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:53:56.049160 [1] proc begin: <DistEnv 1/4 nccl>
14:53:56.077218 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
14:53:56.086593 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:55:22.018135 [1] proc begin: <DistEnv 1/4 nccl>
14:55:22.083562 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
14:55:22.092730 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:55:23.406303 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.254420 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.303074 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.361081 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.392875 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.403086 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.414547 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.423945 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.430636 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.438627 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.445602 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.451787 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.459786 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.467210 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.473594 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.481206 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.488509 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.495113 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.502728 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.511074 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.518991 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.540808 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.650272 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.719410 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.826153 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.913001 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.941954 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.987345 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.029945 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.050275 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.113896 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.155858 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.178355 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.240134 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.284795 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.304833 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.371665 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.415028 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.436118 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.499129 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.534746 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.560672 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.626364 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.667764 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.688872 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.753811 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.789211 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.815151 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.825190 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.837451 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.846050 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.854770 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.862382 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.868921 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.876356 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.886382 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.892627 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.899803 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.912253 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.919480 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.927472 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.934809 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.940550 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.953464 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.961858 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.969272 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.981388 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.991885 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.997972 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.006237 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.013497 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.020449 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.028207 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.036093 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.044268 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.052463 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.059688 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.065832 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.073672 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.161765 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.237293 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.430553 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.580782 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.716719 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.885047 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.076692 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.225944 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.392933 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.589258 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.747355 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.927944 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.086395 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.226775 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.385029 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.527915 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.608974 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.802789 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.934441 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.004575 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.189028 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.231446 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.261323 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.327078 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.376355 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.406047 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.473370 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.521901 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.550919 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.618503 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.652896 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.697114 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.765081 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.813191 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.841495 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.907744 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.954861 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.984099 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.049847 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.098177 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.126645 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.192804 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.240660 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.268653 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.335362 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.384102 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.413756 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.479024 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.526349 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.555437 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.621270 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.668326 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.697887 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.763336 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.812050 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.838442 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.906191 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.953274 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.982498 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.048420 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.096007 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.125756 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.190856 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.239490 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.267878 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.334935 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.366827 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.376080 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.394362 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.404613 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.410795 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.418558 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.425211 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.431212 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.438923 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.446805 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.453929 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.461356 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.469136 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.475667 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.483345 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.491363 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.501079 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.512832 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.522372 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.528717 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.536336 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.544042 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.551920 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.559479 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.567120 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.573549 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.583639 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.593643 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.600431 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.609309 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.617462 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.624206 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.632607 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.640412 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.646736 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.655062 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.662636 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.669112 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.677540 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.685272 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.692058 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.701850 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.709544 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.715553 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.726697 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.734752 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.740732 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.750513 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.760798 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.767645 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.841525 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.967840 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:32.043041 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:32.154357 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:32.198049 [1] Warning: no training nodes in this partition! Backward fake loss.
21:50:15.954327 [1] proc begin: <DistEnv 1/4 nccl>
21:50:34.530086 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
21:50:34.569279 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:50:51.677805 [1] Warning: no training nodes in this partition! Backward fake loss.
21:50:53.166654 [1] Warning: no training nodes in this partition! Backward fake loss.
21:50:53.638636 [1] Warning: no training nodes in this partition! Backward fake loss.
21:50:54.402798 [1] Warning: no training nodes in this partition! Backward fake loss.
21:50:55.115201 [1] Warning: no training nodes in this partition! Backward fake loss.
21:50:55.586491 [1] Warning: no training nodes in this partition! Backward fake loss.
21:50:56.349003 [1] Warning: no training nodes in this partition! Backward fake loss.
21:50:57.060486 [1] Warning: no training nodes in this partition! Backward fake loss.
21:50:57.534931 [1] Warning: no training nodes in this partition! Backward fake loss.
21:50:58.300802 [1] Warning: no training nodes in this partition! Backward fake loss.
21:50:59.014829 [1] Warning: no training nodes in this partition! Backward fake loss.
21:50:59.489755 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:00.254036 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:00.966110 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:01.442064 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:02.227702 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:02.970549 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:03.452488 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:04.218413 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:04.930384 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:05.406631 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:06.170466 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:06.882828 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:07.358369 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:08.122828 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:08.834627 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:09.307463 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:10.072831 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:10.784074 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:11.257762 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:12.024070 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:12.736787 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:13.210484 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:13.975046 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:14.688027 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:15.162035 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:15.926432 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:16.638828 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:17.112029 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:17.921598 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:18.705797 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:19.218233 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:20.040579 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:20.808025 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:21.280489 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:22.047932 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:22.938549 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:23.872014 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:25.032401 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:26.262932 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:26.932061 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:27.836844 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:28.594071 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:29.095289 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:29.912825 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:30.668729 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:31.179822 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:31.975393 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:32.738306 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:33.244962 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:34.041086 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:34.802436 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:35.306143 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:36.099344 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:36.856228 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:37.359220 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:38.151554 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:38.911063 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:39.416208 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:40.210174 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:40.967742 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:41.468278 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:42.253275 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:43.005976 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:43.505869 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:44.289546 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:45.040288 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:45.537431 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:46.323023 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:47.076144 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:47.574429 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:48.359586 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:49.110326 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:49.607529 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:50.393109 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:51.145917 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:51.641613 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:52.429494 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:53.181367 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:53.675092 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:54.457551 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:55.197305 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:55.691610 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:56.500296 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:57.230332 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:57.718027 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:58.528121 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:59.269464 [1] Warning: no training nodes in this partition! Backward fake loss.
21:51:59.754322 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:00.575585 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:01.329475 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:01.849188 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:02.689746 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:03.456960 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:03.959173 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:04.744842 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:05.499149 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:06.001407 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:06.792109 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:07.549637 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:08.051627 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:08.834211 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:09.587826 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:10.089455 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:10.875270 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:11.627480 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:12.129257 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:12.915281 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:13.669515 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:14.171130 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:14.954518 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:15.705747 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:16.204924 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:16.990246 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:17.742706 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:18.245332 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:19.028888 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:19.782315 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:20.284735 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:21.067870 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:21.822081 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:22.318368 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:23.105034 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:23.857737 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:24.355502 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:25.143698 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:25.896540 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:26.393961 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:27.179751 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:27.932233 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:28.434095 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:29.218649 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:29.973167 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:30.471161 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:31.255685 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:32.005981 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:32.506598 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:33.290157 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:34.040977 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:34.535911 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:35.319551 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:36.073741 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:36.571644 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:37.358432 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:38.110134 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:38.609906 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:39.394347 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:40.147784 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:40.645298 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:41.430377 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:42.186394 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:42.683434 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:43.468367 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:44.219646 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:44.714571 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:45.500543 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:46.253043 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:46.750948 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:47.536280 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:48.287150 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:48.786620 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:49.572559 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:50.324909 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:50.824168 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:51.609632 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:52.364772 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:52.865620 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:53.650976 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:54.404258 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:54.902137 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:55.692337 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:56.443407 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:56.936656 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:57.721283 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:58.474569 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:58.968362 [1] Warning: no training nodes in this partition! Backward fake loss.
21:52:59.755871 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:00.509687 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:01.006976 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:01.793504 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:02.565511 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:03.076299 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:03.899355 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:04.631904 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:05.127080 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:05.936464 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:06.670420 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:07.165974 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:07.976008 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:08.709870 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:58.976336 [1] proc begin: <DistEnv 1/4 nccl>
21:55:59.113804 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
21:55:59.126122 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:56:00.668565 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.412905 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.423263 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.435824 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.443803 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.450132 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.458469 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.467297 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.475498 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.483995 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.491550 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.497679 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.505706 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.514284 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.520754 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.528759 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.536466 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.543221 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.551868 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.559138 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.565352 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.573171 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.580670 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.587506 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.595272 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.602913 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.609350 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.617901 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.625216 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.631249 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.639976 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.647814 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.654200 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.666292 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.676766 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.686058 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.694691 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.703056 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.710395 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.720577 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.732098 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.739412 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.747396 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.761465 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.769397 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.779500 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.790032 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.799153 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.807527 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.815209 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.823274 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.831131 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.840081 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.846795 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.856567 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.868275 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.875809 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.885272 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.893649 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.900432 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.910439 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.918837 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.926348 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.937579 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.946325 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.954065 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.964812 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.975261 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.983006 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.995982 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.007708 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.014789 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.026603 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.034732 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.044523 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.053339 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.062329 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.070219 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.079262 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.091394 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.099257 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.110783 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.122340 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.131047 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.139674 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.147935 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.155540 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.166300 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.175585 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.183513 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.192751 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.202883 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.211441 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.219787 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.228020 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.236747 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.244484 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.252744 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.260731 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.269363 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.277385 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.285662 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.294260 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.302669 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.310553 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.320111 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.328974 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.336900 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.346298 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.355276 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.364000 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.371740 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.379867 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.387112 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.395618 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.404364 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.413696 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.422667 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.436419 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.444421 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.453973 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.462811 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.469644 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.479169 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.486605 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.493840 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.502893 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.511483 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.519453 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.528086 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.537739 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.545513 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.554678 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.562937 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.570443 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.579622 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.588297 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.596575 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.606792 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.614865 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.623044 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.633069 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.641813 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.653257 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.661509 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.669903 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.676474 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.685866 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.695064 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.701905 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.711391 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.719114 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.726494 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.736009 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.747150 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.755287 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.764685 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.774071 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.781664 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.791040 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.799253 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.806530 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.815437 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.823582 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.830609 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.842519 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.854274 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.862461 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.870446 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.880138 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.887524 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.896794 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.906663 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.914212 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.924933 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.934258 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.942298 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.950687 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.959801 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.967217 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.979904 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.988986 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.997892 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.006712 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.014016 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.020948 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.030914 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.040676 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.051519 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.061469 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.070208 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.076984 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.087202 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.095691 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.104021 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.115403 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.128436 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.134927 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.144140 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.151837 [1] Warning: no training nodes in this partition! Backward fake loss.
21:58:19.430994 [1] proc begin: <DistEnv 1/4 nccl>
21:58:35.194557 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
21:58:35.211757 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:49:07.086396 [1] proc begin: <DistEnv 1/4 nccl>
22:49:11.975742 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
22:49:11.981303 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3859 KB |    3881 KB |    3932 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      69 KB |      91 KB |     142 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3859 KB |    3881 KB |    3932 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      69 KB |      91 KB |     142 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18668 KB |   18707 KB |   18808 KB |  142848 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1978 KB |    2045 KB |    2118 KB |  142848 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:49:54.822215 [1] proc begin: <DistEnv 1/4 nccl>
22:49:56.347670 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
22:49:56.348596 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3859 KB |    3881 KB |    3932 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      69 KB |      91 KB |     142 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3859 KB |    3881 KB |    3932 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      69 KB |      91 KB |     142 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18668 KB |   18707 KB |   18808 KB |  142848 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1978 KB |    2045 KB |    2118 KB |  142848 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:50:02.039031 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.376334 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.410560 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.439927 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.471520 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.503103 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.534916 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.570406 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.608181 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.638150 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.666844 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.697951 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.732454 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.765144 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.802042 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.837893 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.871994 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.909035 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.942307 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.976683 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.011639 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.045346 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.076627 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.112527 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.143473 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.181920 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.213662 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.248001 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.283108 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.310602 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.346147 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.378371 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.413384 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.448725 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.479339 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.515065 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.546485 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.583636 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.620616 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.650132 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.682177 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.715408 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.747619 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.779323 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.815816 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.850427 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.888138 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.922676 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.952732 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.984871 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.017922 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.055988 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.086083 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.118939 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.152217 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.181922 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.212123 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.245797 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.277402 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.308802 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.342945 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.372796 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.404909 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.431054 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.466136 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.496715 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.531125 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.563200 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.596559 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.628479 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.662643 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.693517 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.727203 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.757639 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.787882 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.825977 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.856261 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.889810 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.920709 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.953433 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.984818 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.020838 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.053567 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.086964 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.119851 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.149695 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.181303 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.214436 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.248917 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.281908 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.315164 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.348473 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.377214 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.409721 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.442523 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.476732 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.508518 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.535861 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.569657 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.604760 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.634757 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.667731 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.698413 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.729395 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.761541 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.792818 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.826770 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.856683 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.893237 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.925210 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.960826 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.996251 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.033220 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.063892 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.096215 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.129596 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.162796 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.192231 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.230511 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.262121 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.293030 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.325465 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.355508 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.388190 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.417781 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.448985 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.482846 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.515394 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.544591 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.576877 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.609797 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.640180 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.669183 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.704906 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.737572 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.768859 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.805866 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.840018 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.873049 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.910856 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.940637 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.968910 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.005704 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.039282 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.066820 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.099772 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.132119 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.167547 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.201404 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.236979 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.268445 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.299601 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.329453 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.362087 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.390230 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.420564 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.453825 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.484469 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.515823 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.549712 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.582492 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.615192 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.648261 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.681219 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.712523 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.746884 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.776657 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.806636 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.839740 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.874492 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.908537 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.942115 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.978889 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.008100 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.043264 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.073108 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.105945 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.139919 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.172376 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.204651 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.235650 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.266518 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.297627 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.332699 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.369048 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.400751 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.432456 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.469486 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.500244 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.534345 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.565819 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.603636 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.637876 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.671492 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.703591 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.736207 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.765954 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.797954 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.831187 [1] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.862559 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:24.931452 [1] proc begin: <DistEnv 1/4 nccl>
10:38:26.452013 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
10:38:26.453172 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3859 KB |    3881 KB |    3932 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      69 KB |      91 KB |     142 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3859 KB |    3881 KB |    3932 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      69 KB |      91 KB |     142 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18668 KB |   18707 KB |   18808 KB |  142848 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1978 KB |    2045 KB |    2118 KB |  142848 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:38:28.162628 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.299905 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.331386 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.368125 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.401946 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.433205 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.464657 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.492798 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.523450 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.559165 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.587653 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.621290 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.653663 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.697788 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.731031 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.760534 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.804447 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.837300 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.871826 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.912361 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.944348 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.978442 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.027953 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.139387 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.175688 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.211681 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.246176 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.283771 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.320645 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.355967 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.391887 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.425977 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.462104 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.501292 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.537532 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.953034 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.985520 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.017311 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.054976 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.089246 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.123375 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.164322 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.196941 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.225708 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.258789 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.296870 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.331545 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.361619 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.393086 [1] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.428411 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:23.788392 [1] proc begin: <DistEnv 1/4 nccl>
10:48:23.896755 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
10:48:23.906215 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:48:25.205169 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.887785 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.899271 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.906345 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.913172 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.919411 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.926789 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.936115 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.944745 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.953352 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.961528 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.969860 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.979844 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.988289 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.997626 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.005872 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.013071 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.023929 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.037303 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.049801 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.061750 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.068773 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.080048 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.088132 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.098213 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.105750 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.112247 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.119488 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.126990 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.135682 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.144690 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.154025 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.160823 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.168200 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.175427 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.182589 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.190957 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.197745 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.204897 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.211667 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.219778 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.228594 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.235290 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.242889 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.249823 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.256067 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.263963 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.273769 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.282799 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.291071 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.301090 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.310122 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.317315 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.324331 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.332037 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.342148 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.349115 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.358684 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.366037 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.372611 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.379683 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.389900 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.397489 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.404976 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.414024 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.421193 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.428416 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.435738 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.442950 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.453079 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.459965 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.472194 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.485990 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.496179 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.506147 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.517792 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.529036 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.536969 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.545846 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.552699 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.560061 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.567580 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.575408 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.586753 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.595119 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.602828 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.610133 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.617412 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.624588 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.634259 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.641586 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.649110 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.656536 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.663664 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.675079 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.684951 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.692050 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.701202 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.708486 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.715517 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.723093 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.731421 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.741507 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.748876 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.755224 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.762836 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.778726 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.788033 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.804573 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.811866 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.819255 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.828894 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.846773 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.856060 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.866243 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.873578 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.884962 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.895103 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.906615 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.915821 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.924934 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.931644 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.938455 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.946746 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.955480 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.962898 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.969312 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.976164 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.983020 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.990301 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.003719 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.013896 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.023270 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.030768 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.037874 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.044312 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.052043 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.061306 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.071385 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.078017 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.088780 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.099036 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.107743 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.116758 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.125527 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.133405 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.141455 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.149024 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.159370 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.168199 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.180555 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.189286 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.208526 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.220566 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.228373 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.235389 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.242498 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.252613 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.260318 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.267521 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.275277 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.282684 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.290367 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.297710 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.304916 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.312438 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.320553 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.328519 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.335715 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.347113 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.357345 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.364625 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.375075 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.385069 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.394196 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.401097 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.410660 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.417759 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.424961 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.433133 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.442332 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.450619 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.457524 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.466822 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.473982 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.481117 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.489304 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.496667 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.506161 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.513096 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.520128 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.528013 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.537403 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.544535 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.551897 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.558913 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.572724 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.584279 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.590975 [1] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.598232 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:08.072963 [1] proc begin: <DistEnv 1/4 nccl>
10:51:08.111890 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
10:51:08.123904 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:51:10.687384 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.412374 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.435290 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.455116 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.472894 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.494285 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.512611 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.533629 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.557515 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.578873 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.600190 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.616741 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.631297 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.641749 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.658372 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.672492 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.690896 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.709123 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.718673 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.729595 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.740360 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.752975 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.763291 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.775657 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.786225 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.802121 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.822750 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.839808 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.861324 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.878109 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.897818 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.913802 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.932894 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.950905 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.962426 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.977326 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.991037 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.000746 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.015238 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.025967 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.035748 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.046669 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.057320 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.075881 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.089162 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.099838 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.109956 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.124409 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.142063 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.162171 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.180661 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.207315 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.227185 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.242954 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.258737 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.272071 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.290358 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.308346 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.330037 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.346642 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.366442 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.384225 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.403308 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.419535 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.432640 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.446394 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.462026 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.476717 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.497793 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.514718 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.534998 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.552294 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.572973 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.592066 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.611172 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.626584 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.638353 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.649661 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.659532 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.676588 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.698479 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.715623 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.737809 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.754515 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.768238 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.789221 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.809097 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.827930 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.847277 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.864363 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.885377 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.914397 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.932879 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.952906 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.972498 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.990803 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.007767 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.020554 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.030539 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.042492 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.052959 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.064160 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.095864 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.112352 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.126621 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.136759 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.163022 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.179599 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.204345 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.218942 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.229372 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.241069 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.251569 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.262609 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.272692 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.285536 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.303504 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.322554 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.342405 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.359698 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.381603 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.402061 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.423337 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.438727 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.458450 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.475641 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.494636 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.512710 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.533678 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.554024 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.572404 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.593522 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.610773 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.630687 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.650833 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.670471 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.688285 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.706768 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.724917 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.745939 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.764497 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.783489 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.798581 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.809320 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.820213 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.832511 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.846102 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.854815 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.864757 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.878380 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.891491 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.911872 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.934346 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.954664 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.972549 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.992927 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.011558 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.031448 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.048004 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.070097 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.093579 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.108886 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.118508 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.128933 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.141414 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.162224 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.180483 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.202628 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.228055 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.240290 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.250143 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.259780 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.270512 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.279878 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.289522 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.302687 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.316880 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.327934 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.338643 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.347802 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.357124 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.366199 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.386111 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.404722 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.425926 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.445390 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.465726 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.485978 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.504102 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.525862 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.544330 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.565364 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.574478 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.589740 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.604166 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.616393 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.626752 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.637528 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.647899 [1] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.657950 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:32.787625 [1] proc begin: <DistEnv 1/4 nccl>
10:52:32.842737 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
10:52:32.852037 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:52:34.271613 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.165132 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.194848 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.220936 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.258030 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.286770 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.319041 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.335073 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.363412 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.390890 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.417435 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.445218 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.472420 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.493797 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.513410 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.541356 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.560656 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.584290 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.608932 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.638568 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.666647 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.693921 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.724357 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.750688 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.778807 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.805649 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.833967 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.862170 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.889262 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.917407 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.955676 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.981520 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.012332 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.039832 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.067090 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.092661 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.117150 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.145416 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.171645 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.197524 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.223505 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.248895 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.271963 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.297741 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.321731 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.349099 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.375527 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.405944 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.432482 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.458314 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.480549 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.506125 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.529718 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.556622 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.583940 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.613020 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.640656 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.666557 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.691887 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.722296 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.750622 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.776719 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.805244 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.832810 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.861192 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.889949 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.923092 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.940267 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.981055 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.999655 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.021922 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.048781 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.073212 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.102685 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.128283 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.149705 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.171478 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.196859 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.218596 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.244889 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.271187 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.297992 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.325965 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.353842 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.378802 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.409695 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.432879 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.462041 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.489881 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.514138 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.541471 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.566927 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.593888 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.617874 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.645930 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.673062 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.693759 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.724315 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.750027 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.776552 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.802175 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.830866 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.858616 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.886472 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.912523 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.942900 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.987260 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.011499 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.038894 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.068792 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.097942 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.124935 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.154975 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.178169 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.204611 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.227405 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.252827 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.280630 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.306587 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.333150 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.361535 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.389819 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.416485 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.439535 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.466978 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.492936 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.518151 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.544119 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.569272 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.599003 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.626346 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.654134 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.679911 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.706071 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.734540 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.758725 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.786803 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.812510 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.838996 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.854028 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.872602 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.889031 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.910301 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.943387 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.983552 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.002248 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.022460 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.042875 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.068564 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.094659 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.120381 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.148564 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.172173 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.196577 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.222323 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.249205 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.273592 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.301081 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.328916 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.356939 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.385220 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.414897 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.439085 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.469511 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.495453 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.521191 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.548331 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.571213 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.598050 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.625590 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.655785 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.683366 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.711566 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.736819 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.762849 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.789071 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.822064 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.842441 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.864280 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.893669 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.923513 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.947842 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.990480 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.009193 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.030791 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.056015 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.083387 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.112751 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.141661 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.169517 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.196856 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.224655 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.245801 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.270713 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.303025 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.320415 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.341160 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.368700 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.392224 [1] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.417084 [1] Warning: no training nodes in this partition! Backward fake loss.
10:53:37.297694 [1] proc begin: <DistEnv 1/4 nccl>
10:53:46.684086 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
10:53:46.698724 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:55:30.960207 [1] proc begin: <DistEnv 1/4 nccl>
10:55:38.124708 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
10:55:38.151347 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:56:22.490352 [1] proc begin: <DistEnv 1/4 nccl>
10:56:26.776578 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
10:56:26.792856 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:58:16.854787 [1] proc begin: <DistEnv 1/4 nccl>
10:58:39.197626 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
10:58:39.322877 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:58:40.280309 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:41.212414 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:41.490533 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:41.766776 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:42.043972 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:42.319945 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:42.595812 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:42.872150 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:43.149719 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:43.427131 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:43.703212 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:43.979054 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:44.257062 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:44.533514 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:44.812004 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:45.088815 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:45.364475 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:45.640294 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:45.916226 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:46.192782 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:46.469584 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:46.746593 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:47.023490 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:47.301493 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:47.578955 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:47.855649 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:48.132816 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:48.410000 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:48.687709 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:48.965481 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:49.241830 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:49.519881 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:49.798251 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:50.078931 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:50.357339 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:50.635013 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:50.912026 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:51.188543 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:51.465000 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:51.742234 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:52.018088 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:52.294356 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:52.570230 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:52.846471 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:53.122544 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:53.399459 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:53.675054 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:53.951705 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:54.228500 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:54.505423 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:54.781527 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:55.059132 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:55.335878 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:55.612721 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:55.889564 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:56.166761 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:56.444028 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:56.720475 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:56.997796 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:57.273574 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:57.550069 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:57.826147 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:58.103777 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:58.381096 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:58.658467 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:58.935199 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:59.211203 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:59.488647 [1] Warning: no training nodes in this partition! Backward fake loss.
10:58:59.764957 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:00.041018 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:00.317966 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:00.595702 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:00.872355 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:01.148555 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:01.424741 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:01.700750 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:01.978685 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:02.266682 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:02.555563 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:02.843482 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:03.131540 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:03.421736 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:03.704151 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:03.981353 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:04.257869 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:04.534616 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:04.811167 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:05.088364 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:05.366120 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:05.643604 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:05.920626 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:06.197726 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:06.474497 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:06.751055 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:07.028907 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:07.306312 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:07.582740 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:07.859718 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:08.136658 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:08.413366 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:08.690483 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:08.968209 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:09.244657 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:09.521336 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:09.798713 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:10.076395 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:10.352851 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:10.629240 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:10.906812 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:11.182885 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:11.459627 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:11.736111 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:12.012891 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:12.290180 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:12.566522 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:12.842588 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:13.120225 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:13.397488 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:13.673673 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:13.950800 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:14.227091 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:14.504104 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:14.782044 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:15.059198 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:15.335641 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:15.612367 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:15.889102 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:16.165462 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:16.442100 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:16.718358 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:16.994965 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:17.271426 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:17.549003 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:17.825021 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:18.101368 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:18.377723 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:18.653795 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:18.929765 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:19.206305 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:19.482911 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:19.759907 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:20.035672 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:20.313202 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:20.589837 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:20.866495 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:21.142775 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:21.418466 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:21.694767 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:21.971185 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:22.246963 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:22.524210 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:22.800579 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:23.076487 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:23.353374 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:23.629084 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:23.905330 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:24.181968 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:24.458068 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:24.734037 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:25.010126 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:25.286460 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:25.562305 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:25.838724 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:26.115740 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:26.392275 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:26.668085 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:26.944249 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:27.220550 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:27.497075 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:27.773695 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:28.050575 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:28.328462 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:28.605973 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:28.882434 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:29.159160 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:29.436730 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:29.713571 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:29.989046 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:30.265159 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:30.542335 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:30.818657 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:31.095801 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:31.371858 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:31.649099 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:31.926234 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:32.202523 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:32.478369 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:32.755561 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:33.032245 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:33.308545 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:33.585432 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:33.862512 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:34.138400 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:34.415120 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:34.692763 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:34.968715 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:35.243780 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:35.519703 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:35.796112 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:36.072777 [1] Warning: no training nodes in this partition! Backward fake loss.
10:59:56.975255 [1] proc begin: <DistEnv 1/4 nccl>
11:00:01.678780 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
11:00:01.687390 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:00:07.352488 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:08.501309 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:08.982904 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:09.461702 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:09.943012 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:10.423696 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:10.903000 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:11.382415 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:11.861643 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:12.340398 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:12.818849 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:13.298560 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:13.778453 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:14.257361 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:14.736470 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:15.217002 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:15.696588 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:16.176362 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:16.655167 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:17.134850 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:17.614373 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:18.093227 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:18.573446 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:19.053331 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:19.532433 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:20.011568 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:20.497637 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:20.977006 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:21.455812 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:21.934915 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:22.415230 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:22.893629 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:23.373568 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:23.853946 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:24.332284 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:24.811959 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:25.292026 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:25.771799 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:26.250465 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:26.729818 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:27.209312 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:27.687461 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:28.167178 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:28.648953 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:29.129216 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:29.608076 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:30.087407 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:30.567602 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:31.046853 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:31.525171 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:32.003668 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:32.485183 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:32.963474 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:33.442437 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:33.922482 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:34.401571 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:34.880639 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:35.359837 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:35.839681 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:36.317564 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:36.797378 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:37.276831 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:37.755950 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:38.234933 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:38.714126 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:39.193680 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:39.673437 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:40.152243 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:40.632446 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:41.110761 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:41.589210 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:42.068172 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:42.547068 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:43.025492 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:43.504169 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:43.982151 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:44.459965 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:44.937817 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:45.416071 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:45.894577 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:46.373407 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:46.851763 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:47.329707 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:47.807873 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:48.286059 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:48.764231 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:49.242728 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:49.721083 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:50.199005 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:50.686192 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:51.166260 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:51.644845 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:52.123879 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:52.603926 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:53.083608 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:53.561869 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:54.041581 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:54.520904 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:55.000589 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:55.481105 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:55.963279 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:56.442517 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:56.922201 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:57.401332 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:57.880976 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:58.360501 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:58.841136 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:59.319941 [1] Warning: no training nodes in this partition! Backward fake loss.
11:00:59.799727 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:00.278993 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:00.758172 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:01.237468 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:01.728104 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:02.229748 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:02.728598 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:03.219541 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:03.699643 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:04.181774 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:04.661097 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:05.141882 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:05.621594 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:06.101412 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:06.580692 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:07.059771 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:07.539119 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:08.018184 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:08.496989 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:08.975231 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:09.454608 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:09.933986 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:10.415083 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:10.895619 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:11.376384 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:11.856255 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:12.336899 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:12.817384 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:13.296033 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:13.775104 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:14.254560 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:14.733844 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:15.213373 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:15.692204 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:16.172478 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:16.652533 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:17.131015 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:17.609868 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:18.089748 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:18.568431 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:19.047815 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:19.528078 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:20.007277 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:20.487000 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:20.967113 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:21.447618 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:21.927085 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:22.405747 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:22.884687 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:23.363447 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:23.842419 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:24.320810 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:24.800232 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:25.279647 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:25.758044 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:26.237349 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:26.716673 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:27.196680 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:27.676435 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:28.155788 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:28.636390 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:29.115434 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:29.594341 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:30.073096 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:30.551797 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:31.030567 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:31.509687 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:31.988200 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:32.466720 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:32.945319 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:33.424846 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:33.904655 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:34.383139 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:34.862518 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:35.340743 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:35.819466 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:36.298378 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:36.776884 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:37.255835 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:37.734100 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:38.213367 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:38.692477 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:39.171820 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:39.650138 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:40.130809 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:40.609378 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:41.087441 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:41.566704 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:42.045214 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:42.523505 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:43.002625 [1] Warning: no training nodes in this partition! Backward fake loss.
11:01:43.482228 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:00.372802 [1] proc begin: <DistEnv 1/4 nccl>
11:02:06.178082 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
11:02:06.185633 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:02:11.306069 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:12.988822 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:13.865876 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:14.744524 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:15.625147 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:16.504147 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:17.382722 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:18.262767 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:19.141451 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:20.019349 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:20.902262 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:21.779437 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:22.658705 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:23.536550 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:24.414706 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:25.291784 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:26.171444 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:27.048681 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:27.927540 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:28.805891 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:29.683557 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:30.560435 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:31.440882 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:32.318116 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:33.196310 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:34.075339 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:34.954163 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:35.832428 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:36.711626 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:37.590690 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:38.469219 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:39.348043 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:40.226579 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:41.107201 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:41.989990 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:42.871091 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:43.750043 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:44.628726 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:45.506413 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:46.385131 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:47.263918 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:48.142874 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:49.021063 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:49.899408 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:50.780279 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:51.658447 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:52.537733 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:53.415855 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:54.294029 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:55.171659 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:56.050623 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:56.929483 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:57.807647 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:58.685998 [1] Warning: no training nodes in this partition! Backward fake loss.
11:02:59.566192 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:00.443853 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:01.322397 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:02.206717 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:03.128237 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:04.029605 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:04.911810 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:05.791892 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:06.672766 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:07.553416 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:08.432653 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:09.312407 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:10.191592 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:11.071080 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:11.950621 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:12.830264 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:13.709126 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:14.588172 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:15.467995 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:16.347027 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:17.226686 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:18.105811 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:18.985998 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:19.864616 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:20.744930 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:21.625486 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:22.505799 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:23.384425 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:24.263302 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:25.142693 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:26.022481 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:26.900827 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:27.779642 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:28.659731 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:29.539081 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:30.417258 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:31.297355 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:32.176846 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:33.055348 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:33.934214 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:34.813519 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:35.692874 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:36.571113 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:37.451377 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:38.330632 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:39.210198 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:40.089867 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:40.969785 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:41.849134 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:42.727196 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:43.605696 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:44.483447 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:45.362779 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:46.240955 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:47.119365 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:47.997166 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:48.875960 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:49.753707 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:50.632098 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:51.510871 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:52.389350 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:53.265745 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:54.142769 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:55.020627 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:55.899774 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:56.777053 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:57.655407 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:58.532616 [1] Warning: no training nodes in this partition! Backward fake loss.
11:03:59.411478 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:00.290520 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:01.171055 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:02.069414 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:02.989867 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:03.879055 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:04.759148 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:05.638701 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:06.517736 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:07.398076 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:08.276565 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:09.155503 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:10.034939 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:10.915563 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:11.794730 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:12.675427 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:13.553718 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:14.433744 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:15.311988 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:16.193496 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:17.071929 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:17.952021 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:18.831332 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:19.711768 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:20.591391 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:21.472859 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:22.352381 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:23.230545 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:24.108398 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:24.986395 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:25.866661 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:26.747898 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:27.630523 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:28.515066 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:29.401782 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:30.280599 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:31.162121 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:32.040378 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:32.919112 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:33.797237 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:34.677486 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:35.556180 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:36.435498 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:37.314354 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:38.192352 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:39.070334 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:39.953982 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:40.837602 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:41.719662 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:42.600723 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:43.485110 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:44.369564 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:45.254502 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:46.142419 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:47.036050 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:47.929411 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:48.823207 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:49.712749 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:50.604688 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:51.496230 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:52.387289 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:53.274126 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:54.164468 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:55.050439 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:55.937224 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:56.823445 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:57.709908 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:58.598396 [1] Warning: no training nodes in this partition! Backward fake loss.
11:04:59.481883 [1] Warning: no training nodes in this partition! Backward fake loss.
11:05:00.365717 [1] Warning: no training nodes in this partition! Backward fake loss.
11:05:01.245491 [1] Warning: no training nodes in this partition! Backward fake loss.
11:05:02.168736 [1] Warning: no training nodes in this partition! Backward fake loss.
11:05:03.080334 [1] Warning: no training nodes in this partition! Backward fake loss.
11:05:03.962524 [1] Warning: no training nodes in this partition! Backward fake loss.
11:05:04.845116 [1] Warning: no training nodes in this partition! Backward fake loss.
11:05:05.728286 [1] Warning: no training nodes in this partition! Backward fake loss.
11:05:06.609308 [1] Warning: no training nodes in this partition! Backward fake loss.
11:05:07.492407 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:37.406936 [1] proc begin: <DistEnv 1/4 nccl>
14:44:37.620308 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
14:44:37.660337 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:44:40.180426 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.357327 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.367956 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.378851 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.388215 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.400113 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.411198 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.422201 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.431297 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.446964 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.457728 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.467857 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.478595 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.488851 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.498405 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.509569 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.520107 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.528648 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.541235 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.552442 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.562860 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.572143 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.582877 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.590353 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.602731 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.614245 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.622336 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.632725 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.642982 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.650323 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.660655 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.671503 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.678933 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.689296 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.699293 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.706766 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.717689 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.728043 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.735551 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.746524 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.755700 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.764281 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.773737 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.782918 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.790906 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.801655 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.810983 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.818827 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.828951 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.840553 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.850056 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.861091 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.871110 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.879513 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.900232 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.914462 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.923505 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.934246 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.944916 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.955594 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.964850 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.977694 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.988619 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.998869 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.007933 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.017621 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.027784 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.037598 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.045087 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.056128 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.066388 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.075442 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.084170 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.093185 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.102236 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.111952 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.121443 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.129710 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.139360 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.149732 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.158644 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.170284 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.179939 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.187536 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.197767 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.206416 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.213805 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.239844 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.249773 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.256473 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.266609 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.275631 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.282615 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.294308 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.303489 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.315441 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.328398 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.337997 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.354129 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.365921 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.374771 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.383138 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.392870 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.409214 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.418315 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.428304 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.436616 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.444439 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.454121 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.462419 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.474790 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.486285 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.495576 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.505619 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.517411 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.527505 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.535967 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.549031 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.559583 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.568472 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.581260 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.592145 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.600441 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.611589 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.621966 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.630387 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.640875 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.650193 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.658892 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.670331 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.680461 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.688830 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.703212 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.716133 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.724070 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.736734 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.747466 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.756542 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.767942 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.777583 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.786404 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.797752 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.805253 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.811724 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.820237 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.827397 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.834616 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.847357 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.856023 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.865250 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.878112 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.888919 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.901420 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.917994 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.929591 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.939450 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.952711 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.964504 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.981045 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.996165 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.014416 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.028002 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.039479 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.051946 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.060295 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.073534 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.082394 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.090897 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.100441 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.110345 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.117527 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.126810 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.135767 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.143192 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.152198 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.161314 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.169099 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.179879 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.188494 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.196604 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.205395 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.214141 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.222116 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.231262 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.242455 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.250753 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.260150 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.269333 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.276654 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.290182 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.300649 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.316858 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.328719 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.338960 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.353418 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.369087 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.380063 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.388860 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.399044 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.412711 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:13.864637 [1] proc begin: <DistEnv 1/4 nccl>
14:45:13.950930 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
14:45:13.961714 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:45:15.209677 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:47.651225 [1] proc begin: <DistEnv 1/4 nccl>
14:45:47.737375 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
14:45:47.750317 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:45:49.066153 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.881710 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.892889 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.904280 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.914449 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.923636 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.935650 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.945805 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.954986 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.965569 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.976169 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.984732 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.995325 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.005416 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.013712 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.024856 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.034648 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.042958 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.054334 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.064088 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.073086 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.083915 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.093919 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.102467 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.113427 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.124200 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.132719 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.143316 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.154070 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.162626 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.173872 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.183815 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.194063 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.208040 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.218480 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.227250 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.237629 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.247449 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.256675 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.267948 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.279006 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.287865 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.300564 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.310764 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.320869 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.333121 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.345608 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.355614 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.365563 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.374719 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.382894 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.395875 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.407409 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.417149 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.431293 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.442648 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.452040 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.466165 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.477435 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.487070 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.500307 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.512770 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.521476 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.541900 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.558704 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.569182 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.578926 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.589511 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.598292 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.608183 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.618930 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.627990 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.638856 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.652765 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.669387 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.677970 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.685318 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.691504 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.702701 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.709941 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.719705 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.730024 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.745592 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.755306 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.766947 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.780068 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.790571 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.813229 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.824000 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.838323 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.857479 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.869901 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.880086 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.888910 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.898279 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.907014 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.916987 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.926206 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.935299 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.946963 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.955957 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.964285 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.974171 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.986547 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.996860 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.006511 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.016185 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.025051 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.035488 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.045513 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.053894 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.063843 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.083670 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.093271 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.102830 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.112284 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.120997 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.131626 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.140819 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.149734 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.160136 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.170331 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.178751 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.189566 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.200814 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.210300 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.220601 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.230289 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.239158 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.250298 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.259639 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.267939 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.278208 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.287432 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.295993 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.305383 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.314667 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.323582 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.333114 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.342498 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.351268 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.361049 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.370185 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.378620 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.389216 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.399381 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.407893 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.417620 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.427364 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.435596 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.445201 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.456212 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.464762 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.475482 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.486287 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.494774 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.506357 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.516334 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.524858 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.535751 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.545602 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.554062 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.566398 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.576386 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.585412 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.595799 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.605831 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.621616 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.637657 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.647837 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.661386 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.673330 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.682939 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.690825 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.700440 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.710472 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.718555 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.728019 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.738254 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.746351 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.756718 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.765797 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.774015 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.783943 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.792949 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.800792 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.816665 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.825953 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.835669 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.845176 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.859696 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.867696 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.877760 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.888182 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.896441 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.906898 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.915750 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.923659 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.933624 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.942090 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:40.034288 [1] proc begin: <DistEnv 1/4 nccl>
14:51:40.105888 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
14:51:40.115895 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:51:42.308585 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.294687 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.306843 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.319067 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.327694 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.335770 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.348258 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.358718 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.368919 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.396854 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.414959 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.423515 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.434010 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.445044 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.454564 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.465262 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.475545 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.483116 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.494146 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.503488 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.513652 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.524981 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.535192 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.542408 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.551882 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.560568 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.568783 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.581809 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.592909 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.602993 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.614588 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.625150 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.637860 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.649162 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.659596 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.667932 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.677124 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.688017 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.696810 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.708474 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.721696 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.730316 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.741522 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.750929 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.758874 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.770254 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.779352 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.786991 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.798865 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.807554 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.814873 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.825326 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.834050 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.841685 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.852767 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.863280 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.871455 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.882098 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.891371 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.899510 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.909938 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.918993 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.926956 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.937511 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.946567 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.954522 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.971382 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.983413 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.992536 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.001484 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.010512 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.018105 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.027834 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.038329 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.046461 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.057117 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.066488 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.074328 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.088328 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.098864 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.108408 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.118880 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.129169 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.135840 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.145087 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.154754 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.161905 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.175547 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.185232 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.192560 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.205638 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.220718 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.229713 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.240099 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.251903 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.260961 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.271941 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.282068 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.291157 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.301055 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.310725 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.319172 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.330420 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.340191 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.349209 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.363594 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.374042 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.383874 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.406462 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.418636 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.433423 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.444250 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.457261 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.464109 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.474768 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.484843 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.492647 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.503215 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.514140 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.522026 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.534407 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.543704 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.552726 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.564171 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.573990 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.582155 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.592265 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.616155 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.625183 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.635960 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.647355 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.661490 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.673249 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.683277 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.694084 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.703033 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.713107 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.723015 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.732607 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.742577 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.752841 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.763221 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.774498 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.782426 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.792562 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.802895 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.811910 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.822706 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.833722 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.841557 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.855638 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.866989 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.874868 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.885671 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.894692 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.902900 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.913113 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.922051 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.930132 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.941072 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.950213 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.958603 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.969799 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.980455 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.988987 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.999895 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.007206 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.014079 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.021835 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.028709 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.036501 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.051629 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.068338 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.078743 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.090635 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.100489 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.108890 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.122930 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.133391 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.142269 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.154859 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.165452 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.174002 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.186762 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.198122 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.207262 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.219873 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.230354 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.238833 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.250655 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.260566 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.269903 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.281897 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.292105 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.301462 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.311835 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.323630 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.334919 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.346250 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.356099 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:00.242620 [1] proc begin: <DistEnv 1/4 nccl>
14:55:00.353916 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
14:55:00.367165 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:55:01.892360 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:02.947406 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:02.959728 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:02.971198 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:02.982334 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:02.992810 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.003676 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.021725 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.036299 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.050425 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.063515 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.076476 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.088150 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.099097 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.112527 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.120992 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.132021 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.147724 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.156164 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.164476 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.178889 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.188118 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.200469 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.213212 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.221989 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.238708 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.247971 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.260130 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.274765 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.289936 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.300443 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.310982 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.323152 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.332880 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.347920 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.356561 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.375004 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.385312 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.397509 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.409828 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.421161 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.434203 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.442197 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.450622 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.462357 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.471437 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.480053 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.489941 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.498163 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.506776 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.517657 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.525343 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.539669 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.547830 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.556732 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.563508 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.575809 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.582721 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.592471 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.602847 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.615771 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.624852 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.638648 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.646951 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.661367 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.669238 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.682531 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.691350 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.703731 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.712591 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.727357 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.734545 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.747687 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.756572 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.770787 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.778490 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.793036 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.801281 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.815384 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.822521 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.835902 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.843773 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.857703 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.865604 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.878732 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.888535 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.907053 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.919065 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.930719 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.939982 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.952701 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.964657 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.978569 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.986835 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.001184 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.008124 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.021366 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.031449 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.043844 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.051710 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.074460 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.091621 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.106437 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.115324 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.128395 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.136607 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.149708 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.159415 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.173464 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.181555 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.194232 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.203481 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.215765 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.224273 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.243359 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.254473 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.270785 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.279924 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.294241 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.303029 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.314768 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.323976 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.337557 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.348229 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.365016 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.375754 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.387153 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.398554 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.409807 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.422198 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.433380 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.444084 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.456193 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.464739 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.477133 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.485695 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.499791 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.508831 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.522127 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.531427 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.543116 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.552136 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.566771 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.574093 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.587463 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.596728 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.612196 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.623661 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.650694 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.659303 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.667905 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.676274 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.690099 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.701080 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.719663 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.731032 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.743872 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.756442 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.773626 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.786498 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.803852 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.816727 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.827130 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.836963 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.847130 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.856675 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.866097 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.874448 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.886813 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.895013 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.904488 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.913162 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.925919 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.936867 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.948344 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.957397 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.968328 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.978962 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.987852 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.996109 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.005796 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.014183 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.023453 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.031299 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.044549 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.054741 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.064031 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.072799 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.083683 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.093693 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.105581 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.115917 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.126231 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.136239 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.146233 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.155429 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.165932 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.175459 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.185937 [1] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.195186 [1] Warning: no training nodes in this partition! Backward fake loss.
14:58:29.570495 [1] proc begin: <DistEnv 1/4 nccl>
14:58:41.839747 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
14:58:41.856369 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:00:07.885483 [1] proc begin: <DistEnv 1/4 nccl>
15:00:07.959640 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
15:00:07.970488 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:00:09.246432 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:09.992438 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.009036 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.020580 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.028462 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.035888 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.043285 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.055133 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.070885 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.079681 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.087739 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.094877 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.103644 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.110649 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.119907 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.127807 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.135224 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.143169 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.151105 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.158597 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.165611 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.172497 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.180227 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.189028 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.197284 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.205227 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.212978 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.220396 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.227855 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.235478 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.243034 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.250907 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.258235 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.265953 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.273714 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.281411 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.289079 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.299713 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.308630 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.316542 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.326514 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.334983 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.345108 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.354843 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.362143 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.369939 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.377087 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.384573 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.392396 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.399962 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.407383 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.414023 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.422070 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.428421 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.436577 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.443597 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.451183 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.457798 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.468628 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.475421 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.483008 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.489511 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.498173 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.504608 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.511911 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.519239 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.530924 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.537495 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.545861 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.552259 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.560214 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.566896 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.576494 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.583380 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.593421 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.601342 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.611000 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.620159 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.630336 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.637238 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.646056 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.652398 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.660341 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.669040 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.678009 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.683878 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.693792 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.700629 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.709313 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.715412 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.724114 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.730905 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.739281 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.745698 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.758163 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.767214 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.774841 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.780725 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.788140 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.795018 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.802943 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.809055 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.816625 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.822805 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.831120 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.836835 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.845382 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.852406 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.859732 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.867005 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.874888 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.881328 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.899247 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.906178 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.915002 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.924493 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.938054 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.944453 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.952407 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.958998 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.967229 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.974065 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.982105 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.988008 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.000896 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.011117 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.019670 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.026072 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.034221 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.040047 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.051369 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.058431 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.070982 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.078552 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.087356 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.093292 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.104535 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.114948 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.122760 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.132184 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.144785 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.157717 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.167065 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.175327 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.183744 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.190145 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.201453 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.209916 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.219035 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.225216 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.233499 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.239883 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.247972 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.254667 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.269771 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.276347 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.284877 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.291005 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.302607 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.309573 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.318073 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.324332 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.332047 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.338438 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.346091 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.352156 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.360119 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.367668 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.376212 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.383500 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.391775 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.402781 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.410916 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.418411 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.426810 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.433982 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.442316 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.450450 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.458912 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.466769 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.475172 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.482091 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.490455 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.497610 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.505818 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.513463 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.527041 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.538516 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.548962 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.556171 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.564668 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.572008 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.580036 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.587787 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.596296 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.604284 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.612391 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.619832 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.628417 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.635987 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:18.404458 [1] proc begin: <DistEnv 1/4 nccl>
15:18:18.470894 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
15:18:18.481980 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:18:19.805103 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.463169 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.475140 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.483047 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.491207 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.498752 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.509320 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.519724 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.528067 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.538051 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.545699 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.554618 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.562042 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.570377 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.577579 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.584973 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.592749 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.600387 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.608034 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.615554 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.623085 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.630198 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.638379 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.645948 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.653271 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.660431 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.668917 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.678553 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.687011 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.694884 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.702609 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.713940 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.722909 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.731379 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.743318 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.750817 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.759285 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.768027 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.775774 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.783002 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.790068 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.798420 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.805661 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.813093 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.820951 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.828540 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.835874 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.843097 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.850742 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.858651 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.866570 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.872984 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.880401 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.886565 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.895063 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.901249 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.920694 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.929799 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.938069 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.944436 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.952524 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.958642 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.967604 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.973820 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.981896 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.988057 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.995850 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.002287 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.010457 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.016272 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.026606 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.033358 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.041285 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.047720 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.056355 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.063139 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.070731 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.076875 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.085386 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.091946 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.100413 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.106775 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.114755 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.120751 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.128796 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.135572 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.145593 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.152973 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.162875 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.169197 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.179004 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.184947 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.196810 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.205222 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.212962 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.219659 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.230614 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.237258 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.245326 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.251150 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.259491 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.266421 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.279370 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.285819 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.293469 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.299553 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.307815 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.314408 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.322610 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.328337 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.336224 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.342642 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.355547 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.364047 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.377070 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.384095 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.393462 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.399685 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.416168 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.428340 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.441974 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.448382 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.461555 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.471578 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.480109 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.486526 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.494082 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.501242 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.518139 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.530048 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.544558 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.551983 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.561131 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.567091 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.575455 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.582017 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.589759 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.598160 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.606676 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.612584 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.620448 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.627040 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.635900 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.642358 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.650420 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.656524 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.665310 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.671658 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.679689 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.685683 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.693690 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.699688 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.707792 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.714511 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.722137 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.728498 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.742165 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.755968 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.764033 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.770716 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.778396 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.785021 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.792852 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.802306 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.810287 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.816468 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.824311 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.830586 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.842665 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.850716 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.858564 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.867454 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.878262 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.884732 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.895287 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.901813 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.909497 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.915685 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.923963 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.930867 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.939080 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.945074 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.965376 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.976076 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.989193 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.998302 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.008818 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.017576 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.025653 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.031784 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.039761 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.046457 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.054636 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.060840 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.069516 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.075763 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.084057 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.090522 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.100703 [1] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.107055 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:57.742561 [1] proc begin: <DistEnv 1/4 nccl>
08:58:57.908894 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
08:58:57.920962 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

08:58:59.156091 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.818365 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.830389 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.838049 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.845905 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.853974 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.860931 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.868962 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.877122 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.885393 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.897535 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.908701 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.917778 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.924770 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.932396 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.939192 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.947007 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.954286 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.962792 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.969719 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.977190 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.984669 [1] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.994347 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.005552 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.014477 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.027259 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.035631 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.043757 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.054823 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.065298 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.074211 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.082116 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.089545 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.096176 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.104145 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.111279 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.118495 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.125866 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.132742 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.140814 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.148315 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.155666 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.164554 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.174700 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.182382 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.192537 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.200749 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.208628 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.216687 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.224773 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.232763 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.241245 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.249412 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.259018 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.267694 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.276548 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.284487 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.292785 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.303843 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.314341 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.324541 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.335091 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.346597 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.356447 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.363728 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.371434 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.378827 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.386148 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.393403 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.400968 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.408153 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.415198 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.433934 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.446691 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.455150 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.462695 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.470500 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.477385 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.484729 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.492452 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.499858 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.507256 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.516508 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.525061 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.533085 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.540885 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.547824 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.554987 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.564175 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.571611 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.579292 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.586596 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.593925 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.611783 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.622710 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.630372 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.637098 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.643770 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.651530 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.658710 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.666674 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.673452 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.680841 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.688575 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.695949 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.702883 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.714402 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.736851 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.749589 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.759408 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.776191 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.787334 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.796244 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.807425 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.818848 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.825982 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.833440 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.840670 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.848042 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.857875 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.871502 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.881261 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.890602 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.898735 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.907110 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.915371 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.923246 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.932094 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.940253 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.948904 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.957236 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.965230 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.973332 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.981556 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.990113 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.998456 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.006398 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.014219 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.021812 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.030434 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.038416 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.046744 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.054551 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.062346 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.070291 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.078105 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.086433 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.094603 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.102658 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.110275 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.118097 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.126631 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.134364 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.142587 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.152816 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.163897 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.174790 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.185328 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.193669 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.202050 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.209839 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.218734 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.226519 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.234041 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.241035 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.250586 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.258180 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.265298 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.272216 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.279910 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.287146 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.298069 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.309157 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.316579 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.322980 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.329358 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.338636 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.345726 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.353524 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.360253 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.367912 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.376167 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.383650 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.391006 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.398582 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.406127 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.413027 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.422527 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.432744 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.441892 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.456818 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.467329 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.474671 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.482441 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.490318 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.501239 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.508497 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.515741 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.523028 [1] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.532637 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:05.549714 [1] proc begin: <DistEnv 1/4 nccl>
09:00:05.605495 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
09:00:05.618311 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:00:07.193116 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:07.893274 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:07.915825 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:07.938792 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:07.962152 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:07.982523 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.002386 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.026824 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.042892 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.058314 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.077832 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.097743 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.123195 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.146495 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.165851 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.187577 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.209741 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.231171 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.253989 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.272728 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.294469 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.312374 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.328273 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.350409 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.369414 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.390594 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.414078 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.434499 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.457180 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.477333 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.503693 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.520362 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.542681 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.570698 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.594236 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.618409 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.643626 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.666376 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.686877 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.710036 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.731537 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.754818 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.774719 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.796350 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.817810 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.838560 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.862245 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.883509 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.903634 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.923234 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.944092 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.958345 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.969802 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.981910 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.002647 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.026443 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.045529 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.066892 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.086154 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.106077 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.128922 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.149119 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.170656 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.194123 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.215193 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.234407 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.255002 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.273153 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.288265 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.298529 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.310970 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.324356 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.335720 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.346584 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.357278 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.372580 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.386966 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.398528 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.414555 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.435346 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.457917 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.478908 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.500980 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.521291 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.541203 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.566070 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.585467 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.606220 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.633296 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.653042 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.666346 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.679930 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.691443 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.702798 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.714129 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.724950 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.736153 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.747695 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.759275 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.770019 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.781038 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.794056 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.818048 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.841554 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.859962 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.882859 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.898305 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.913440 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.934695 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.955464 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.972740 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.994380 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.014598 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.037157 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.060356 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.081125 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.096763 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.116472 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.133100 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.156086 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.178693 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.198761 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.219092 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.235889 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.258509 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.277072 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.298086 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.322270 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.342879 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.361881 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.379816 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.402047 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.422723 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.446513 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.467710 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.489779 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.509348 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.530556 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.552887 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.586163 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.604872 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.638457 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.654841 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.681089 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.702484 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.726183 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.749439 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.770762 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.794039 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.814962 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.834008 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.850979 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.873595 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.892775 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.914614 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.937701 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.959520 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.981780 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.001193 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.022368 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.041438 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.062638 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.085962 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.107399 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.129588 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.149747 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.174755 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.197952 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.217748 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.238644 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.258401 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.275996 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.298264 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.317186 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.336795 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.353510 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.368784 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.390840 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.414302 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.435212 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.454058 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.473093 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.494096 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.515592 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.538161 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.559603 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.591116 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.614190 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.645863 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.665686 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.686814 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.706549 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.724770 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.747210 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.770098 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.789043 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.810361 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.834133 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.854913 [1] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.875317 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:09.028244 [1] proc begin: <DistEnv 1/4 nccl>
09:01:09.061479 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
09:01:09.074971 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:01:11.516061 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.218359 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.248646 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.279550 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.309793 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.344632 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.374208 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.405408 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.438388 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.469675 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.504249 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.542914 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.571266 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.599600 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.628273 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.661530 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.692708 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.719006 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.745605 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.777769 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.808859 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.832721 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.862260 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.894530 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.926560 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.958913 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.987043 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.015872 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.041658 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.070968 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.096912 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.125787 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.154711 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.181545 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.209427 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.243903 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.271899 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.299056 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.328645 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.354732 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.380807 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.405470 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.434423 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.461962 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.490701 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.517404 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.546589 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.582858 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.611316 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.642840 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.674597 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.704725 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.735215 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.763735 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.792926 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.827979 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.856808 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.889304 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.922839 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.955512 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.985112 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.025452 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.062229 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.094497 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.124949 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.150277 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.181284 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.211410 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.241986 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.270599 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.298476 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.328745 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.357182 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.388254 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.420876 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.450213 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.477355 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.505324 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.535697 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.565091 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.593120 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.622480 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.651528 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.677922 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.708176 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.733962 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.764264 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.791643 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.820864 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.852225 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.881486 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.915003 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.944871 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.979147 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.004977 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.035403 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.069302 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.099491 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.133950 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.161373 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.189855 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.218371 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.248192 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.274480 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.299762 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.329293 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.358241 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.388369 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.417791 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.448848 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.482381 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.512547 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.541343 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.572744 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.601668 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.630469 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.657853 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.687168 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.714390 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.744021 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.773663 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.802064 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.833513 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.869908 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.901426 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.936296 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.970920 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.000172 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.027722 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.078393 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.105121 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.133727 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.161654 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.189961 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.218648 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.247277 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.274491 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.299500 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.326012 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.351832 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.381366 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.411539 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.439136 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.468981 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.496889 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.527505 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.557246 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.586220 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.617200 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.647405 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.679594 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.705737 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.733211 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.763237 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.789640 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.817405 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.845886 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.873678 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.903741 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.933273 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.956237 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.990606 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.021086 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.056367 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.095912 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.125887 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.154578 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.182852 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.203233 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.233281 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.262152 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.292157 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.321132 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.350149 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.378039 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.406995 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.434855 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.462946 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.492503 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.522218 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.554363 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.583803 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.610141 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.639059 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.666286 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.694005 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.724213 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.753888 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.783602 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.813557 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.836329 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.861066 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.886007 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.917215 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.950557 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.994285 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:18.030496 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:18.066963 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:18.107138 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:18.135244 [1] Warning: no training nodes in this partition! Backward fake loss.
09:01:42.474677 [1] proc begin: <DistEnv 1/4 nccl>
09:01:48.839852 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:01:48.853627 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:02:59.915845 [1] proc begin: <DistEnv 1/4 nccl>
09:03:05.897204 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:03:05.917612 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:04:05.263506 [1] proc begin: <DistEnv 1/4 nccl>
09:04:11.209593 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:04:11.227782 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:05:07.889973 [1] proc begin: <DistEnv 1/4 nccl>
09:05:22.932341 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
09:05:22.939798 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:05:34.319153 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:35.279015 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:35.559870 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:35.840236 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:36.119068 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:36.399570 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:36.680227 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:36.960250 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:37.240844 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:37.521444 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:37.801679 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:38.081603 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:38.363467 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:38.643310 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:38.923375 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:39.209066 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:39.489590 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:39.769503 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:40.050354 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:40.331179 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:40.614051 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:40.896022 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:41.176548 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:41.457584 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:41.737763 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:42.018329 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:42.298476 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:42.579138 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:42.859286 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:43.140536 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:43.420692 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:43.703180 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:43.983578 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:44.265259 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:44.546339 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:44.827113 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:45.108080 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:45.388348 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:45.668694 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:45.949110 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:46.229086 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:46.511042 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:46.792105 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:47.072209 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:47.353282 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:47.633563 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:47.913908 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:48.194104 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:48.476917 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:48.756784 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:49.037865 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:49.319318 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:49.599987 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:49.880758 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:50.161304 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:50.441452 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:50.721581 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:51.001593 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:51.280873 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:51.562501 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:51.842534 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:52.124069 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:52.404153 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:52.685253 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:52.966547 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:53.247120 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:53.527585 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:53.807813 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:54.088778 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:54.368806 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:54.649195 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:54.929302 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:55.209433 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:55.491509 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:55.772941 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:56.054018 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:56.334440 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:56.614503 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:56.895841 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:57.175380 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:57.455402 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:57.735364 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:58.015409 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:58.295291 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:58.575513 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:58.855595 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:59.136214 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:59.417044 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:59.696625 [1] Warning: no training nodes in this partition! Backward fake loss.
09:05:59.977460 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:00.258795 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:00.540135 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:00.820289 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:01.100363 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:01.381956 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:01.662083 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:01.945609 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:02.236567 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:02.526562 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:02.817668 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:03.109337 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:03.399432 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:03.680260 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:03.961269 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:04.241894 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:04.523466 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:04.803888 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:05.084762 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:05.365460 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:05.646998 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:05.927535 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:06.209532 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:06.491929 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:06.773574 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:07.053803 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:07.335298 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:07.616171 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:07.896930 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:08.177317 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:08.457475 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:08.737772 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:09.017542 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:09.298793 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:09.579902 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:09.861166 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:10.141602 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:10.421580 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:10.701493 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:10.981782 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:11.263348 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:11.545009 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:11.825289 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:12.105401 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:12.385740 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:12.666135 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:12.946610 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:13.227460 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:13.507469 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:13.787491 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:14.067239 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:14.347794 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:14.628362 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:14.908879 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:15.189345 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:15.469963 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:15.750132 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:16.030559 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:16.311576 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:16.591392 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:16.871984 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:17.152866 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:17.433155 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:17.712787 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:17.993520 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:18.274727 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:18.555277 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:18.837353 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:19.117882 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:19.398598 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:19.679693 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:19.961730 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:20.241493 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:20.522162 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:20.802592 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:21.084248 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:21.364774 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:21.645696 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:21.925921 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:22.206580 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:22.486826 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:22.767375 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:23.048550 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:23.328893 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:23.610447 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:23.891238 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:24.171605 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:24.452131 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:24.732660 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:25.014014 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:25.294890 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:25.574726 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:25.854675 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:26.134941 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:26.415620 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:26.695429 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:26.975023 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:27.255396 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:27.536460 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:27.817624 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:28.098120 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:28.377768 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:28.657527 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:28.937732 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:29.217978 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:29.498143 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:29.778725 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:30.059640 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:30.340226 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:30.621546 [1] Warning: no training nodes in this partition! Backward fake loss.
09:06:30.902043 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:37.989048 [1] proc begin: <DistEnv 1/4 nccl>
09:07:42.628850 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
09:07:42.636779 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:07:47.895866 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:49.093712 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:49.589517 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:50.086122 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:50.582443 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:51.077567 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:51.573777 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:52.068714 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:52.563795 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:53.060064 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:53.555287 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:54.049300 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:54.544861 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:55.040812 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:55.536767 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:56.033071 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:56.529371 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:57.024186 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:57.518326 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:58.012841 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:58.507852 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:59.001892 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:59.496647 [1] Warning: no training nodes in this partition! Backward fake loss.
09:07:59.992183 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:00.486888 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:00.982803 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:01.479365 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:01.975786 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:02.486932 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:02.999381 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:03.512987 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:04.011560 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:04.507453 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:05.003647 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:05.499885 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:05.995252 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:06.491271 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:06.988052 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:07.484335 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:07.979490 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:08.475663 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:08.972727 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:09.469462 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:09.966601 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:10.464224 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:10.959468 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:11.455236 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:11.951557 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:12.447976 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:12.943819 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:13.441230 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:13.936586 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:14.432719 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:14.929456 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:15.426647 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:15.924194 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:16.420210 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:16.916723 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:17.412300 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:17.908160 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:18.404901 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:18.899914 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:19.395136 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:19.892771 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:20.388215 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:20.884433 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:21.380282 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:21.875516 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:22.370120 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:22.865197 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:23.360287 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:23.855552 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:24.350339 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:24.845169 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:25.341648 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:25.837576 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:26.332937 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:26.828032 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:27.324509 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:27.819516 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:28.314041 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:28.809878 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:29.306039 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:29.801948 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:30.298498 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:30.796000 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:31.292163 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:31.787072 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:32.283459 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:32.778595 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:33.275747 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:33.771893 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:34.266511 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:34.761558 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:35.258215 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:35.754189 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:36.249769 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:36.744466 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:37.241162 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:37.736639 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:38.231744 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:38.726725 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:39.222474 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:39.719014 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:40.214532 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:40.709763 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:41.205006 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:41.701131 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:42.197185 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:42.693237 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:43.188190 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:43.684565 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:44.180092 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:44.676069 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:45.172745 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:45.668398 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:46.163125 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:46.659239 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:47.155546 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:47.651373 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:48.146407 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:48.641716 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:49.136358 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:49.632054 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:50.127776 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:50.622511 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:51.117726 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:51.613775 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:52.108622 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:52.603913 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:53.099625 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:53.594923 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:54.090106 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:54.584808 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:55.079879 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:55.574684 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:56.070377 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:56.565374 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:57.060595 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:57.556121 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:58.050860 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:58.545743 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:59.041491 [1] Warning: no training nodes in this partition! Backward fake loss.
09:08:59.537549 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:00.033815 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:00.529282 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:01.025852 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:01.521424 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:02.026893 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:02.539382 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:03.053802 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:03.557074 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:04.053471 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:04.549002 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:05.044338 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:05.540337 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:06.036536 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:06.532276 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:07.029683 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:07.525653 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:08.021332 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:08.517237 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:09.012925 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:09.509575 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:10.006628 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:10.502423 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:11.000101 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:11.496942 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:11.992931 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:12.489064 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:12.985743 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:13.481693 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:13.976898 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:14.471901 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:14.967625 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:15.462883 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:15.958575 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:16.454180 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:16.949612 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:17.445610 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:17.941651 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:18.437261 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:18.932826 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:19.428004 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:19.926748 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:20.421945 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:20.918358 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:21.413707 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:21.908886 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:22.404709 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:22.900384 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:23.395136 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:23.890831 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:24.386470 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:24.882283 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:25.377262 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:25.874434 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:26.370420 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:26.865876 [1] Warning: no training nodes in this partition! Backward fake loss.
09:09:27.361760 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:03.524124 [1] proc begin: <DistEnv 1/4 nccl>
09:10:07.996182 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
09:10:08.003674 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:10:12.678188 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:14.235559 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:15.130752 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:16.023805 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:16.919859 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:17.811648 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:18.706239 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:19.603513 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:20.498889 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:21.396204 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:22.297012 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:23.194434 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:24.090834 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:24.988662 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:25.885778 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:26.782350 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:27.675584 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:28.573480 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:29.469083 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:30.365996 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:31.264274 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:32.162012 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:33.058460 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:33.963245 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:34.861852 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:35.758425 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:36.656111 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:37.552491 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:38.449961 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:39.346133 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:40.245465 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:41.141153 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:42.035689 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:42.931957 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:43.827599 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:44.723255 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:45.617100 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:46.511337 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:47.403410 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:48.294697 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:49.185724 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:50.077660 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:50.967829 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:51.857542 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:52.746154 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:53.635461 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:54.526181 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:55.416606 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:56.305622 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:57.197979 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:58.088075 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:58.978782 [1] Warning: no training nodes in this partition! Backward fake loss.
09:10:59.869376 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:00.760327 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:01.675028 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:02.606636 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:03.505344 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:04.401335 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:05.296473 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:06.192520 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:07.090048 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:07.984354 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:08.879972 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:09.774892 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:10.672588 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:11.569886 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:12.467496 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:13.363013 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:14.258864 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:15.154716 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:16.049630 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:16.946047 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:17.835176 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:18.727434 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:19.618188 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:20.509929 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:21.401455 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:22.293029 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:23.183978 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:24.077245 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:24.967704 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:25.863720 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:26.756765 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:27.650320 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:28.545088 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:29.440472 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:30.335060 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:31.233577 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:32.128189 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:33.022674 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:33.916290 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:34.807535 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:35.701609 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:36.596175 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:37.494240 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:38.388415 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:39.284116 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:40.181083 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:41.077427 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:41.971721 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:42.868816 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:43.763503 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:44.659467 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:45.553492 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:46.447824 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:47.343088 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:48.237971 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:49.133621 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:50.028647 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:50.924566 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:51.819142 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:52.716398 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:53.611105 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:54.507064 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:55.400358 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:56.294423 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:57.189587 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:58.086790 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:58.982835 [1] Warning: no training nodes in this partition! Backward fake loss.
09:11:59.876807 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:00.772109 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:01.668202 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:02.595946 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:03.517397 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:04.413726 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:05.308972 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:06.205044 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:07.097874 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:07.997025 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:08.892116 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:09.790754 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:10.685609 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:11.582157 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:12.477508 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:13.371332 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:14.266987 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:15.163243 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:16.057769 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:16.950250 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:17.841965 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:18.733754 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:19.625819 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:20.516042 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:21.407897 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:22.299766 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:23.191209 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:24.082534 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:24.973525 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:25.866110 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:26.757407 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:27.648729 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:28.540301 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:29.431888 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:30.322606 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:31.214648 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:32.105736 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:32.997273 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:33.887960 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:34.778883 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:35.670534 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:36.561977 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:37.454523 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:38.347440 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:39.238196 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:40.130102 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:41.025584 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:41.924601 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:42.820121 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:43.712193 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:44.603108 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:45.498399 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:46.396179 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:47.292074 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:48.186118 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:49.081450 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:49.975754 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:50.870590 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:51.766936 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:52.661393 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:53.555695 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:54.450987 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:55.346552 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:56.240510 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:57.134187 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:58.029740 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:58.925042 [1] Warning: no training nodes in this partition! Backward fake loss.
09:12:59.819318 [1] Warning: no training nodes in this partition! Backward fake loss.
09:13:00.714050 [1] Warning: no training nodes in this partition! Backward fake loss.
09:13:01.618639 [1] Warning: no training nodes in this partition! Backward fake loss.
09:13:02.541970 [1] Warning: no training nodes in this partition! Backward fake loss.
09:13:03.460763 [1] Warning: no training nodes in this partition! Backward fake loss.
09:13:04.354863 [1] Warning: no training nodes in this partition! Backward fake loss.
09:13:05.251053 [1] Warning: no training nodes in this partition! Backward fake loss.
09:13:06.148131 [1] Warning: no training nodes in this partition! Backward fake loss.
09:13:07.044009 [1] Warning: no training nodes in this partition! Backward fake loss.
09:13:07.940351 [1] Warning: no training nodes in this partition! Backward fake loss.
09:13:08.836416 [1] Warning: no training nodes in this partition! Backward fake loss.
09:13:09.732660 [1] Warning: no training nodes in this partition! Backward fake loss.
09:13:10.628759 [1] Warning: no training nodes in this partition! Backward fake loss.
09:13:11.525333 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:04.734689 [1] proc begin: <DistEnv 1/4 nccl>
14:50:21.908504 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:50:21.950826 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:50:30.856622 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:32.295238 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:33.010661 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:33.724225 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:34.436283 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:35.149345 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:35.864241 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:36.578411 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:37.294125 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:38.008062 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:38.722491 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:39.437142 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:40.148701 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:40.861464 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:41.574852 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:42.289092 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:43.002490 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:43.714894 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:44.429825 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:45.142809 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:45.856057 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:46.568813 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:47.281892 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:47.995618 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:48.710031 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:49.425802 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:50.140345 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:50.854350 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:51.566449 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:52.281453 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:52.994757 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:53.710593 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:54.425719 [1] Warning: no training nodes in this partition! Backward fake loss.
14:50:55.140303 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:10.075638 [1] proc begin: <DistEnv 1/4 nccl>
14:51:10.166810 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
14:51:10.175800 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:51:11.644859 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.357648 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.372856 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.380894 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.391989 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.400076 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.407218 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.415058 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.422403 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.432685 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.440974 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.449847 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.456682 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.464776 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.472017 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.479679 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.490866 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.498905 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.506112 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.513708 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.521102 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.528545 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.536818 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.543986 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.551478 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.559123 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.566970 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.573934 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.581324 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.589340 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.598108 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.606087 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.613379 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.620608 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.628146 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.635382 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.643417 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.650838 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.658603 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.666285 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.679583 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.688321 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.696234 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.705745 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.714240 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.723379 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.732179 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.742689 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.750376 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.758151 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.765627 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.771960 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.779707 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.787936 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.795595 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.801986 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.810671 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.816990 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.825009 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.830918 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.839908 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.845697 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.853891 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.860424 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.868074 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.878444 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.885898 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.892780 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.900163 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.905988 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.915605 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.921869 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.930043 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.936465 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.943416 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.950067 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.958528 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.964217 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.972263 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.979428 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.992161 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.002470 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.010176 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.017021 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.025088 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.031489 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.043140 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.052854 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.064251 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.070620 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.084919 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.091341 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.105094 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.114605 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.123813 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.134007 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.142063 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.148414 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.156244 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.162333 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.171351 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.177286 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.185681 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.192001 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.199615 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.208891 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.217320 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.230985 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.239008 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.249283 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.268677 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.277305 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.285991 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.291801 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.299709 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.306010 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.314152 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.320370 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.332022 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.341228 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.351082 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.357618 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.364732 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.371004 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.383539 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.389818 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.397716 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.405893 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.413641 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.422594 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.430090 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.436374 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.444798 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.450301 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.458642 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.465579 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.480613 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.490393 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.500643 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.506838 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.526805 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.535984 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.546949 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.556626 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.565087 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.570384 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.579278 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.585045 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.593025 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.599040 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.608840 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.614752 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.623097 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.629173 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.637078 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.646109 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.655139 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.661341 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.669320 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.674910 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.685932 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.692418 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.700044 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.705825 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.713700 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.719973 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.730748 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.736802 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.744915 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.751626 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.764236 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.772467 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.781073 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.787424 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.795988 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.802627 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.812514 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.819998 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.829925 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.840205 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.847934 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.854833 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.863121 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.869095 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.876510 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.882928 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.890383 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.897134 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.907618 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.913937 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.922154 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.928697 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.935471 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.941989 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.949781 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.956377 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.963746 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.969962 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.977498 [1] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.983692 [1] Warning: no training nodes in this partition! Backward fake loss.
14:54:37.195158 [1] proc begin: <DistEnv 1/4 nccl>
14:54:49.658749 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
14:54:49.673126 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:47:45.383151 [1] proc begin: <DistEnv 1/4 nccl>
20:48:16.760004 [1] proc begin: <DistEnv 1/4 nccl>
20:48:24.134701 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
20:48:24.152124 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:49:01.886758 [1] proc begin: <DistEnv 1/4 nccl>
20:49:08.022140 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
20:49:08.038424 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:52:06.236308 [1] proc begin: <DistEnv 1/4 nccl>
20:52:13.061143 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
20:52:13.083682 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:52:51.553708 [1] proc begin: <DistEnv 1/4 nccl>
20:52:51.644904 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
20:52:51.654237 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:52:53.090361 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.814099 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.829305 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.840834 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.850171 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.862156 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.872902 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.886111 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.896174 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.905513 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.914682 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.924078 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.935620 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.949718 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.959663 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.968916 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.978576 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.987763 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.997032 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.006243 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.016140 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.025279 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.034529 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.047186 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.058005 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.068376 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.079974 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.089581 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.098661 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.108950 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.118788 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.127958 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.137076 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.153329 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.166619 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.175794 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.184836 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.194334 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.206595 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.218879 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.229907 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.242288 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.252418 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.264956 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.275251 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.285694 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.296486 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.308357 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.320351 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.331609 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.344441 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.354069 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.364719 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.376794 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.386427 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.400893 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.415143 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.425988 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.435340 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.446691 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.456072 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.465729 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.475020 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.484265 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.493549 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.502946 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.514137 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.526811 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.536188 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.548202 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.567638 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.578649 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.587613 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.597080 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.608809 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.621335 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.630884 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.639995 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.652161 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.661336 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.672255 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.692576 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.706746 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.717535 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.727855 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.738130 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.749902 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.761211 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.771140 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.785272 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.796754 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.809517 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.820453 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.836834 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.855069 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.870283 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.880983 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.891567 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.903850 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.915907 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.926173 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.937476 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.946644 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.956218 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.965249 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.974395 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.983005 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.995724 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.007060 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.018265 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.030297 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.040427 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.053698 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.063758 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.073130 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.083829 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.097855 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.109278 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.118311 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.128687 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.138801 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.150638 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.163652 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.173684 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.182781 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.191371 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.200497 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.209418 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.218578 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.227393 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.236681 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.245920 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.254627 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.264733 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.275446 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.286458 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.297264 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.307352 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.320453 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.329962 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.339456 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.351848 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.360945 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.372661 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.381960 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.393600 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.406233 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.416280 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.428939 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.438258 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.450278 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.464306 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.476945 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.487836 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.497583 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.520983 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.533572 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.543117 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.553405 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.573095 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.585938 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.595478 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.604627 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.614731 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.625191 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.635924 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.647474 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.657796 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.668191 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.678666 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.699564 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.711974 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.722356 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.733807 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.744525 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.754863 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.766563 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.777917 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.788787 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.801094 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.812387 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.822028 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.833458 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.843670 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.852910 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.868417 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.878012 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.887054 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.899811 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.908736 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.920460 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.931963 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.946551 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.955171 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.964532 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.973581 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.982559 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.991300 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:56.000242 [1] Warning: no training nodes in this partition! Backward fake loss.
20:52:56.012418 [1] Warning: no training nodes in this partition! Backward fake loss.
20:53:19.658397 [1] proc begin: <DistEnv 1/4 nccl>
20:53:25.709953 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
20:53:25.726597 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:54:39.274881 [1] proc begin: <DistEnv 1/4 nccl>
20:54:39.305029 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
20:54:39.315945 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:54:40.672274 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.410492 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.426439 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.439740 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.454358 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.471560 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.481796 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.494819 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.509325 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.519246 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.530079 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.543078 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.552938 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.562592 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.572267 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.586355 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.595920 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.606046 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.615522 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.625159 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.635280 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.649232 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.661074 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.679370 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.694873 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.706649 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.719974 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.729412 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.740258 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.750149 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.759437 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.772349 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.782340 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.792080 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.804366 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.814518 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.824616 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.840269 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.855171 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.870812 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.881838 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.892560 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.905108 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.915805 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.932149 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.945258 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.957105 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.969043 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.981662 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.993687 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.004705 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.017447 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.031967 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.044895 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.055532 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.066473 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.079664 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.090016 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.102801 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.112932 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.126864 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.143542 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.158196 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.168873 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.179494 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.190288 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.200854 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.215262 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.226771 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.239842 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.251265 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.266040 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.285257 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.299579 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.312284 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.325648 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.339294 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.358266 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.374744 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.391362 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.405105 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.416575 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.428868 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.441592 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.457800 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.471430 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.483175 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.492545 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.503721 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.518961 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.532703 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.546933 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.557226 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.567764 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.577483 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.588206 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.605233 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.622951 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.636216 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.648002 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.661451 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.673399 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.692833 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.710977 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.724559 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.737427 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.751054 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.763991 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.775945 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.787770 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.800174 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.811721 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.823684 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.837030 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.848906 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.860581 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.872120 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.885776 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.897450 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.909898 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.925588 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.939719 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.950877 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.963088 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.973550 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.988609 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.000021 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.010710 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.021225 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.033927 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.047153 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.056570 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.067774 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.078671 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.093653 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.107301 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.118425 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.132166 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.141822 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.154490 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.165685 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.179618 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.189562 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.199231 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.208622 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.218127 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.227658 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.237277 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.246766 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.263476 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.274485 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.290417 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.300630 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.313824 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.323637 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.332862 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.342122 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.351460 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.360428 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.369404 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.378546 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.387524 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.397503 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.414347 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.424184 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.433441 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.443042 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.458197 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.475239 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.490345 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.499856 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.509331 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.520727 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.535974 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.545371 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.555104 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.565586 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.575479 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.585122 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.596713 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.606061 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.619409 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.632023 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.646113 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.656493 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.669993 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.681225 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.692930 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.704906 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.716722 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.729220 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.740532 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.751842 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.762770 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.774832 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.789769 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.801461 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.813654 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.826567 [1] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.842735 [1] Warning: no training nodes in this partition! Backward fake loss.
20:55:42.674756 [1] proc begin: <DistEnv 1/4 nccl>
20:55:49.159074 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
20:55:49.175771 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:57:56.481402 [1] proc begin: <DistEnv 1/4 nccl>
20:57:56.582740 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
20:57:56.597131 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:57:57.679772 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.408432 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.422799 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.435841 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.448242 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.457606 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.468931 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.480980 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.491617 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.501263 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.510422 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.520289 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.529763 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.538778 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.548009 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.557474 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.566668 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.581328 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.592040 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.601589 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.610863 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.619976 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.629493 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.638851 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.647861 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.657629 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.670387 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.679878 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.690658 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.700517 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.710040 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.719527 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.728810 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.738211 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.747187 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.757410 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.768560 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.778918 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.788304 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.798005 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.810022 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.824118 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.836055 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.846180 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.855618 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.865334 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.874812 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.884231 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.893439 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.902775 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.914089 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.923342 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.932934 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.943043 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.952903 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.963212 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.973069 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.984529 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.994531 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.003758 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.013180 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.022668 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.032313 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.042009 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.051159 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.061153 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.074037 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.086219 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.095531 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.105135 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.114556 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.124897 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.135981 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.145743 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.155146 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.168381 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.178421 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.187753 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.197184 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.207112 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.216533 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.226727 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.238984 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.247952 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.259441 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.269434 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.279191 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.292356 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.304578 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.317072 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.326132 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.335902 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.345116 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.354227 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.366988 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.376159 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.385696 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.394808 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.409234 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.419401 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.428945 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.440016 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.451897 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.463659 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.473953 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.501247 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.516780 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.529608 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.539471 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.549020 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.558704 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.568382 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.580101 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.590416 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.602284 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.614386 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.628946 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.640556 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.649911 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.659129 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.670599 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.679506 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.689075 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.700720 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.713850 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.725242 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.734591 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.746005 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.755197 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.764825 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.778901 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.790734 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.801437 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.811499 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.826931 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.837116 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.850861 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.861089 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.870286 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.879332 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.888871 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.901265 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.910740 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.921548 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.933554 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.946357 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.955081 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.967714 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.980953 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.989846 [1] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.998881 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.008219 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.027586 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.041690 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.051538 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.060959 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.070424 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.079621 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.088849 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.100092 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.109459 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.118725 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.127864 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.136915 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.146065 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.162215 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.172184 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.182792 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.194730 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.203314 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.212877 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.224048 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.233874 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.243338 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.252787 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.265792 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.275499 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.284484 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.300092 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.310669 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.322686 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.331961 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.341207 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.353047 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.365247 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.377528 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.386542 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.395969 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.405606 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.415236 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.425004 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.437660 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.447041 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.459250 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.470922 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.487917 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.506224 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.519193 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.528817 [1] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.548784 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:48.144520 [1] proc begin: <DistEnv 1/4 nccl>
21:00:48.212409 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
21:00:48.223362 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:00:49.809752 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.559460 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.573687 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.586465 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.596934 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.606483 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.620450 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.632659 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.641932 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.651420 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.660668 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.673990 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.685624 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.696454 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.706162 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.715372 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.725210 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.734745 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.743979 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.753964 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.763165 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.772583 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.783805 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.793033 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.805300 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.814809 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.824311 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.833380 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.842677 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.851785 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.861453 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.874291 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.885208 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.897391 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.906844 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.916196 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.925897 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.937808 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.949091 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.958664 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.968158 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.977725 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.987211 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.996532 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.005945 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.015164 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.024655 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.034395 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.043524 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.052689 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.064210 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.074135 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.083448 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.092897 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.104652 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.113788 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.122831 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.135520 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.152679 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.167307 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.177614 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.187835 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.197367 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.207113 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.216439 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.227023 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.236387 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.245851 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.260862 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.271251 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.287537 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.303266 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.314091 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.323369 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.332563 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.341629 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.351032 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.360067 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.369164 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.378366 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.387704 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.397043 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.406171 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.417291 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.430169 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.441925 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.452987 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.462605 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.471947 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.491956 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.504739 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.515893 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.527788 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.536963 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.546640 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.557202 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.568343 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.577606 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.586844 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.596064 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.605682 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.614857 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.624018 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.633540 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.642760 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.651802 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.661194 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.676790 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.690800 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.702848 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.718283 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.737719 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.754516 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.764622 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.774796 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.783887 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.793113 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.802225 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.813019 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.823174 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.832177 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.841898 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.852045 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.861406 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.873581 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.885006 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.893969 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.903277 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.913507 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.925364 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.937007 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.947975 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.957499 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.966747 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.976112 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.985716 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.997400 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.006445 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.019165 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.030887 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.040171 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.049558 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.058476 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.073086 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.085984 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.095812 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.105756 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.119520 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.129167 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.138476 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.159113 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.172275 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.183286 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.192661 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.201605 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.214578 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.225162 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.233906 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.243064 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.262005 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.276047 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.304515 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.319243 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.333035 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.345580 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.355835 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.364850 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.375912 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.384712 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.393981 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.402973 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.411882 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.421202 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.433360 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.442956 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.452136 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.461565 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.473214 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.482449 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.491357 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.502923 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.513981 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.524192 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.535193 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.544513 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.553564 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.562370 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.571486 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.580562 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.589881 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.599203 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.608204 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.617289 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.626490 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.638476 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.649766 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.659225 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.671486 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.682305 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.692563 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:24.354027 [1] proc begin: <DistEnv 1/4 nccl>
21:07:24.407197 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
21:07:24.416958 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:07:26.516302 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.238727 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.254115 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.266567 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.279498 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.290079 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.299593 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.309036 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.321632 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.334319 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.343289 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.352709 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.363878 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.373906 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.383095 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.395100 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.403870 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.413308 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.423124 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.436647 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.446225 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.460505 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.476276 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.486585 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.496988 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.508283 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.519189 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.528532 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.540381 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.549627 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.559061 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.568409 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.578081 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.587056 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.600545 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.610734 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.619955 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.632625 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.646235 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.655785 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.665742 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.675460 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.684847 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.694733 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.704274 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.715791 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.730453 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.741567 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.755051 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.767313 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.776636 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.786327 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.795627 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.806880 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.816534 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.825755 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.834940 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.844258 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.855348 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.866505 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.882535 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.896219 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.907062 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.916220 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.927151 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.941393 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.951132 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.961191 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.972408 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.985344 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.998908 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.008265 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.020882 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.031995 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.042116 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.053392 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.067881 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.078093 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.090967 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.102268 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.115413 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.131044 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.143062 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.155987 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.165559 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.178267 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.190652 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.204738 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.214463 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.230048 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.245777 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.261440 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.283850 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.296193 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.308699 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.317518 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.328428 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.340172 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.349175 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.372261 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.386192 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.395510 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.407265 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.419843 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.433556 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.443816 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.452995 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.461996 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.474432 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.486593 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.496191 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.505078 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.513810 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.522409 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.531149 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.539908 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.548955 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.557761 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.566452 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.575129 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.584081 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.592733 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.605656 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.619514 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.632000 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.641857 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.650948 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.663296 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.674089 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.690297 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.700922 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.715223 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.728612 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.739036 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.748149 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.757071 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.765950 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.774736 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.783439 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.792282 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.801118 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.810481 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.821994 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.832811 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.842115 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.850937 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.860756 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.870246 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.878801 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.890824 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.900029 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.910239 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.924371 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.935190 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.946426 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.957751 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.969536 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.982331 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.992039 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.000891 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.009752 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.020515 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.030762 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.039388 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.048520 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.057239 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.066465 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.075281 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.084045 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.092807 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.105076 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.116042 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.129305 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.142047 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.153994 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.163074 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.172032 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.186271 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.201509 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.212610 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.222343 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.231212 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.240378 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.251764 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.270564 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.291219 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.305806 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.317478 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.326989 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.336219 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.345294 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.365336 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.381828 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.394424 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.404295 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.413227 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.421753 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.433185 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.446497 [1] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.456908 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:31.467918 [1] proc begin: <DistEnv 1/4 nccl>
21:09:31.497735 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
21:09:31.509223 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:09:32.801775 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.512115 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.527515 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.538764 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.547965 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.557578 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.570311 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.582090 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.591727 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.602912 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.614095 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.623312 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.636680 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.652117 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.662520 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.673673 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.684332 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.694383 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.704648 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.717273 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.729863 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.740898 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.752912 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.762904 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.774288 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.784884 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.796261 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.806066 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.815241 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.831562 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.844657 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.856451 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.870175 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.880073 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.889968 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.902163 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.912898 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.922251 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.934785 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.946606 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.956207 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.969489 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.980748 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.991737 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.006209 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.018156 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.030213 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.039511 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.048550 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.057937 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.067400 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.078338 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.089589 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.098783 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.111566 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.121863 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.132722 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.142621 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.154637 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.163662 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.172833 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.184298 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.193558 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.202947 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.212529 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.222317 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.231344 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.243171 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.254756 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.264611 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.275549 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.285474 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.295120 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.304766 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.322732 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.335007 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.344458 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.356101 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.367011 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.377357 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.390726 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.401498 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.412649 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.424039 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.435495 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.444890 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.466381 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.494453 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.508469 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.525203 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.535167 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.545547 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.558616 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.569567 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.579268 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.593120 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.605481 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.617929 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.634426 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.644950 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.657044 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.671405 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.681199 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.691035 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.701624 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.711225 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.720121 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.729452 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.739261 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.750285 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.760276 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.771525 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.781090 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.791054 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.804494 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.815257 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.825149 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.834600 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.843904 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.855805 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.866264 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.875303 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.884513 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.895775 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.906316 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.918365 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.926959 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.938718 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.947919 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.957261 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.966419 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.977870 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.988873 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.998587 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.007586 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.017306 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.026547 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.038647 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.048241 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.057709 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.071616 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.083608 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.092764 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.102405 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.111525 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.120431 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.130197 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.141105 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.153889 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.163601 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.181724 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.193971 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.206575 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.216314 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.227621 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.240522 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.249624 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.262502 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.276828 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.290050 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.301122 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.310274 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.321315 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.330740 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.340709 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.349746 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.359384 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.370594 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.382698 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.391834 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.401401 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.410682 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.419771 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.429275 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.438724 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.451479 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.474032 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.496918 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.507572 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.517402 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.528504 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.542117 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.550930 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.562400 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.575769 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.585482 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.594511 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.607074 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.616605 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.627299 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.641136 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.653344 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.666302 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.675745 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.684912 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.695316 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.704558 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.719529 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.730106 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.739773 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:50.839599 [1] proc begin: <DistEnv 1/4 nccl>
21:09:50.908126 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
21:09:50.920824 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:09:52.436099 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.178205 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.191749 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.205539 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.217320 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.228436 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.238314 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.247947 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.257403 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.269704 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.278988 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.288365 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.297836 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.307112 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.322386 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.335407 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.345938 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.356714 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.369470 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.381813 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.391298 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.400964 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.410565 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.420177 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.429527 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.439118 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.448458 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.458065 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.469828 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.479912 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.489664 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.500811 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.513736 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.528661 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.542353 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.555099 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.563930 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.572971 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.581805 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.591891 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.610578 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.624412 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.632988 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.642010 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.652683 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.664845 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.673874 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.683711 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.692866 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.702280 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.711136 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.726730 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.738747 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.752247 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.763884 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.773608 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.782644 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.791795 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.801003 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.814175 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.824251 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.840847 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.852241 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.862409 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.875046 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.886924 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.895833 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.909706 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.919506 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.929383 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.939058 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.960087 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.970859 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.980181 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.989315 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.000636 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.009893 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.018935 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.027703 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.036713 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.046367 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.055424 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.064664 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.073785 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.087349 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.097023 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.105972 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.115542 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.128521 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.141043 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.151254 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.162725 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.174386 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.185658 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.195535 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.207608 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.218085 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.227321 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.236281 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.245442 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.254187 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.263369 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.272249 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.281435 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.290456 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.299637 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.308510 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.317489 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.326233 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.335209 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.344104 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.353845 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.366178 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.378850 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.390876 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.403153 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.412104 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.421225 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.430037 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.439137 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.447980 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.458047 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.467783 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.479737 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.492231 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.504318 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.513062 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.522066 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.530750 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.539506 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.552253 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.560698 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.574926 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.587208 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.595838 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.605400 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.614393 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.623337 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.632263 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.641536 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.650426 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.659101 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.668963 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.681890 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.693215 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.702360 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.715104 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.723996 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.733610 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.747249 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.759893 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.773768 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.783471 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.792455 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.803180 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.814766 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.823563 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.834552 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.851375 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.862735 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.871982 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.880895 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.889649 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.898912 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.907636 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.916631 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.925440 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.934388 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.943254 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.968223 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.980631 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.990876 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.000083 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.009568 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.018620 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.027584 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.036471 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.045542 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.054198 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.063148 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.072037 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.080984 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.089782 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.098592 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.107569 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.116631 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.127247 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.136912 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.150104 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.159717 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.168605 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.177622 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.187009 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.199135 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.208120 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.216893 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.225934 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.234707 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.243746 [1] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.255274 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:37.733451 [1] proc begin: <DistEnv 1/4 nccl>
21:12:37.811986 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
21:12:37.821460 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:12:39.035113 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.739984 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.754054 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.763681 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.773111 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.782536 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.792114 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.801870 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.812705 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.825603 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.843894 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.857121 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.866520 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.878054 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.887217 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.896637 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.907293 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.919131 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.928765 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.942397 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.951687 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.963554 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.977395 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.989270 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.998522 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.008025 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.018180 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.027852 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.039963 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.053397 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.066164 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.075534 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.085744 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.095150 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.104458 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.113860 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.123649 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.135013 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.144741 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.154057 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.163451 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.177983 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.194373 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.206111 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.215649 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.226595 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.239252 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.252102 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.261884 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.277876 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.287760 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.297159 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.306214 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.315259 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.324409 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.335987 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.346700 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.357073 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.367350 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.380468 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.393113 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.402205 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.411314 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.420318 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.429541 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.439242 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.450631 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.463815 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.474793 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.485706 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.495001 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.505000 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.517452 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.534657 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.547099 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.559000 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.568202 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.579159 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.588086 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.602106 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.611943 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.623171 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.632045 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.641176 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.650745 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.663472 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.675708 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.685274 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.694727 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.713273 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.723599 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.755092 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.767382 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.776314 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.786176 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.798903 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.807706 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.820290 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.829242 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.838755 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.848157 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.859765 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.868950 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.878938 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.888298 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.897222 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.909349 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.920159 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.929254 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.938045 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.949253 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.957998 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.966713 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.975589 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.987775 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.996626 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.006364 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.016683 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.026187 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.035184 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.045314 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.054909 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.068456 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.078643 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.087376 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.097824 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.106848 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.115500 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.125355 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.137272 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.149463 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.160699 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.170836 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.180403 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.189821 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.199220 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.208373 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.217533 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.230170 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.246083 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.258731 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.268345 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.277716 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.286829 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.295954 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.304955 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.314037 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.327393 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.336762 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.349639 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.361857 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.370782 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.379800 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.388883 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.397832 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.406731 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.417103 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.429613 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.438593 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.447617 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.456408 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.465719 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.478030 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.487788 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.498372 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.508543 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.517701 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.531455 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.540260 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.549344 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.559551 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.568319 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.577690 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.586454 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.595435 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.610596 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.622799 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.631382 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.640500 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.653749 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.663446 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.672323 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.684184 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.695649 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.707440 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.716335 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.727223 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.741880 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.764444 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.777935 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.787474 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.798416 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.808379 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.824399 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.836518 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.848069 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.857060 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.869885 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.881606 [1] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.893168 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:12.681467 [1] proc begin: <DistEnv 1/4 nccl>
21:16:12.758724 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
21:16:12.769181 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:16:13.970185 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.698847 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.713733 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.723108 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.732745 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.744750 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.756284 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.769610 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.779149 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.787840 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.796830 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.807789 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.817168 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.826494 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.835665 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.844481 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.854323 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.863341 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.876228 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.885389 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.898234 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.916491 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.928711 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.937707 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.947048 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.955684 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.964884 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.976193 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.985971 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.999688 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.009953 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.022639 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.033349 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.044037 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.055117 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.065572 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.078650 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.089331 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.099440 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.110897 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.120488 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.132547 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.146148 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.161110 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.173610 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.183049 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.196486 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.205892 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.217918 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.228019 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.242068 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.253524 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.262922 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.272709 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.281908 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.291173 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.300395 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.318079 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.333226 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.342932 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.355327 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.366308 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.375642 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.386061 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.398799 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.410580 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.420152 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.429683 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.438686 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.447707 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.457015 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.466789 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.478759 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.492000 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.502969 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.519994 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.542019 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.552784 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.562268 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.571307 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.580227 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.590878 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.601973 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.612229 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.624747 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.638408 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.651199 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.663324 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.673936 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.702075 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.715177 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.740221 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.753784 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.764428 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.776423 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.786031 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.796999 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.806540 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.819078 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.828773 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.838367 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.847632 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.856833 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.869747 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.881106 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.891377 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.900937 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.912072 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.924012 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.935006 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.944150 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.953699 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.967722 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.981049 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.990608 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.999784 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.009440 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.018472 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.029436 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.039334 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.048403 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.057842 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.070853 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.080520 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.089937 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.102850 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.112918 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.122087 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.131423 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.140421 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.149770 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.165549 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.174925 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.184328 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.194048 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.203081 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.215341 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.224509 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.233711 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.242826 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.252137 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.261308 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.271917 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.281033 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.293460 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.305072 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.316344 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.328541 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.338602 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.347727 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.357260 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.370478 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.380788 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.390078 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.400193 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.412829 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.423616 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.432572 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.442376 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.451300 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.460666 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.470804 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.480082 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.489982 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.499271 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.508376 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.517741 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.526728 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.535804 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.544963 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.554304 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.566192 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.576059 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.585395 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.597622 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.607097 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.616483 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.627013 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.642694 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.653410 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.663386 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.672701 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.682268 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.702538 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.716921 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.726873 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.736213 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.745662 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.754989 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.764612 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.774190 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.783573 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.793386 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.802918 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.812885 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.822314 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.831462 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.840810 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.852419 [1] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.860945 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:43.211690 [1] proc begin: <DistEnv 1/4 nccl>
21:20:43.275156 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
21:20:43.284357 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:20:44.450329 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.143855 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.159661 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.169929 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.181416 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.192318 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.202776 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.216003 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.225358 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.238568 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.251889 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.263122 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.272490 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.284852 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.295418 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.307935 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.321226 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.334436 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.349730 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.359393 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.369270 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.379937 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.389682 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.398703 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.408429 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.421352 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.430501 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.439904 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.449010 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.458197 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.469207 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.478802 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.488334 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.500218 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.510716 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.520305 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.529791 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.538886 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.548345 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.560746 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.577914 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.590713 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.600359 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.612326 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.622822 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.632758 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.642003 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.651438 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.661046 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.670726 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.681822 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.692325 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.702559 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.713151 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.723920 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.736525 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.746310 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.755406 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.765022 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.778157 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.789392 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.799339 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.809405 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.819462 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.830184 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.840498 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.854661 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.871308 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.884107 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.895634 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.909717 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.919921 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.931020 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.941541 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.952021 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.965078 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.975958 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.995435 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.009027 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.022300 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.033248 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.044909 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.063763 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.078694 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.090263 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.102338 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.112748 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.124336 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.137208 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.157377 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.171852 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.183772 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.194129 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.217982 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.229787 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.239801 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.248383 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.259420 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.272209 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.284884 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.303668 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.313945 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.333307 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.345526 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.365876 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.378536 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.387631 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.396219 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.407769 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.417086 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.426468 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.435258 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.445498 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.454508 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.464324 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.473235 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.482335 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.493725 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.507492 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.520381 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.530431 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.539290 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.548300 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.560875 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.572204 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.581423 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.593759 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.603785 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.615837 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.625367 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.634184 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.643001 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.651783 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.663392 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.673370 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.683086 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.691683 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.700991 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.710355 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.718823 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.727979 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.737374 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.749621 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.761329 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.770570 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.779229 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.788121 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.797426 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.809427 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.818607 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.827475 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.843563 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.854741 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.864030 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.873011 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.881801 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.891300 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.900334 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.909511 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.921460 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.930595 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.939743 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.948895 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.960913 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.969536 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.978525 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.988108 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.999360 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.009957 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.019859 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.029767 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.041821 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.051043 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.064986 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.078047 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.086510 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.095358 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.104209 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.112979 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.121744 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.130269 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.139145 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.148073 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.157125 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.165837 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.174672 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.183228 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.192316 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.201479 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.210608 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.233361 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.243684 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.252884 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.262194 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.272813 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.281314 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.293760 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.305961 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.314583 [1] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.323597 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:32.445726 [1] proc begin: <DistEnv 1/4 nccl>
20:31:39.625343 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
20:31:39.648181 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:33:07.329790 [1] proc begin: <DistEnv 1/4 nccl>
20:33:13.499633 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
20:33:13.519564 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:59:42.887766 [1] proc begin: <DistEnv 1/4 nccl>
21:00:04.953670 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
21:00:04.981553 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:00:09.422553 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:10.975286 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:11.746625 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:12.519720 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:13.292409 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:14.064867 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:14.839379 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:15.611344 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:16.381608 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:17.153900 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:17.924298 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:18.697438 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:19.470246 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:20.239619 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:21.010489 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:21.783249 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:22.552806 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:23.321805 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:24.091364 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:24.859182 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:25.627594 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:26.395103 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:27.166076 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:27.933408 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:28.702742 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:29.470988 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:30.240213 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:31.008211 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:31.778666 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:32.586339 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:33.359016 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:34.128400 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:34.894994 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:35.663694 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:36.431810 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:37.196321 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:37.960950 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:38.726858 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:39.494195 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:40.262592 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:41.033197 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:41.804681 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:42.576418 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:43.347209 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:44.118312 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:44.889433 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:45.660802 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:46.430443 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:47.202183 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:47.974386 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:48.746410 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:49.516784 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.286708 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.056712 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.826458 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.596628 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:53.366940 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:54.140460 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:54.911704 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:55.683767 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:56.454654 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:57.224111 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:57.994892 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:58.766487 [1] Warning: no training nodes in this partition! Backward fake loss.
21:00:59.536777 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:00.307897 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:01.078276 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:01.851445 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:02.656596 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:03.458807 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:04.232826 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:05.002583 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:05.770926 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:06.538797 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:07.307306 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:08.076616 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:08.843558 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:09.716191 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:10.488609 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:11.261217 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:12.028075 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:12.795130 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:13.561308 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:14.327167 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:15.092567 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:15.862068 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:16.628871 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:17.401862 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:18.173018 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:18.950109 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:19.718715 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:20.491044 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:21.263406 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:22.035644 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:22.808328 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:23.581171 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:24.352999 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:25.125410 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:25.899091 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:26.671470 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:27.443503 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:28.215390 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:28.988374 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:29.759201 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:30.530636 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:31.301589 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:32.073010 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:32.841539 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:33.611215 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:34.381927 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:35.151786 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:35.921873 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:36.692188 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:37.463699 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:38.233948 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:39.003727 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:39.778682 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:40.547804 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:41.316392 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:42.085375 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:42.855015 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:43.624212 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:44.395022 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:45.428850 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:46.200378 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:46.971072 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:47.738676 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:48.506386 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:49.272588 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:50.037194 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:50.801381 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:51.564980 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:52.331555 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:53.096117 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:53.865323 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:54.634560 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:55.403545 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:56.173639 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:56.943528 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:57.713877 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:58.483381 [1] Warning: no training nodes in this partition! Backward fake loss.
21:01:59.253964 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:00.024719 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:00.795806 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:01.571687 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:02.378959 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:03.173087 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:03.944432 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:04.715796 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:05.487870 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:06.258580 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:07.031022 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:07.802370 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:08.573337 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:09.344605 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:10.113250 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:10.883198 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:11.654166 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:12.426348 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:13.196905 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:13.967650 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:14.738624 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:15.507206 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:16.274821 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:17.047145 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:17.819351 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:18.587877 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:19.356503 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:20.126003 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:20.896755 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:21.665871 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:22.646106 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:23.798464 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:24.568783 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:25.339263 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:26.105477 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:26.872030 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:27.637242 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:28.402128 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:29.167783 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:29.932951 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:30.699398 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:31.465903 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:32.235199 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:33.006182 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:33.778353 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:34.548851 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:35.319538 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:36.090320 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:36.861872 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:37.632871 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:38.404416 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:39.176562 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:39.948050 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:40.720176 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:41.491798 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:42.264019 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:43.036566 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:43.808561 [1] Warning: no training nodes in this partition! Backward fake loss.
21:02:44.580258 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:05.703524 [1] proc begin: <DistEnv 1/4 nccl>
14:32:22.046349 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:32:22.076192 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:32:32.107272 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:33.726986 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:34.493521 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:35.261158 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:36.029807 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:36.797421 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:37.566229 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:38.332854 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:39.101880 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:39.869385 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:40.636721 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:41.405637 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:42.174942 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:42.943170 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:43.712257 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:44.481520 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:45.249101 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:46.017249 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:46.785321 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:47.553833 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:48.322483 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:49.089587 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:49.860131 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:50.629648 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:51.398858 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:52.166687 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:52.935195 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:53.703657 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:54.471454 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:55.239415 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:56.006877 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:56.775100 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:57.543074 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:58.311141 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:59.080029 [1] Warning: no training nodes in this partition! Backward fake loss.
14:32:59.848532 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:00.615780 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:01.383725 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:02.154090 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:02.955802 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:03.747558 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:04.514977 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:05.282372 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:06.048554 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:06.816535 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:07.585294 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:08.352911 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:09.121506 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:09.889842 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:10.658619 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:11.426687 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:12.195390 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:12.964599 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:13.733258 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:14.502787 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:15.274050 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:16.044088 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:16.813914 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:17.585642 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:18.357665 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:19.126647 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:19.897724 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:20.669130 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:21.438624 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:22.207254 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:22.976637 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:23.745744 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:24.514963 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:25.279783 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:26.045178 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:26.810189 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:27.575522 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:28.341204 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:29.105818 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:29.870626 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:30.636906 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:31.402243 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:32.167267 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:32.933476 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:33.701108 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:34.468060 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:35.235696 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:36.002126 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:36.768882 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:37.534885 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:38.301473 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:39.068088 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:39.834200 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:40.607995 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:41.443161 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:42.460348 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:43.709610 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:45.002452 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:46.325420 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:47.654083 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:48.981952 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:50.306813 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:51.635576 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:52.955985 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:54.277194 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:55.604411 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:56.935446 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:58.265362 [1] Warning: no training nodes in this partition! Backward fake loss.
14:33:59.600278 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:00.935989 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:01.793277 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:02.594897 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:03.385830 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:04.152563 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:04.919992 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:05.685920 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:06.451790 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:07.218674 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:07.984559 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:08.751151 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:09.518168 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:10.284646 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:11.051067 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:11.817259 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:12.584486 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:13.351747 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:14.118567 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:14.885849 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:15.653715 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:16.420753 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:17.188666 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:17.956964 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:18.725257 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:19.493724 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:20.263478 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:21.030722 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:21.797845 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:22.565657 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:23.333157 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:24.099657 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:24.866454 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:25.632728 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:26.398903 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:27.163453 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:27.929348 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:28.695053 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:29.461109 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:30.227359 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:30.993468 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:31.761160 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:32.527752 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:33.293506 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:34.060715 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:34.829204 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:35.596932 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:36.364727 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:37.133124 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:37.901264 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:38.670131 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:39.438926 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:40.207789 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:40.975503 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:41.744652 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:42.513190 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:43.280783 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:44.049117 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:44.816779 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:45.585213 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:46.353262 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:47.120934 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:47.888276 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:48.656181 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:49.423448 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:50.192238 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:50.962168 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:51.731236 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:52.501313 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:53.271040 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:54.042480 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:54.811697 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:55.578756 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:56.346017 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:57.113255 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:57.879774 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:58.646390 [1] Warning: no training nodes in this partition! Backward fake loss.
14:34:59.413430 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:00.179224 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:00.946193 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:01.730683 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:02.533970 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:03.314378 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:04.082748 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:04.851626 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:05.621456 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:06.390228 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:07.159571 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:07.928477 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:08.698319 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:09.466916 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:10.235721 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:11.004285 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:11.773543 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:12.542763 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:13.309315 [1] Warning: no training nodes in this partition! Backward fake loss.
14:35:14.076173 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:04.049588 [1] proc begin: <DistEnv 1/4 nccl>
14:40:08.932236 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:40:08.940599 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:40:14.509823 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:16.130516 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:17.000382 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:17.868707 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:18.738829 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:19.608560 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:20.481859 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:21.350689 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:22.222229 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:23.093826 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:23.963133 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:24.832284 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:25.702792 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:26.574552 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:27.445325 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:28.316918 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:29.187260 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:30.055729 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:30.923989 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:31.792944 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:32.661724 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:33.530925 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:34.399375 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:35.268263 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:36.135755 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:37.003359 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:37.871383 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:38.739638 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:39.605496 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:40.472161 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:41.338975 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:42.205976 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:43.075203 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:43.945621 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:44.812273 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:45.679914 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:46.549591 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:47.417160 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:48.286014 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:49.156149 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:50.025625 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:50.902110 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:51.771939 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:52.640178 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:53.513681 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:54.386469 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:55.259209 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:56.132196 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:57.006430 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:57.876846 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:58.751388 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:59.626045 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:00.498601 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:01.372654 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:02.281671 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:03.181545 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:04.052938 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:04.924719 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:05.795777 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:06.666162 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:07.538662 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:08.410811 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:09.283037 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:10.156016 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:11.027987 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:11.900732 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:12.772636 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:13.646847 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:14.517811 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:15.391812 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:16.264753 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:17.136358 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:18.009384 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:18.881815 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:19.753072 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:20.625314 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:21.495142 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:22.363774 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:23.234540 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:24.103514 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:24.973743 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:25.844213 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:26.712960 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:27.578765 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:28.446714 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:29.315030 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:30.185722 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:31.056587 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:31.928830 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:32.800649 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:33.672503 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:34.543819 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:35.417160 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:36.290372 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:37.163935 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:38.037254 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:38.911438 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:39.784112 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:40.657161 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:41.530234 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:42.400995 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:43.275215 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:44.148077 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:45.021709 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:45.895372 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:46.766667 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:47.639024 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:48.512882 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:49.378934 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:50.245303 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:51.111675 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:51.976666 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:52.842529 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:53.711299 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:54.580816 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:55.450195 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:56.321648 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:57.191542 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:58.059312 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:58.927842 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:59.795651 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:00.665258 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:01.533007 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:02.423889 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:03.335192 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:04.211133 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:05.083352 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:05.956129 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:06.828523 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:07.703307 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:08.575382 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:09.445261 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:10.313310 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:11.180788 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:12.048309 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:12.913655 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:13.778417 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:14.643441 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:15.508378 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:16.373820 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:17.240070 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:18.105479 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:18.970553 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:19.835678 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:20.701802 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:21.568766 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:22.437589 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:23.305229 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:24.172436 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:25.039713 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:25.906311 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:26.773224 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:27.640378 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:28.507225 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:29.374473 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:30.241625 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:31.109167 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:31.976482 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:32.843042 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:33.710121 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:34.578027 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:35.442924 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:36.308985 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:37.174531 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:38.039761 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:38.904822 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:39.769554 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:40.635153 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:41.503646 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:42.374311 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:43.242404 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:44.110906 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:44.980147 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:45.847866 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:46.716472 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:47.585064 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:48.456476 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:49.330413 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:50.202736 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:51.076464 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:51.949969 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:52.822188 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:53.695020 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:54.569074 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:55.439351 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:56.311944 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:57.186696 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:58.060030 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:58.932953 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:59.808312 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:00.680122 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:01.553600 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:02.466178 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:03.365380 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:04.238300 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:05.113189 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:05.986620 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:06.860274 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:07.732908 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:08.604394 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:54.965769 [1] proc begin: <DistEnv 1/4 nccl>
14:44:59.198226 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:44:59.205845 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:45:05.623470 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:07.207186 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:08.082550 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:08.957406 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:09.833422 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:10.709555 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:11.585437 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:12.461161 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:13.335806 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:14.212250 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:15.084299 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:15.956248 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:16.828802 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:17.698012 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:18.568640 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:19.441567 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:20.314837 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:21.187401 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:22.057435 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:22.928152 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:23.801144 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:24.671509 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:25.542065 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:26.411887 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:27.283162 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:28.155346 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:29.028326 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:29.900683 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:30.773370 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:31.648169 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:32.521629 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:33.394809 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:34.268709 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:35.141672 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:36.014470 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:36.888016 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:37.762219 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:38.632362 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:39.504357 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:40.377744 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:41.249537 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:42.124994 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:42.997615 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:43.869191 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:44.740162 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:45.614418 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:46.488309 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:47.362402 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:48.235188 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.108572 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.979111 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.853568 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.727598 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:52.600106 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:53.473886 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:54.345247 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:55.216676 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:56.089656 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:56.960426 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:57.832919 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:58.704623 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:59.578549 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:00.453128 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:01.328134 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:02.240993 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:03.134654 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:04.012364 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:04.889791 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:05.764497 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:06.637036 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:07.512089 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:08.385863 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:09.260126 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:10.135306 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:11.010652 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:11.886778 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:12.762850 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:13.636330 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:14.514138 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:15.389679 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:16.262185 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:17.132578 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:18.002460 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:18.874523 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:19.747520 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:20.622338 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:21.496839 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:22.372442 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:23.249297 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:24.127411 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:25.003058 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:25.879185 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:26.750688 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:27.619852 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:28.487437 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:29.354561 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:30.221827 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:31.090815 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:31.961656 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:32.831366 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:33.699930 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:34.568806 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:35.440135 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:36.310592 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:37.179727 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:38.050078 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:38.918452 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:39.787109 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:40.656274 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:41.525387 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:42.396693 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:43.269088 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:44.138872 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:45.008135 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:45.878944 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:46.748079 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:47.617172 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:48.486175 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:49.356222 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:50.225898 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:51.095380 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:51.963825 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:52.831839 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:53.700281 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:54.569185 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:55.437360 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:56.305544 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:57.176191 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:58.046851 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:58.920112 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:59.793272 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:00.663857 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:01.536265 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:02.434000 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:03.351059 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:04.226346 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:05.098690 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:05.970916 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:06.845121 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:07.717203 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:08.587842 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:09.458161 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:10.327077 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:11.196330 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:12.066256 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:12.935486 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:13.803998 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:14.672780 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:15.541734 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:16.411927 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:17.281364 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:18.150588 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:19.019013 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:19.887199 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:20.755554 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:21.626824 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:22.496720 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:23.364532 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:24.232242 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:25.100254 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:25.969891 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:26.841929 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:27.712473 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:28.583042 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:29.454382 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:30.326052 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:31.198047 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:32.070789 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:32.942285 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:33.814435 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:34.686391 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:35.557173 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:36.426313 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:37.302343 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:38.172597 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:39.043407 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:39.914442 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:40.784938 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:41.654589 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:42.525119 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:43.393788 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:44.263732 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:45.134115 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:46.003617 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:46.878151 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:47.748826 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:48.618078 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:49.488919 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:50.361120 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:51.233875 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:52.106247 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:52.978328 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:53.848194 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:54.718429 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:55.590185 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:56.463313 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:57.337705 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:58.211338 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:59.084757 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:59.955669 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:14.336991 [1] proc begin: <DistEnv 1/4 nccl>
14:59:18.309795 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:59:18.315859 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:59:23.846841 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:25.776699 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:27.092357 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:28.413773 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:29.733309 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:31.048878 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:32.356702 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:33.667494 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:34.978725 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:36.293218 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:37.604454 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:38.913808 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:40.221089 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:41.526075 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:42.830731 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:44.134739 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:45.439742 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:46.744585 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:48.047625 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:49.353145 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:50.657696 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:51.962971 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:53.269590 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:54.572893 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:55.877717 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:57.180935 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:58.487211 [1] Warning: no training nodes in this partition! Backward fake loss.
14:59:59.793205 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:01.098939 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:02.446729 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:03.765217 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:05.068692 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:06.373208 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:07.679021 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:08.982798 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.286790 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.597636 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:12.390188 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:13.152522 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:13.914371 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:14.676082 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:15.436911 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:16.200109 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:16.961595 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:17.721149 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:18.482254 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:19.243426 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:20.004331 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:20.765251 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:21.525634 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:22.285619 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:23.047094 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:23.807072 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:24.567653 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:25.327742 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:26.087798 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:26.847247 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.608379 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:28.368174 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:29.128296 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:29.887731 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:30.649619 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:31.409131 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:32.168814 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:32.928078 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:33.687079 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:34.445922 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:35.204422 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:35.963911 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:36.724211 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:37.483490 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:38.242257 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:39.001422 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:39.761621 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:40.522439 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:41.282856 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:42.043678 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:42.803448 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:43.563958 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:44.323700 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:45.085343 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:45.846265 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:46.606644 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:47.366479 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:48.126963 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:48.887206 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:49.647145 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:50.407821 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:51.168191 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:51.927744 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:52.691310 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:53.452237 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:54.214936 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:54.978419 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:55.739892 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:56.502804 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:57.263074 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:58.032242 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:58.794379 [1] Warning: no training nodes in this partition! Backward fake loss.
15:00:59.555891 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:00.317708 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:01.079682 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:01.841944 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:02.633202 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:03.428628 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:04.189024 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:04.950659 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:05.712104 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:06.484516 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:07.259860 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:08.036447 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:08.812564 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:09.590390 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:10.366631 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:11.142592 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:11.920225 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:12.696038 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:13.472422 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:14.248362 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:15.024834 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:15.800225 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:16.576452 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:17.353677 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:18.128072 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:18.904146 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:19.679094 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:20.454700 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:21.228450 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:22.003749 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:22.777434 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:23.553284 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:24.328308 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:25.103257 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:25.877853 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:26.651829 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:27.426803 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:28.200550 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:28.974824 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:29.749931 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:30.524884 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:31.299675 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:32.073862 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:32.849035 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:33.624621 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:34.401001 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:35.175968 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:35.952094 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:36.727683 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:37.503675 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:38.279228 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:39.053771 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:39.829466 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:40.604698 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:41.380427 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:42.156215 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:42.931700 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:43.707086 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:44.483642 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:45.259727 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:46.036029 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:46.812026 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:47.587796 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:48.363200 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:49.138693 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:49.915491 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:50.690898 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:51.467147 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:52.242766 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:53.019262 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:53.796945 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:54.573211 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:55.349010 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:56.124932 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:56.899927 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:57.676119 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.451826 [1] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.227304 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:00.003762 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:00.780487 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:01.556320 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:02.363467 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:03.166571 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:03.942309 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:04.719044 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:05.494967 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:06.271320 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:07.047818 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:07.823473 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:08.599325 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:09.376187 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:10.153292 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:10.929974 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:11.706163 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:12.481473 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:13.257041 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:14.033210 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:14.809202 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:15.585689 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:16.361604 [1] Warning: no training nodes in this partition! Backward fake loss.
15:02:17.137859 [1] Warning: no training nodes in this partition! Backward fake loss.
14:03:56.440383 [1] proc begin: <DistEnv 1/4 nccl>
14:04:12.048247 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:04:12.197229 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:04:24.058179 [1] Warning: no training nodes in this partition! Backward fake loss.
14:04:25.684605 [1] Warning: no training nodes in this partition! Backward fake loss.
14:04:26.445917 [1] Warning: no training nodes in this partition! Backward fake loss.
14:04:27.207770 [1] Warning: no training nodes in this partition! Backward fake loss.
14:04:27.969513 [1] Warning: no training nodes in this partition! Backward fake loss.
14:04:28.732436 [1] Warning: no training nodes in this partition! Backward fake loss.
14:04:29.493815 [1] Warning: no training nodes in this partition! Backward fake loss.
14:04:30.256122 [1] Warning: no training nodes in this partition! Backward fake loss.
14:04:31.016978 [1] Warning: no training nodes in this partition! Backward fake loss.
14:04:31.778811 [1] Warning: no training nodes in this partition! Backward fake loss.
14:04:32.540982 [1] Warning: no training nodes in this partition! Backward fake loss.
14:04:33.302211 [1] Warning: no training nodes in this partition! Backward fake loss.
14:04:34.063789 [1] Warning: no training nodes in this partition! Backward fake loss.
14:04:34.825520 [1] Warning: no training nodes in this partition! Backward fake loss.
14:04:35.588285 [1] Warning: no training nodes in this partition! Backward fake loss.
14:04:36.351932 [1] Warning: no training nodes in this partition! Backward fake loss.
14:04:37.116393 [1] Warning: no training nodes in this partition! Backward fake loss.
14:04:37.879979 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:19.506172 [1] proc begin: <DistEnv 1/4 nccl>
14:09:24.919192 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:09:25.027040 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:09:29.913084 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:31.800899 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:32.565354 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:33.327311 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:34.089872 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:34.854754 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:35.617434 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:36.379484 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:37.143058 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:37.905976 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:38.669395 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:39.431777 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:40.193779 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:40.955395 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:41.719923 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:42.483325 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:43.247210 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:44.009686 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:44.773461 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:45.536846 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:46.299804 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:47.062819 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:47.825110 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:48.587240 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:49.349283 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:50.113433 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:16.758549 [1] proc begin: <DistEnv 1/4 nccl>
14:10:22.168628 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:10:22.210486 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:10:28.619616 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:30.536404 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:31.302159 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:32.068473 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:32.834651 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:33.601870 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:34.367828 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:35.134369 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:35.900220 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:36.663905 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:37.428771 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:38.193212 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:38.958574 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:39.723458 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:40.488714 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:41.254282 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:42.018062 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:42.783564 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:43.549119 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:44.314639 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:45.079057 [1] Warning: no training nodes in this partition! Backward fake loss.
14:18:18.314742 [1] proc begin: <DistEnv 1/4 nccl>
14:18:52.412508 [1] proc begin: <DistEnv 1/4 nccl>
14:18:57.454964 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
14:18:57.489526 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:34:48.606714 [1] proc begin: <DistEnv 1/4 nccl>
14:34:54.300501 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
14:34:54.319873 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:43:13.666952 [1] proc begin: <DistEnv 1/4 nccl>
14:43:18.287481 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:43:18.304223 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:43:23.920116 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:26.637221 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:27.953580 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:29.359787 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:30.692990 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:32.040913 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:33.305164 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:34.465177 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:35.855439 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:37.190511 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:38.347894 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:39.695928 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:41.086305 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:42.429948 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:43.693510 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:45.037657 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:46.375233 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:47.712711 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:49.050666 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:50.387291 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:51.719301 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:53.054238 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:54.387137 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:55.802135 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:57.135304 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:58.470206 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:59.809658 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:01.142951 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:02.502652 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:03.706769 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:05.074532 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:06.407013 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:07.747201 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:08.913495 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:10.191152 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:11.569505 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:12.910342 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:14.251993 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:15.597235 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:16.935649 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:18.268067 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:19.604538 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:20.942256 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:22.274481 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:23.610520 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:24.948332 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:26.290586 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:27.621431 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:28.952913 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:30.291107 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:31.622782 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:32.955383 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:34.289064 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:35.627642 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:36.781317 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:38.123840 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:39.468121 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:40.812322 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.001752 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.415028 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:44.709259 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:46.056979 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:47.399052 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:48.701373 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:49.895946 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:51.237861 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:52.580629 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:53.814993 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:55.147970 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:56.495114 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:57.843446 [1] Warning: no training nodes in this partition! Backward fake loss.
14:44:59.191423 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:00.535560 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:01.873550 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:03.312289 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:04.644947 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:05.982942 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:07.321852 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:08.658378 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:09.993428 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:11.324709 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:12.585769 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:13.813572 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:15.174652 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:16.521669 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:17.716285 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:19.092424 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:20.437980 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:21.676289 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:23.060353 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:24.342178 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:25.683710 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:27.029275 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:28.375048 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:29.721947 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:31.065914 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:32.405993 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:33.743440 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:35.078738 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:36.411602 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:37.658146 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:39.033124 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:40.301213 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:41.512841 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:42.877286 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:44.220982 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:45.565713 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:46.910579 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:48.256916 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.602535 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.939263 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:52.273077 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:53.613571 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:54.954544 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:56.294401 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:57.629925 [1] Warning: no training nodes in this partition! Backward fake loss.
14:45:58.888679 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:00.105094 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:01.483350 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:02.729375 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:04.156344 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:05.425584 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:06.773446 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:08.120139 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:09.284201 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:10.679312 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:12.017231 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:13.171292 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:14.578691 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:15.841338 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:17.189639 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:18.447603 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:19.731162 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:21.105463 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:22.452295 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:23.614862 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:25.012046 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:26.360695 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:27.624859 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:28.969548 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:30.308973 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:31.606744 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:32.799142 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:34.138768 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:35.383627 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:36.725083 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:38.062175 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:39.406266 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:40.750737 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:42.093800 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:43.250819 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:44.585375 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:45.929256 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:47.173946 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:48.443251 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:49.841825 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:51.184841 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:52.528405 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:53.868448 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:55.204511 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:56.538050 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:57.872267 [1] Warning: no training nodes in this partition! Backward fake loss.
14:46:59.207494 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:00.546751 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:01.981745 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:03.320162 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:04.632158 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:05.966846 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:07.303772 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:08.636721 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:09.970767 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:11.305789 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:12.640528 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:13.975105 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:15.304997 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:16.644963 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:17.983598 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:19.327642 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:20.658704 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:21.957643 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:23.312330 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:24.641066 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:25.913526 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:27.272418 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:28.629862 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:29.956075 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:31.284885 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:32.586171 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:33.829733 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:35.229594 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:36.518117 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:37.863740 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:39.210154 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:40.556305 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:41.899950 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:43.243872 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:44.588007 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:45.927599 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:47.153766 [1] Warning: no training nodes in this partition! Backward fake loss.
14:47:48.542907 [1] Warning: no training nodes in this partition! Backward fake loss.
16:31:49.817321 [1] proc begin: <DistEnv 1/4 nccl>
16:31:54.924700 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
16:31:54.933528 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:31:59.434501 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:01.609047 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:03.035855 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:04.370663 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:05.731167 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:07.001682 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:08.384835 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:09.647978 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:11.067159 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:12.409671 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:13.749939 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:15.091791 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:16.439470 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:17.781430 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:19.120717 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:20.457949 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:21.862570 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:23.188340 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:24.513987 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:25.839667 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:27.053759 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:28.425445 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:29.654573 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:31.047956 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:32.338065 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:33.678513 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:35.023217 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:36.362356 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:37.707014 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:38.893101 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:40.265237 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:41.591024 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:42.808324 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:44.177770 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:45.518944 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:46.860972 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:48.209882 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:49.551496 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:50.896396 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:52.318683 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:53.644909 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:54.942871 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:56.151918 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:57.556058 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:58.746612 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:00.139958 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:01.464653 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:02.831755 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:04.165149 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:05.505365 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:06.848649 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:08.183264 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:09.512406 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:10.840658 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:12.176423 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:13.509002 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:14.835878 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:16.175042 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:17.510877 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:18.836870 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:19.994481 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:21.399569 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:22.766445 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:23.936594 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:25.342856 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:26.684529 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:28.025883 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:29.214076 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:30.602561 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:31.931617 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:33.133479 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:34.530346 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:35.873622 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:37.220625 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:38.564092 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:39.903839 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:41.247934 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:42.595289 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:43.935804 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:45.272908 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:46.685839 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:48.011753 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:49.338750 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:50.671055 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:52.002886 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:53.205835 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:54.585905 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:55.921441 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:57.268245 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:58.678729 [1] Warning: no training nodes in this partition! Backward fake loss.
16:33:59.957034 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:01.295492 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:02.727567 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:04.085076 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:05.274611 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:06.717964 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:08.067776 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:09.346059 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:10.687174 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:12.029882 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:13.376199 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:14.721596 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:16.063854 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:17.406132 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:18.654582 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:20.054198 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:21.393444 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:22.593177 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:24.038251 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:25.310721 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:26.660062 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:27.886821 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:29.281715 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:30.585453 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:31.781051 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:33.218591 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:34.555935 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:35.896676 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:37.054154 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:38.468395 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:39.791770 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:41.039272 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:42.424487 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:43.765155 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:45.002320 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:46.397064 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:47.691369 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:48.929631 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:50.331319 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:51.659972 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:52.926873 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:54.212245 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:55.592845 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:56.755335 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:58.145837 [1] Warning: no training nodes in this partition! Backward fake loss.
16:34:59.446966 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:00.699164 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:02.082819 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:03.512134 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:04.729668 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:06.110053 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:07.455064 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:08.803542 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:10.059065 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:11.444430 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:12.770709 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:14.109474 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:15.447459 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:16.790006 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:18.139394 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:19.478669 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:20.818366 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:22.151675 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:23.477550 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:24.805387 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:26.131510 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:27.297056 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:28.682868 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:29.983934 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:31.256311 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:32.653399 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:33.991898 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:35.179524 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:36.577236 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:37.773955 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:39.160666 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:40.502783 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:41.846434 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:43.188168 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:44.528888 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:45.782695 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:47.117150 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:48.535137 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:49.709240 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:51.028457 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:52.442620 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:53.770007 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:55.112249 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:56.429666 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:57.645182 [1] Warning: no training nodes in this partition! Backward fake loss.
16:35:59.057613 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:00.249536 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:01.707443 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:03.074872 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:04.414709 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:05.755206 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:07.095306 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:08.438493 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:09.773889 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:11.101912 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:12.429238 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:13.559212 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:14.958039 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:16.284165 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:17.627004 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:18.833658 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:20.295145 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:21.637635 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:22.978857 [1] Warning: no training nodes in this partition! Backward fake loss.
16:36:24.320232 [1] Warning: no training nodes in this partition! Backward fake loss.
19:15:54.591636 [1] proc begin: <DistEnv 1/4 nccl>
19:15:59.613769 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
19:15:59.976540 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:16:55.761106 [1] proc begin: <DistEnv 1/4 nccl>
19:17:01.435676 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
19:17:01.473664 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:30:46.333837 [1] proc begin: <DistEnv 1/4 nccl>
19:30:52.288450 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
19:30:52.305641 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:32:54.846013 [1] proc begin: <DistEnv 1/4 nccl>
19:33:00.053818 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
19:33:00.414659 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:33:50.559927 [1] proc begin: <DistEnv 1/4 nccl>
19:33:56.439279 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
19:33:56.460916 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:35:02.157887 [1] proc begin: <DistEnv 1/4 nccl>
19:35:08.531051 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
19:35:08.552930 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:46:47.418178 [1] proc begin: <DistEnv 1/4 nccl>
22:46:51.369399 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
22:46:51.377155 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

08:22:44.335917 [1] proc begin: <DistEnv 1/4 nccl>
08:22:49.462992 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
08:22:49.479003 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

08:22:55.820543 [1] Warning: no training nodes in this partition! Backward fake loss.
08:22:57.644997 [1] Warning: no training nodes in this partition! Backward fake loss.
08:22:58.352730 [1] Warning: no training nodes in this partition! Backward fake loss.
08:22:59.058771 [1] Warning: no training nodes in this partition! Backward fake loss.
08:22:59.765752 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:00.474515 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:01.184789 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:01.924870 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:02.661204 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:03.374857 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:04.085839 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:04.793976 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:05.502592 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:06.209739 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:06.918137 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:07.625439 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:08.333235 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:09.040901 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:09.748259 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:10.453514 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:11.159933 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:11.865078 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:12.571605 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:13.277895 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:13.983599 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:14.687620 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:15.395112 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:16.100455 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:16.806351 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:17.511391 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:18.218712 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:18.924821 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:19.630709 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:20.336451 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:21.041897 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:21.747272 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:22.453268 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:23.158742 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:23.863459 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:24.569190 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:25.276895 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:25.981325 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:26.686220 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:27.391568 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:28.096450 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:28.801227 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:29.507154 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:30.212308 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:30.917980 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:31.622248 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:32.329405 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:32.798285 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:33.554176 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:34.023570 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:34.781337 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:35.249984 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:36.005408 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:36.474009 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:37.230433 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:37.698741 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:38.455445 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:38.923016 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:39.677189 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:40.144970 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:40.899496 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:41.368109 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:42.122394 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:42.591019 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:43.346198 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:43.814301 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:44.569502 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:45.036862 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:45.791713 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:46.259339 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:47.013578 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:47.481662 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:48.236654 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:48.704805 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:49.460623 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:49.928139 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:50.683115 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:51.151243 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:51.906683 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:52.374069 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:53.128213 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:53.596255 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:54.352044 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:54.819588 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:55.576303 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:56.043668 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:56.798408 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:57.266788 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:58.021526 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:58.488289 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:59.242228 [1] Warning: no training nodes in this partition! Backward fake loss.
08:23:59.709244 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:00.463285 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:00.929691 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:01.684832 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:02.159650 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:02.942792 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:03.425417 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:04.185658 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:04.655545 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:05.416373 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:05.886488 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:06.645787 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:07.115748 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:07.875458 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:08.344564 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:09.104576 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:09.574373 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:10.333212 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:10.804057 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:11.564372 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:12.033704 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:12.795807 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:13.265896 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:14.024185 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:14.492289 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:15.248391 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:15.716224 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:16.473594 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:16.943002 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:17.699702 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:18.167725 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:18.926511 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:19.393982 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:20.149374 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:20.616453 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:21.373263 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:21.840692 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:22.596440 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:23.063641 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:23.819426 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:24.285713 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:25.040248 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:25.506169 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:26.259497 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:26.725449 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:27.480216 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:27.947055 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:28.701453 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:29.167957 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:29.922332 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:30.388714 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:31.143796 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:31.609778 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:32.363118 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:32.829542 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:33.584539 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:34.050852 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:34.805351 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:35.271264 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:36.025708 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:36.492863 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:37.247468 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:37.713613 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:38.469043 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:38.935656 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:39.689679 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:40.156037 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:40.910367 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:41.376568 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:42.131570 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:42.598548 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:43.354206 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:43.820465 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:44.575559 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:45.041983 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:45.796177 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:46.262868 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:47.016971 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:47.483406 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:48.238201 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:48.704798 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:49.460759 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:49.928074 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:50.681753 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:51.147765 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:51.901734 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:52.368899 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:53.123289 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:53.589624 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:54.344224 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:54.810700 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:55.565592 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:56.031743 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:56.785883 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:57.252095 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:58.006797 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:58.473301 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:59.227442 [1] Warning: no training nodes in this partition! Backward fake loss.
08:24:59.693918 [1] Warning: no training nodes in this partition! Backward fake loss.
08:25:00.448530 [1] Warning: no training nodes in this partition! Backward fake loss.
08:25:00.914890 [1] Warning: no training nodes in this partition! Backward fake loss.
08:25:01.670301 [1] Warning: no training nodes in this partition! Backward fake loss.
08:25:02.155652 [1] Warning: no training nodes in this partition! Backward fake loss.
08:25:02.942358 [1] Warning: no training nodes in this partition! Backward fake loss.
08:25:03.416954 [1] Warning: no training nodes in this partition! Backward fake loss.
08:26:31.222519 [1] proc begin: <DistEnv 1/4 nccl>
08:26:37.927589 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
08:26:37.946030 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

08:28:24.994373 [1] proc begin: <DistEnv 1/4 nccl>
08:28:31.600957 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
08:28:31.618841 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:38:03.476621 [1] proc begin: <DistEnv 1/4 nccl>
10:38:10.514661 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
10:38:10.532391 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:40:00.818101 [1] proc begin: <DistEnv 1/4 nccl>
10:40:07.941164 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
10:40:07.959686 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:42:04.715529 [1] proc begin: <DistEnv 1/4 nccl>
10:42:10.443765 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
10:42:10.779329 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:43:37.862942 [1] proc begin: <DistEnv 1/4 nccl>
10:43:43.385762 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
10:43:43.392968 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:43:47.782345 [1] Warning: no training nodes in this partition! Backward fake loss.
10:43:49.027629 [1] Warning: no training nodes in this partition! Backward fake loss.
10:43:49.732630 [1] Warning: no training nodes in this partition! Backward fake loss.
10:43:50.438055 [1] Warning: no training nodes in this partition! Backward fake loss.
10:43:51.143015 [1] Warning: no training nodes in this partition! Backward fake loss.
10:43:51.848712 [1] Warning: no training nodes in this partition! Backward fake loss.
10:43:52.556326 [1] Warning: no training nodes in this partition! Backward fake loss.
10:43:53.260775 [1] Warning: no training nodes in this partition! Backward fake loss.
10:43:53.967469 [1] Warning: no training nodes in this partition! Backward fake loss.
10:43:54.672917 [1] Warning: no training nodes in this partition! Backward fake loss.
10:43:55.377184 [1] Warning: no training nodes in this partition! Backward fake loss.
10:43:56.081306 [1] Warning: no training nodes in this partition! Backward fake loss.
10:43:56.786262 [1] Warning: no training nodes in this partition! Backward fake loss.
10:43:57.491130 [1] Warning: no training nodes in this partition! Backward fake loss.
10:43:58.196386 [1] Warning: no training nodes in this partition! Backward fake loss.
10:43:58.901177 [1] Warning: no training nodes in this partition! Backward fake loss.
10:43:59.606723 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:00.311148 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:01.015691 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:01.735158 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:02.467012 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:03.190109 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:03.895615 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:04.603425 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:05.309806 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:06.017329 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:06.723251 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:07.429401 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:08.134839 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:08.841031 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:09.549377 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:10.255237 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:10.961257 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:11.666876 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:12.371328 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:13.076038 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:13.782430 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:14.488781 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:15.195113 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:15.900571 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:16.605818 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:17.310656 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:18.017085 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:18.724497 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:19.431015 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:20.136422 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:20.840669 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:21.545768 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:22.249732 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:22.954461 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:23.659412 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:24.127791 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:24.883521 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:25.351135 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:26.106749 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:26.575520 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:27.330764 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:27.799403 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:28.555047 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:29.023264 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:29.779167 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:30.248116 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:31.004487 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:31.472654 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:32.228138 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:32.696263 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:33.453240 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:33.922086 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:34.679739 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:35.147571 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:35.904188 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:36.372460 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:37.129173 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:37.597415 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:38.353965 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:38.822345 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:39.581611 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:40.050005 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:40.806877 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:41.274694 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:42.033081 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:42.500984 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:43.260990 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:43.728508 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:44.485512 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:44.953034 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:45.708609 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:46.177328 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:46.931685 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:47.399872 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:48.154351 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:48.622221 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:49.377156 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:49.845625 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:50.600943 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:51.068212 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:51.823062 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:52.291128 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:53.046543 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:53.514689 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:54.269968 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:54.738697 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:55.493363 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:55.960865 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:56.715591 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:57.183699 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:57.938042 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:58.404812 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:59.158801 [1] Warning: no training nodes in this partition! Backward fake loss.
10:44:59.625558 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:00.379684 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:00.848159 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:01.603892 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:02.072129 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:02.851961 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:03.336629 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:04.107511 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:04.576624 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:05.332559 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:05.801500 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:06.557765 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:07.027504 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:07.783436 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:08.252813 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:09.009216 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:09.479862 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:10.237960 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:10.706335 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:11.462464 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:11.932004 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:12.690473 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:13.159404 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:13.918073 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:14.386320 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:15.143283 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:15.612468 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:16.368974 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:16.837263 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:17.592459 [1] Warning: no training nodes in this partition! Backward fake loss.
10:45:18.060275 [1] Warning: no training nodes in this partition! Backward fake loss.
10:49:47.403501 [1] proc begin: <DistEnv 1/4 nccl>
10:49:51.573146 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
10:49:51.581212 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:49:58.518465 [1] Warning: no training nodes in this partition! Backward fake loss.
10:50:01.445706 [1] Warning: no training nodes in this partition! Backward fake loss.
10:50:03.325348 [1] Warning: no training nodes in this partition! Backward fake loss.
10:50:05.151484 [1] Warning: no training nodes in this partition! Backward fake loss.
10:50:06.979684 [1] Warning: no training nodes in this partition! Backward fake loss.
10:50:08.809650 [1] Warning: no training nodes in this partition! Backward fake loss.
10:50:10.639507 [1] Warning: no training nodes in this partition! Backward fake loss.
10:50:12.464964 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:14.536922 [1] proc begin: <DistEnv 1/4 nccl>
14:00:20.413066 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:00:20.424883 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:00:24.909572 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:26.376712 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:26.806924 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:27.237417 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:27.666950 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:28.097339 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:28.528229 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:28.963723 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:29.395018 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:29.825116 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:30.255743 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:30.685830 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:31.115667 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:31.546392 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:31.977261 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:32.408022 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:32.838860 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:33.270055 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:33.701109 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:34.132228 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:34.561961 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:34.991980 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:35.422550 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:35.852916 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:36.284078 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:36.714993 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:37.145384 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:37.576180 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:38.006959 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:38.437320 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:38.867136 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:39.296595 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:39.726573 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:40.156838 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:40.587593 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:41.018101 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:41.447857 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:41.878028 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:42.308084 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:42.738000 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:43.168163 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:43.597349 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:44.027331 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:44.456876 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:44.885865 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:45.315119 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:45.744009 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:46.173794 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:46.603335 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:47.033225 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:47.463834 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:47.763946 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:48.232300 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:48.532251 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:49.000537 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:49.300953 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:49.767979 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:50.068130 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:50.536820 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:50.836144 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:51.303540 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:51.603533 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:52.071121 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:52.371266 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:52.838787 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:53.138851 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:53.607022 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:53.907035 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:54.374473 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:54.675335 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:55.143173 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:55.443390 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:55.911075 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:56.211374 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:56.679545 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:56.979436 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:57.448398 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:57.748744 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:58.217771 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:58.516681 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:58.985506 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:59.285962 [1] Warning: no training nodes in this partition! Backward fake loss.
14:00:59.755962 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:00.056878 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:00.525337 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:00.825219 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:01.294772 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:01.594882 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:02.063531 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:02.376067 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:02.863344 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:03.173884 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:03.655897 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:03.956741 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:04.427581 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:04.727984 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:05.198768 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:05.499755 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:05.970103 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:06.270569 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:06.740283 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:07.040332 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:07.512065 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:07.813614 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:08.284308 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:08.585599 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:09.056491 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:09.358172 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:09.828427 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:10.129295 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:10.601659 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:10.903817 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:11.374045 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:11.675898 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:12.146390 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:12.446361 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:12.916374 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:13.217486 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:13.687813 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:13.988473 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:14.458321 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:14.759173 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:15.229781 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:15.530349 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:16.000880 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:16.301918 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:16.771848 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:17.070952 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:17.539635 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:17.839506 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:18.309380 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:18.609590 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:19.079422 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:19.379377 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:19.848309 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:20.148945 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:20.617735 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:20.918199 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:21.385855 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:21.684856 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:22.152062 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:22.451818 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:22.920531 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:23.219847 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:23.687565 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:23.987443 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:24.455068 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:24.754892 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:25.222659 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:25.522813 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:25.990504 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:26.289904 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:26.758428 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:27.058024 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:27.526126 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:27.825531 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:28.293632 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:28.593725 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:29.061948 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:29.362342 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:29.830620 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:30.130914 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:30.599340 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:30.899893 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:31.367804 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:31.668047 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:32.136609 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:32.436594 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:32.906324 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:33.207885 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:33.676422 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:33.976150 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:34.444463 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:34.745287 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:35.214170 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:35.514258 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:35.983308 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:36.283893 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:36.752232 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:37.052181 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:37.521335 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:37.824368 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:38.293984 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:38.594151 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:39.062702 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:39.363187 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:39.831567 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:40.131886 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:40.600608 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:40.902336 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:41.370639 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:41.671067 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:42.139600 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:42.440272 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:42.909034 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:43.209608 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:43.678581 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:43.979488 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:44.449164 [1] Warning: no training nodes in this partition! Backward fake loss.
14:01:44.750427 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:13.719512 [1] proc begin: <DistEnv 1/4 nccl>
16:28:18.731878 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
16:28:18.746973 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:28:24.569457 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:26.479265 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:27.187425 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:27.896366 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:28.606736 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:29.316092 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:30.025253 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:30.733710 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:31.443043 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:32.150719 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:32.857885 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:33.567166 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:34.275728 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:34.982411 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:35.692042 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:36.402832 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:37.111380 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:37.820156 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:38.529131 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:39.236811 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:39.944870 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:40.651862 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:41.359987 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:42.067352 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:42.775089 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:43.481838 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:44.189100 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:44.897133 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:45.605236 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:46.312250 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:47.019572 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:47.726911 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:48.434930 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:49.142703 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:49.851107 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:50.558400 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:51.266283 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:51.973158 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:52.681228 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:53.387873 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:54.096823 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:54.803335 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:55.511320 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:56.218276 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:56.926037 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:57.634339 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:58.342066 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:59.050310 [1] Warning: no training nodes in this partition! Backward fake loss.
16:28:59.759227 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:00.466735 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:01.174170 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:01.641567 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:02.431529 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:02.916306 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:03.694053 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:04.162121 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:04.926529 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:05.394300 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:06.159011 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:06.626536 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:07.390358 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:07.858081 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:08.622504 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:09.090520 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:09.853926 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:10.321414 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:11.084102 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:11.550835 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:12.313547 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:12.781515 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:13.542618 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:14.010060 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:14.771697 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:15.239332 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:16.002417 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:16.469696 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:17.231597 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:17.699637 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:18.462430 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:18.929342 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:19.691805 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:20.159244 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:20.920921 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:21.388064 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:22.150889 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:22.617258 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:23.379769 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:23.847059 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:24.609591 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:25.076487 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:25.838311 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:26.305602 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:27.067247 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:27.534448 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:28.297339 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:28.766083 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:29.528500 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:29.995966 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:30.758156 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:31.224732 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:31.987421 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:32.454429 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:33.219678 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:33.686628 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:34.449508 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:34.916306 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:35.680201 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:36.146444 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:36.910333 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:37.377412 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:38.139148 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:38.605896 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:39.367938 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:39.835180 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:40.597724 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:41.064807 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:41.829202 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:42.297137 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:43.061742 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:43.529002 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:44.294156 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:44.761551 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:45.525086 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:45.993707 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:46.758255 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:47.226518 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:47.990845 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:48.458394 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:49.222314 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:49.689764 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:50.453669 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:50.919455 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:51.682604 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:52.148517 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:52.913122 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:53.379323 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:54.143326 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:54.609918 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:55.372067 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:55.838463 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:56.599211 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:57.066133 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:57.827608 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:58.294005 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:59.057522 [1] Warning: no training nodes in this partition! Backward fake loss.
16:29:59.523704 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:00.285520 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:00.750997 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:01.519508 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:02.001494 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:02.793833 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:03.263259 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:04.025558 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:04.493231 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:05.256170 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:05.724099 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:06.487366 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:06.954717 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:07.717178 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:08.184506 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:08.946393 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:09.413420 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:10.177556 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:10.644748 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:11.409325 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:11.875505 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:12.639061 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:13.106030 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:13.869227 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:14.336512 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:15.098331 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:15.564945 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:16.328714 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:16.795028 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:17.557548 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:18.025284 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:18.788479 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:19.255346 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:20.017330 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:20.484597 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:21.245802 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:21.712588 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:22.473078 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:22.939291 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:23.700070 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:24.166871 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:24.929337 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:25.395042 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:26.156097 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:26.622803 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:27.384469 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:27.850398 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:28.612977 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:29.079865 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:29.841450 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:30.307281 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:31.068281 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:31.534282 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:32.296141 [1] Warning: no training nodes in this partition! Backward fake loss.
16:30:32.762232 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:04.636125 [1] proc begin: <DistEnv 1/4 nccl>
16:32:09.772530 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
16:32:10.083404 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:32:15.078917 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:16.874459 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:17.584078 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:18.295676 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:19.005318 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:19.712906 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:20.420768 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:21.130503 [1] Warning: no training nodes in this partition! Backward fake loss.
16:32:42.024134 [1] proc begin: <DistEnv 1/4 nccl>
16:32:48.224512 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:32:48.241970 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:40:52.953478 [1] proc begin: <DistEnv 1/4 nccl>
16:40:58.410469 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:40:58.428121 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:47:03.682279 [1] proc begin: <DistEnv 1/4 nccl>
16:47:08.802737 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
16:47:08.812796 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:47:13.142938 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:13.986762 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.124867 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.280385 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.435531 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.592272 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.747883 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.902445 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.057488 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.212881 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.367437 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.522329 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.677083 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.831867 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.986795 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.141689 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.297295 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.453603 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.608715 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.768514 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.920283 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.076776 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.231513 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.386710 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.542030 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.696521 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.852199 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.007445 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.162794 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.319419 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.473790 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.629128 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.784760 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.941013 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.095531 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.251367 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.406612 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.561536 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.716941 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.870719 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.025895 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.179750 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.334734 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.489753 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.644039 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.799215 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.954494 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.108123 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.262368 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.417293 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.571907 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.727175 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.882442 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.037855 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.192420 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.347237 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.502522 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.656414 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.812201 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.966137 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.120703 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.276496 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.431448 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.585941 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.740636 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.895212 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.050736 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.205359 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.360158 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.516427 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.670858 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.828597 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.982053 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.137289 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.292036 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.447195 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.602603 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.757310 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.913598 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.068062 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.223788 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.377992 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.532705 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.688008 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.842268 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.997651 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.153094 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.307268 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.462044 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.618520 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.772655 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.927667 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.082201 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.237885 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.393225 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.547531 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.703117 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.857599 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.012587 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.167255 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.322985 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.478127 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.632706 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.789291 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.943106 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.098561 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.253886 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.408787 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.562821 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.717536 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.871784 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.028781 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.183638 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.338373 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.491975 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.646245 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.802922 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.957038 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.111735 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.266405 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.421177 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.576447 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.731177 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.885708 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.041289 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.195227 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.349982 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.504464 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.660262 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.814195 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.968458 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.123162 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.277892 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.432238 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.587364 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.742909 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.897555 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.052114 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.206711 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.366027 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.517806 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.671785 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.826259 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.981874 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.136013 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.291110 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.445323 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.600048 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.754266 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.908413 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.062775 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.217709 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.371720 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.525934 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.680262 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.834752 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.989327 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.143104 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.297229 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.451408 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.605438 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.760009 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.915327 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.069622 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.223960 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.378384 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.532963 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.687377 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.842846 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.996512 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.150975 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.305320 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.460503 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.615087 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.769575 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.924970 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.079463 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.233780 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.390056 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.544575 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.699391 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.854099 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.009798 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.164231 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.318815 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.473202 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.627701 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.782869 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.937573 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.092431 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.247675 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.402217 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.556978 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.712012 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.866710 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.021376 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.175878 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.331011 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.485057 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.639519 [1] Warning: no training nodes in this partition! Backward fake loss.
15:45:35.462760 [1] proc begin: <DistEnv 1/4 nccl>
15:45:53.445029 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
15:45:53.468684 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:46:08.911685 [1] proc begin: <DistEnv 1/4 nccl>
15:46:09.039952 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
15:46:09.082702 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:50:26.705293 [1] proc begin: <DistEnv 1/4 nccl>
15:50:26.899566 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
15:50:26.918457 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:50:28.032267 [1] L1 tensor(91906.2109, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:52:58.329729 [1] proc begin: <DistEnv 1/4 nccl>
15:52:58.360571 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
15:52:58.400557 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:52:59.546109 [1] L1 tensor(91906.2109, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:00.315201 [1] L2 tensor(438.5738, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:00.371257 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.107507 [1] L1 tensor(91832.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.145478 [1] L2 tensor(442.4138, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.149199 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.221925 [1] L1 tensor(91855.8203, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.226391 [1] L2 tensor(445.8104, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.250754 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.303129 [1] L1 tensor(91963.5078, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.326107 [1] L2 tensor(448.3091, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.374549 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.405758 [1] L1 tensor(92143.7031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.453069 [1] L2 tensor(450.3099, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.456754 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.529298 [1] L1 tensor(92384.4141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.534146 [1] L2 tensor(452.1290, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.557440 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.615919 [1] L1 tensor(92675.2109, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.638601 [1] L2 tensor(454.0485, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.661599 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.695499 [1] L1 tensor(93008.1797, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.699574 [1] L2 tensor(456.0366, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.725226 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.778241 [1] L1 tensor(93376.7422, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.800544 [1] L2 tensor(457.9691, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.846040 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.879752 [1] L1 tensor(93775.1719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.927679 [1] L2 tensor(459.7110, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:01.932120 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.003873 [1] L1 tensor(94198.1797, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.010446 [1] L2 tensor(461.1994, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.034241 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.086069 [1] L1 tensor(94641.2031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.112269 [1] L2 tensor(462.4901, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.156186 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.191741 [1] L1 tensor(95100.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.236821 [1] L2 tensor(463.6808, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.240110 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.313648 [1] L1 tensor(95573.6953, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.319039 [1] L2 tensor(464.8087, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.343193 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.397394 [1] L1 tensor(96059.2422, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.419351 [1] L2 tensor(465.8306, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.465515 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.497994 [1] L1 tensor(96555.1172, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.543755 [1] L2 tensor(466.6919, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.546900 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.620244 [1] L1 tensor(97059.4141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.625437 [1] L2 tensor(467.4421, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.651564 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.706329 [1] L1 tensor(97571.1094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.730143 [1] L2 tensor(468.2280, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.753934 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.786423 [1] L1 tensor(98089.2344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.813388 [1] L2 tensor(469.1948, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.855040 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.891792 [1] L1 tensor(98612.5547, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.936027 [1] L2 tensor(470.4097, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:02.939268 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.013293 [1] L1 tensor(99140.3125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.018447 [1] L2 tensor(471.9096, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.044558 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.074182 [1] L1 tensor(99671.9844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.078045 [1] L2 tensor(473.7235, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.099403 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.137329 [1] L1 tensor(100206.1875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.179389 [1] L2 tensor(475.8296, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.185007 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.259177 [1] L1 tensor(100741.9688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.262536 [1] L2 tensor(478.1782, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.290501 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.343057 [1] L1 tensor(101279.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.369342 [1] L2 tensor(480.7126, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.412878 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.444901 [1] L1 tensor(101816.7188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.491869 [1] L2 tensor(483.3638, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.494544 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.569783 [1] L1 tensor(102353.3750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.572677 [1] L2 tensor(486.0809, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.600375 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.652655 [1] L1 tensor(102887.5938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.677279 [1] L2 tensor(488.8194, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.716036 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.755005 [1] L1 tensor(103417.3438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.799568 [1] L2 tensor(491.5266, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.801982 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.876777 [1] L1 tensor(103939.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.879664 [1] L2 tensor(494.1455, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.907238 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.962551 [1] L1 tensor(104451.4609, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:03.985855 [1] L2 tensor(496.6190, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.008780 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.020203 [1] L1 tensor(104950.7891, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.045799 [1] L2 tensor(498.9019, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.069830 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.124088 [1] L1 tensor(105436.0234, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.149053 [1] L2 tensor(500.9770, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.187814 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.226946 [1] L1 tensor(105906.2500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.271541 [1] L2 tensor(502.8625, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.274116 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.348924 [1] L1 tensor(106361.8828, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.351847 [1] L2 tensor(504.6041, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.379412 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.431758 [1] L1 tensor(106804.1875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.453931 [1] L2 tensor(506.2580, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.492758 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.531309 [1] L1 tensor(107234.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.578267 [1] L2 tensor(507.8826, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.580691 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.654880 [1] L1 tensor(107654.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.657850 [1] L2 tensor(509.5375, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.685539 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.737856 [1] L1 tensor(108064.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.761574 [1] L2 tensor(511.2832, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.800288 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.837576 [1] L1 tensor(108465.7812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.884493 [1] L2 tensor(513.1110, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.886942 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.959954 [1] L1 tensor(108857.4531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.965178 [1] L2 tensor(514.9779, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:04.990733 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.046778 [1] L1 tensor(109240.2734, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.069298 [1] L2 tensor(516.8826, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.105741 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.144379 [1] L1 tensor(109615.5781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.191307 [1] L2 tensor(518.7771, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.193752 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.266734 [1] L1 tensor(109982.1172, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.271956 [1] L2 tensor(520.5668, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.297432 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.350225 [1] L1 tensor(110338.7422, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.374903 [1] L2 tensor(522.2884, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.413802 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.450210 [1] L1 tensor(110692.7422, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.496924 [1] L2 tensor(524.0200, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.499361 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.572288 [1] L1 tensor(111051.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.575151 [1] L2 tensor(525.9103, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.602680 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.655678 [1] L1 tensor(111417.1562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.680252 [1] L2 tensor(528.1434, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.716642 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.755075 [1] L1 tensor(111786.0312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.801961 [1] L2 tensor(530.8064, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.804487 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.877693 [1] L1 tensor(112153.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.880613 [1] L2 tensor(533.7210, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.908245 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.960356 [1] L1 tensor(112519.6016, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:05.985068 [1] L2 tensor(536.6836, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.032287 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.063154 [1] L1 tensor(112882.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.108301 [1] L2 tensor(539.6154, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.111120 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.186398 [1] L1 tensor(113245.2031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.191573 [1] L2 tensor(542.6844, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.217091 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.265237 [1] L1 tensor(113612.4688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.269310 [1] L2 tensor(546.6610, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.294906 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.351562 [1] L1 tensor(113989.8672, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.371782 [1] L2 tensor(551.9967, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.410714 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.452883 [1] L1 tensor(114380.3281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.497270 [1] L2 tensor(558.6206, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.499662 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.574561 [1] L1 tensor(114780.7344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.577466 [1] L2 tensor(565.5466, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.605022 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.661146 [1] L1 tensor(115179.1719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.683601 [1] L2 tensor(571.8951, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.720180 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.761992 [1] L1 tensor(115564.8125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.806607 [1] L2 tensor(577.6732, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.809003 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.882011 [1] L1 tensor(115930.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.884850 [1] L2 tensor(582.9789, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.912444 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.963797 [1] L1 tensor(116275.6094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:06.989056 [1] L2 tensor(587.8942, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.025400 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.063996 [1] L1 tensor(116604.0625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.111039 [1] L2 tensor(592.4735, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.113628 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.188343 [1] L1 tensor(116924.6953, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.193782 [1] L2 tensor(596.7705, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.219447 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.275105 [1] L1 tensor(117243.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.297625 [1] L2 tensor(600.9476, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.334316 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.374343 [1] L1 tensor(117559.8125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.418245 [1] L2 tensor(605.2831, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.423594 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.498690 [1] L1 tensor(117869.3125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.504037 [1] L2 tensor(609.3708, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.529177 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.585794 [1] L1 tensor(118170.4219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.608031 [1] L2 tensor(613.2331, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.652068 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.687624 [1] L1 tensor(118466.0312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.729202 [1] L2 tensor(616.9452, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.734555 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.809411 [1] L1 tensor(118757.4688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.815801 [1] L2 tensor(620.6169, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.840694 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.892144 [1] L1 tensor(119055.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.916246 [1] L2 tensor(624.6840, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:07.943680 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.994654 [1] L1 tensor(119353.8672, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.015967 [1] L2 tensor(629.0459, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.041239 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.116266 [1] L1 tensor(119651.8281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.121568 [1] L2 tensor(633.7097, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.147067 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.201805 [1] L1 tensor(119951.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.226185 [1] L2 tensor(638.6428, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.262877 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.305589 [1] L1 tensor(120246.9609, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.351303 [1] L2 tensor(643.5590, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.355070 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.429320 [1] L1 tensor(120528.7812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.434735 [1] L2 tensor(648.2156, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.460329 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.516918 [1] L1 tensor(120794.8438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.539461 [1] L2 tensor(652.5349, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.578792 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.619761 [1] L1 tensor(121047.7188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.650453 [1] L2 tensor(656.5295, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.667026 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.719268 [1] L1 tensor(121292.0703, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.725037 [1] L2 tensor(660.2439, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.728273 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.759976 [1] L1 tensor(121528.4375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.764350 [1] L2 tensor(663.6747, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.789869 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.843132 [1] L1 tensor(121754.7969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.867687 [1] L2 tensor(666.8158, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.906691 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.944381 [1] L1 tensor(121969.1562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.991915 [1] L2 tensor(669.6705, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:08.994377 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.069101 [1] L1 tensor(122171.3672, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.074453 [1] L2 tensor(672.2507, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.099954 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.152611 [1] L1 tensor(122361.7891, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.177311 [1] L2 tensor(674.5707, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.214055 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.257097 [1] L1 tensor(122540.7188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.301423 [1] L2 tensor(676.6443, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.304594 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.379982 [1] L1 tensor(122708.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.384729 [1] L2 tensor(678.4858, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.410273 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.466563 [1] L1 tensor(122865.5859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.486393 [1] L2 tensor(680.1101, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.532288 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.568618 [1] L1 tensor(123012.6484, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.612807 [1] L2 tensor(681.5339, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.616028 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.690517 [1] L1 tensor(123150.4297, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.695436 [1] L2 tensor(682.7748, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.720917 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.752638 [1] L1 tensor(123279.7344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.756138 [1] L2 tensor(683.8511, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.759651 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.793236 [1] L1 tensor(123401.2266, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.797173 [1] L2 tensor(684.7797, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.820070 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.877095 [1] L1 tensor(123515.4062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.897850 [1] L2 tensor(685.5751, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:09.944006 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.979792 [1] L1 tensor(123622.6016, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.021784 [1] L2 tensor(686.2515, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.027245 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.101678 [1] L1 tensor(123723.1094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.106656 [1] L2 tensor(686.8276, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.132222 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.188565 [1] L1 tensor(123817.4922, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.208522 [1] L2 tensor(687.3495, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.247825 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.290266 [1] L1 tensor(123907.2188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.311530 [1] L2 tensor(687.9585, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.316816 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.326073 [1] L1 tensor(123994.1172, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.351689 [1] L2 tensor(688.7996, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.372595 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.427440 [1] L1 tensor(124077.1094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.431217 [1] L2 tensor(689.6537, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.457213 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.513623 [1] L1 tensor(124155.7812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.533400 [1] L2 tensor(690.4276, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.579404 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.615128 [1] L1 tensor(124230.6875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.636283 [1] L2 tensor(691.1127, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.638636 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.645981 [1] L1 tensor(124302., device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.649689 [1] L2 tensor(691.7106, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.654710 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.665731 [1] L1 tensor(124369.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.672166 [1] L2 tensor(692.2195, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.675760 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.686637 [1] L1 tensor(124433.0625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.689953 [1] L2 tensor(692.6395, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.693053 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.704557 [1] L1 tensor(124492.7578, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.710768 [1] L2 tensor(692.9738, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.715275 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.722118 [1] L1 tensor(124548.8281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.725198 [1] L2 tensor(693.2272, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.727755 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.734252 [1] L1 tensor(124601.5703, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.737260 [1] L2 tensor(693.4050, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.740113 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.747355 [1] L1 tensor(124651.2734, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.749921 [1] L2 tensor(693.5128, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.752476 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.758642 [1] L1 tensor(124698.2031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.761573 [1] L2 tensor(693.5555, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.763868 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.770556 [1] L1 tensor(124742.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.773296 [1] L2 tensor(693.5380, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.775895 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.782993 [1] L1 tensor(124784.8281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.785873 [1] L2 tensor(693.4649, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.788797 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.796021 [1] L1 tensor(124825.0234, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.798928 [1] L2 tensor(693.3407, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.801598 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.808716 [1] L1 tensor(124863.3828, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.811528 [1] L2 tensor(693.1694, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.814017 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.821791 [1] L1 tensor(124900.1250, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.824238 [1] L2 tensor(692.9548, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.826609 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.834016 [1] L1 tensor(124935.3750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.837215 [1] L2 tensor(692.7004, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.839759 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.848004 [1] L1 tensor(124969.2734, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.851317 [1] L2 tensor(692.4095, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.854321 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.860433 [1] L1 tensor(125001.8828, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.863132 [1] L2 tensor(692.0851, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.865766 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.873164 [1] L1 tensor(125033.2969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.875724 [1] L2 tensor(691.7300, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.878107 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.886757 [1] L1 tensor(125063.5781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.893946 [1] L2 tensor(691.3472, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.896512 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.905596 [1] L1 tensor(125092.7812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.908673 [1] L2 tensor(690.9393, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.911420 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.918158 [1] L1 tensor(125120.9609, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.920689 [1] L2 tensor(690.5090, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.923310 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.929553 [1] L1 tensor(125148.1797, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.932231 [1] L2 tensor(690.0588, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.934644 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.941937 [1] L1 tensor(125174.4922, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.944540 [1] L2 tensor(689.5912, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.947257 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.953732 [1] L1 tensor(125199.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.956319 [1] L2 tensor(689.1080, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.958629 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.964131 [1] L1 tensor(125224.6094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.967884 [1] L2 tensor(688.6113, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.970471 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.976541 [1] L1 tensor(125248.5312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.979185 [1] L2 tensor(688.1028, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.981646 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.988023 [1] L1 tensor(125271.7812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.990625 [1] L2 tensor(687.5839, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:10.992968 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.000785 [1] L1 tensor(125294.3984, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.003424 [1] L2 tensor(687.0554, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.005732 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.013369 [1] L1 tensor(125316.4453, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.016294 [1] L2 tensor(686.5184, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.018940 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.027349 [1] L1 tensor(125337.9688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.030764 [1] L2 tensor(685.9733, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.034086 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.042407 [1] L1 tensor(125359.0312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.045252 [1] L2 tensor(685.4206, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.047755 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.055701 [1] L1 tensor(125379.6875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.058166 [1] L2 tensor(684.8604, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.060857 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.068459 [1] L1 tensor(125399.9844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.071149 [1] L2 tensor(684.2928, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.073569 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.081028 [1] L1 tensor(125419.9609, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.083544 [1] L2 tensor(683.7180, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.085851 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.093849 [1] L1 tensor(125439.6953, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.097229 [1] L2 tensor(683.1360, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.101223 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.109213 [1] L1 tensor(125459.2188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.112046 [1] L2 tensor(682.5469, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.115314 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.124958 [1] L1 tensor(125478.6172, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.127600 [1] L2 tensor(681.9510, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.129925 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.136256 [1] L1 tensor(125497.9297, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.138966 [1] L2 tensor(681.3486, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.141685 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.149920 [1] L1 tensor(125517.2578, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.152702 [1] L2 tensor(680.7401, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.155265 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.162984 [1] L1 tensor(125536.6797, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.165669 [1] L2 tensor(680.1261, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.168191 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.175628 [1] L1 tensor(125556.3047, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.178323 [1] L2 tensor(679.5076, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.181218 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.191914 [1] L1 tensor(125576.2812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.198328 [1] L2 tensor(678.8854, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.202984 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.214078 [1] L1 tensor(125596.7422, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.217245 [1] L2 tensor(678.2612, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.220866 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.231948 [1] L1 tensor(125617.9062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.238391 [1] L2 tensor(677.6365, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.243158 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.253407 [1] L1 tensor(125639.9375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.257621 [1] L2 tensor(677.0134, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.261015 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.267681 [1] L1 tensor(125663.0312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.270294 [1] L2 tensor(676.3940, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.272590 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.278955 [1] L1 tensor(125687.2578, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.281901 [1] L2 tensor(675.7798, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.284167 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.292321 [1] L1 tensor(125712.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.295487 [1] L2 tensor(675.1722, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.298273 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.306059 [1] L1 tensor(125739.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.308547 [1] L2 tensor(674.5723, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.310848 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.316696 [1] L1 tensor(125766.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.319309 [1] L2 tensor(673.9815, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.321626 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.327906 [1] L1 tensor(125795.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.331463 [1] L2 tensor(673.4016, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.333945 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.342911 [1] L1 tensor(125824.4844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.345804 [1] L2 tensor(672.8350, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.348402 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.355336 [1] L1 tensor(125854.6250, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.357745 [1] L2 tensor(672.2845, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.360333 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.366554 [1] L1 tensor(125885.4609, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.369367 [1] L2 tensor(671.7540, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.371832 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.378483 [1] L1 tensor(125916.9141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.381090 [1] L2 tensor(671.2478, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.383418 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.389799 [1] L1 tensor(125948.8203, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.392386 [1] L2 tensor(670.7714, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.394669 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.400726 [1] L1 tensor(125981.0156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.403515 [1] L2 tensor(670.3314, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.405856 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.412351 [1] L1 tensor(126013.2656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.415030 [1] L2 tensor(669.9373, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.417589 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.423473 [1] L1 tensor(126045.3281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.425963 [1] L2 tensor(669.6034, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.428421 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.436704 [1] L1 tensor(126077., device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.439316 [1] L2 tensor(669.3516, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.441851 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.448719 [1] L1 tensor(126108.1641, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.451355 [1] L2 tensor(669.2137, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.453844 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.459895 [1] L1 tensor(126138.6797, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.462759 [1] L2 tensor(669.2305, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.465196 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.471844 [1] L1 tensor(126168.5391, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.477226 [1] L2 tensor(669.4468, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.482278 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.493345 [1] L1 tensor(126197.7188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.497606 [1] L2 tensor(669.9043, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.501599 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.511439 [1] L1 tensor(126226.2656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.516570 [1] L2 tensor(670.6352, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.520047 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.529295 [1] L1 tensor(126254.2500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.532455 [1] L2 tensor(671.6612, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.535421 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.541565 [1] L1 tensor(126281.7969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.544257 [1] L2 tensor(672.9954, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.546718 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.553100 [1] L1 tensor(126309.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.555515 [1] L2 tensor(674.6449, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.558024 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.566882 [1] L1 tensor(126336.4375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.570774 [1] L2 tensor(676.6127, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.574757 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.580548 [1] L1 tensor(126364.2031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.583389 [1] L2 tensor(678.8989, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.586082 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.592416 [1] L1 tensor(126392.9219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.594994 [1] L2 tensor(681.5011, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.597553 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.603309 [1] L1 tensor(126423.2500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.605906 [1] L2 tensor(684.4136, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.608258 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.615420 [1] L1 tensor(126455.8672, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.617740 [1] L2 tensor(687.6250, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.620038 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.626773 [1] L1 tensor(126491.2344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.629503 [1] L2 tensor(691.1115, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.632346 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.638549 [1] L1 tensor(126529.3906, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.641328 [1] L2 tensor(694.8279, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.643711 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.650652 [1] L1 tensor(126570.0078, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.653636 [1] L2 tensor(698.7007, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.656580 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.663620 [1] L1 tensor(126612.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.666155 [1] L2 tensor(702.6328, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.668706 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.676824 [1] L1 tensor(126656.5859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.679766 [1] L2 tensor(706.5289, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.682338 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.688696 [1] L1 tensor(126701.7656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.691153 [1] L2 tensor(710.3220, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.693435 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.699558 [1] L1 tensor(126748.0078, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.704145 [1] L2 tensor(713.9838, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.706718 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.715081 [1] L1 tensor(126795.2266, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.717519 [1] L2 tensor(717.5139, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.720511 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.729965 [1] L1 tensor(126842.9688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.733774 [1] L2 tensor(720.9189, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.737889 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.748631 [1] L1 tensor(126890.2969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.752706 [1] L2 tensor(724.1981, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.756482 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.765155 [1] L1 tensor(126936.3516, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.768075 [1] L2 tensor(727.3528, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.771353 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.781514 [1] L1 tensor(126980.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.784013 [1] L2 tensor(730.3943, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.786497 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.792293 [1] L1 tensor(127022.9766, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.794713 [1] L2 tensor(733.3419, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.797079 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.804160 [1] L1 tensor(127063.2969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.809611 [1] L2 tensor(736.2206, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.814693 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.825282 [1] L1 tensor(127101.6484, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.829323 [1] L2 tensor(739.0592, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.833340 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.842516 [1] L1 tensor(127138.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.848453 [1] L2 tensor(741.8875, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.852028 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.861874 [1] L1 tensor(127172.7656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.865788 [1] L2 tensor(744.7334, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.869811 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.879275 [1] L1 tensor(127205.7969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.883636 [1] L2 tensor(747.6213, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.887934 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.893863 [1] L1 tensor(127237.4219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.896334 [1] L2 tensor(750.5713, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.899069 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.905142 [1] L1 tensor(127267.8750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.907959 [1] L2 tensor(753.6005, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.910518 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.916730 [1] L1 tensor(127297.5156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.919323 [1] L2 tensor(756.7241, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.921634 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.927702 [1] L1 tensor(127326.8750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.930349 [1] L2 tensor(759.9565, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.932649 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.938706 [1] L1 tensor(127356.7500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.941230 [1] L2 tensor(763.3117, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.943627 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.950313 [1] L1 tensor(127388.3281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.956217 [1] L2 tensor(766.8024, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.959745 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.967790 [1] L1 tensor(127423.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.970361 [1] L2 tensor(770.4375, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.972943 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.979084 [1] L1 tensor(127462.8516, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.981891 [1] L2 tensor(774.2180, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.984508 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.992173 [1] L1 tensor(127508.5781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.994973 [1] L2 tensor(778.1348, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:11.997298 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:12.004850 [1] L1 tensor(127560.2109, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:12.007576 [1] L2 tensor(782.1676, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:12.010542 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:12.018575 [1] L1 tensor(127616.3828, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
15:53:12.021071 [1] L2 tensor(786.2880, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
15:53:12.023391 [1] Warning: no training nodes in this partition! Backward fake loss.
15:53:58.513310 [1] proc begin: <DistEnv 1/4 nccl>
15:54:16.439731 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
15:54:16.448550 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:54:17.408057 [1] L1 tensor(6418.3506, device='cuda:1', grad_fn=<SumBackward0>) tensor(131.0658, device='cuda:1', grad_fn=<SumBackward0>)
16:28:41.074957 [1] proc begin: <DistEnv 1/4 nccl>
16:28:53.202021 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:28:53.271491 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:28:55.899812 [1] L1 tensor(38548.0586, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.5969, device='cuda:1', grad_fn=<SumBackward0>)
17:02:50.198028 [1] proc begin: <DistEnv 1/4 nccl>
17:03:01.307452 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
17:03:01.328427 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:03:02.773127 [1] L1 tensor(38548.0586, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.5969, device='cuda:1', grad_fn=<SumBackward0>)
17:04:43.478900 [1] proc begin: <DistEnv 1/4 nccl>
17:04:49.251269 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
17:04:49.273091 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:04:50.043327 [1] L1 tensor(38548.0586, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.5969, device='cuda:1', grad_fn=<SumBackward0>)
17:05:11.646891 [1] proc begin: <DistEnv 1/4 nccl>
17:05:17.850669 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
17:05:17.868164 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:05:18.593778 [1] L1 tensor(38548.0586, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.5969, device='cuda:1', grad_fn=<SumBackward0>)
17:19:05.527424 [1] proc begin: <DistEnv 1/4 nccl>
17:19:05.734847 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
17:19:05.748947 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:19:06.804135 [1] L1 tensor(91906.2109, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:07.700816 [1] L2 tensor(438.5738, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:07.743753 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.643170 [1] L1 tensor(91832.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.646673 [1] L2 tensor(442.4138, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.649336 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.660786 [1] L1 tensor(91855.8125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.664638 [1] L2 tensor(445.8104, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.668276 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.677783 [1] L1 tensor(91963.5078, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.680479 [1] L2 tensor(448.3092, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.683237 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.694937 [1] L1 tensor(92143.7031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.701562 [1] L2 tensor(450.3099, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.704663 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.715403 [1] L1 tensor(92384.4141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.720667 [1] L2 tensor(452.1290, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.725431 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.735287 [1] L1 tensor(92675.2109, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.739429 [1] L2 tensor(454.0485, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.743263 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.752619 [1] L1 tensor(93008.1797, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.755808 [1] L2 tensor(456.0366, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.759057 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.767794 [1] L1 tensor(93376.7344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.770957 [1] L2 tensor(457.9691, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.773936 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.783472 [1] L1 tensor(93775.1562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.786138 [1] L2 tensor(459.7110, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.788595 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.795094 [1] L1 tensor(94198.1719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.798038 [1] L2 tensor(461.1994, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.800905 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.807294 [1] L1 tensor(94641.2031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.810135 [1] L2 tensor(462.4901, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.812643 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.819081 [1] L1 tensor(95100.4922, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.821953 [1] L2 tensor(463.6807, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.824496 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.833554 [1] L1 tensor(95573.6875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.840224 [1] L2 tensor(464.8087, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.843624 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.850977 [1] L1 tensor(96059.2188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.854005 [1] L2 tensor(465.8305, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.856612 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.863330 [1] L1 tensor(96555.1094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.866128 [1] L2 tensor(466.6918, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.868681 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.874873 [1] L1 tensor(97059.3906, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.877657 [1] L2 tensor(467.4421, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.880193 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.887363 [1] L1 tensor(97571.1094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.890299 [1] L2 tensor(468.2280, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.892955 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.899844 [1] L1 tensor(98089.2266, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.902452 [1] L2 tensor(469.1947, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.904910 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.912044 [1] L1 tensor(98612.5469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.914790 [1] L2 tensor(470.4096, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.918402 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.927214 [1] L1 tensor(99140.3047, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.933419 [1] L2 tensor(471.9096, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.935826 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.946631 [1] L1 tensor(99671.9766, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.952529 [1] L2 tensor(473.7235, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.955658 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.965912 [1] L1 tensor(100206.1719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.969035 [1] L2 tensor(475.8296, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.971943 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.980381 [1] L1 tensor(100741.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.983607 [1] L2 tensor(478.1783, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.986891 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.995916 [1] L1 tensor(101279.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:08.998614 [1] L2 tensor(480.7126, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.001548 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.008762 [1] L1 tensor(101816.7031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.012065 [1] L2 tensor(483.3637, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.015123 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.027302 [1] L1 tensor(102353.3594, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.033246 [1] L2 tensor(486.0808, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.035742 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.046231 [1] L1 tensor(102887.5781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.052946 [1] L2 tensor(488.8194, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.055619 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.066157 [1] L1 tensor(103417.3359, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.071601 [1] L2 tensor(491.5267, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.076968 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.086576 [1] L1 tensor(103939.6328, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.089306 [1] L2 tensor(494.1456, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.091781 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.102146 [1] L1 tensor(104451.4453, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.105486 [1] L2 tensor(496.6192, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.108496 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.119845 [1] L1 tensor(104950.7656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.125870 [1] L2 tensor(498.9021, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.129354 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.138063 [1] L1 tensor(105436.0156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.140895 [1] L2 tensor(500.9772, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.143671 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.153316 [1] L1 tensor(105906.2344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.156377 [1] L2 tensor(502.8627, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.159201 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.165870 [1] L1 tensor(106361.8750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.168582 [1] L2 tensor(504.6043, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.171084 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.180178 [1] L1 tensor(106804.1875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.182850 [1] L2 tensor(506.2584, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.185442 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.192295 [1] L1 tensor(107234.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.195110 [1] L2 tensor(507.8830, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.198034 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.205422 [1] L1 tensor(107654.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.208009 [1] L2 tensor(509.5380, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.210732 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.217858 [1] L1 tensor(108064.9219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.221072 [1] L2 tensor(511.2837, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.223985 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.234437 [1] L1 tensor(108465.7500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.237728 [1] L2 tensor(513.1116, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.240921 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.252151 [1] L1 tensor(108857.4062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.256969 [1] L2 tensor(514.9783, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.261393 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.267405 [1] L1 tensor(109240.2344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.270079 [1] L2 tensor(516.8829, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.272583 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.279409 [1] L1 tensor(109615.5156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.282206 [1] L2 tensor(518.7773, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.284981 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.291044 [1] L1 tensor(109982.0469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.293534 [1] L2 tensor(520.5670, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.295989 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.304053 [1] L1 tensor(110338.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.306843 [1] L2 tensor(522.2886, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.309718 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.316643 [1] L1 tensor(110692.6250, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.319306 [1] L2 tensor(524.0200, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.321834 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.329352 [1] L1 tensor(111051.7969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.333226 [1] L2 tensor(525.9098, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.336338 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.343511 [1] L1 tensor(111416.9688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.346331 [1] L2 tensor(528.1423, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.349634 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.359880 [1] L1 tensor(111785.8281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.362816 [1] L2 tensor(530.8048, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.365806 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.375794 [1] L1 tensor(112153.7266, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.380598 [1] L2 tensor(533.7192, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.384586 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.395562 [1] L1 tensor(112519.3594, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.399754 [1] L2 tensor(536.6816, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.403583 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.411173 [1] L1 tensor(112882.2344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.414484 [1] L2 tensor(539.6133, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.419232 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.429746 [1] L1 tensor(113244.9219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.433838 [1] L2 tensor(542.6822, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.437815 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.447771 [1] L1 tensor(113612.1719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.452125 [1] L2 tensor(546.6586, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.456272 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.467588 [1] L1 tensor(113989.5312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.473672 [1] L2 tensor(551.9941, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.478033 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.488419 [1] L1 tensor(114379.9844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.492482 [1] L2 tensor(558.6183, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.496304 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.502622 [1] L1 tensor(114780.3672, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.505415 [1] L2 tensor(565.5444, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.508123 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.514707 [1] L1 tensor(115178.8047, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.517695 [1] L2 tensor(571.8928, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.520368 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.527552 [1] L1 tensor(115564.4531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.530584 [1] L2 tensor(577.6707, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.533019 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.539457 [1] L1 tensor(115930.2031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.542384 [1] L2 tensor(582.9760, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.545202 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.551166 [1] L1 tensor(116275.2656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.553858 [1] L2 tensor(587.8910, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.556349 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.565348 [1] L1 tensor(116603.7188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.567888 [1] L2 tensor(592.4698, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.570373 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.578602 [1] L1 tensor(116924.3672, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.581501 [1] L2 tensor(596.7665, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.584328 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.594450 [1] L1 tensor(117242.7812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.597109 [1] L2 tensor(600.9441, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.599759 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.606261 [1] L1 tensor(117559.5391, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.609031 [1] L2 tensor(605.2783, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.611517 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.620081 [1] L1 tensor(117869.0703, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.623156 [1] L2 tensor(609.3643, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.626064 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.632806 [1] L1 tensor(118170.2266, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.635624 [1] L2 tensor(613.2253, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.638142 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.646828 [1] L1 tensor(118465.9219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.650615 [1] L2 tensor(616.9367, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.653709 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.663485 [1] L1 tensor(118757.4531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.666657 [1] L2 tensor(620.6086, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.669905 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.677499 [1] L1 tensor(119055.7500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.680016 [1] L2 tensor(624.6782, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.682685 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.693207 [1] L1 tensor(119354.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.696585 [1] L2 tensor(629.0445, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.700045 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.709533 [1] L1 tensor(119652.2812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.713520 [1] L2 tensor(633.7127, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.717221 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.728847 [1] L1 tensor(119952.5938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.733736 [1] L2 tensor(638.6491, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.737663 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.748518 [1] L1 tensor(120247.7344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.752614 [1] L2 tensor(643.5670, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.756791 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.762962 [1] L1 tensor(120529.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.765988 [1] L2 tensor(648.2248, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.768722 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.774991 [1] L1 tensor(120795.8281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.777740 [1] L2 tensor(652.5451, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.780291 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.788432 [1] L1 tensor(121048.8281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.791553 [1] L2 tensor(656.5410, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.794542 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.801175 [1] L1 tensor(121293.3203, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.803623 [1] L2 tensor(660.2567, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.806029 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.812016 [1] L1 tensor(121529.7969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.814736 [1] L2 tensor(663.6885, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.817174 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.824597 [1] L1 tensor(121756.2656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.827591 [1] L2 tensor(666.8307, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.830558 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.845973 [1] L1 tensor(121970.7266, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.852531 [1] L2 tensor(669.6866, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.855565 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.864623 [1] L1 tensor(122173.0469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.867627 [1] L2 tensor(672.2680, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.870651 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.878774 [1] L1 tensor(122363.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.881427 [1] L2 tensor(674.5891, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.884579 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.892183 [1] L1 tensor(122542.6094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.894922 [1] L2 tensor(676.6639, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.897484 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.904212 [1] L1 tensor(122710.4844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.906681 [1] L2 tensor(678.5065, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.909112 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.915552 [1] L1 tensor(122867.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.921069 [1] L2 tensor(680.1316, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.923963 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.931449 [1] L1 tensor(123014.7969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.934234 [1] L2 tensor(681.5560, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.936661 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.946417 [1] L1 tensor(123152.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.949492 [1] L2 tensor(682.7974, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.952841 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.960541 [1] L1 tensor(123282.0078, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.963325 [1] L2 tensor(683.8741, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.966319 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.973123 [1] L1 tensor(123403.5469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.975704 [1] L2 tensor(684.8029, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.978189 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.984615 [1] L1 tensor(123517.7812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.987454 [1] L2 tensor(685.5984, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:09.990083 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.997597 [1] L1 tensor(123625.0156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.000386 [1] L2 tensor(686.2748, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.003115 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.010935 [1] L1 tensor(123725.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.013689 [1] L2 tensor(686.8505, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.016470 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.023163 [1] L1 tensor(123819.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.025841 [1] L2 tensor(687.3707, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.028410 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.037310 [1] L1 tensor(123909.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.040040 [1] L2 tensor(687.9740, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.042487 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.051068 [1] L1 tensor(123996.5156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.054094 [1] L2 tensor(688.8097, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.057431 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.063883 [1] L1 tensor(124079.4844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.066540 [1] L2 tensor(689.6624, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.068985 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.075201 [1] L1 tensor(124158.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.077903 [1] L2 tensor(690.4357, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.080494 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.086899 [1] L1 tensor(124233.0156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.089725 [1] L2 tensor(691.1203, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.092228 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.100694 [1] L1 tensor(124304.3203, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.103163 [1] L2 tensor(691.7180, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.105584 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.113365 [1] L1 tensor(124371.8125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.116130 [1] L2 tensor(692.2268, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.118630 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.125168 [1] L1 tensor(124435.3438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.128354 [1] L2 tensor(692.6467, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.131198 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.137669 [1] L1 tensor(124495.0312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.141287 [1] L2 tensor(692.9810, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.143799 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.150263 [1] L1 tensor(124551.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.153210 [1] L2 tensor(693.2344, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.155954 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.162607 [1] L1 tensor(124603.8125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.165660 [1] L2 tensor(693.4122, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.168343 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.179295 [1] L1 tensor(124653.4844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.185787 [1] L2 tensor(693.5199, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.190460 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.198862 [1] L1 tensor(124700.3906, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.201763 [1] L2 tensor(693.5624, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.204563 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.210591 [1] L1 tensor(124744.7734, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.213318 [1] L2 tensor(693.5448, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.215843 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.222951 [1] L1 tensor(124786.9219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.225750 [1] L2 tensor(693.4716, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.228351 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.236312 [1] L1 tensor(124827.0469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.239160 [1] L2 tensor(693.3471, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.242197 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.250304 [1] L1 tensor(124865.3672, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.253417 [1] L2 tensor(693.1757, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.256603 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.263187 [1] L1 tensor(124902.0469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.266082 [1] L2 tensor(692.9608, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.268614 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.275001 [1] L1 tensor(124937.2500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.277864 [1] L2 tensor(692.7062, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.280384 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.290158 [1] L1 tensor(124971.0859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.295000 [1] L2 tensor(692.4150, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.299859 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.306295 [1] L1 tensor(125003.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.310034 [1] L2 tensor(692.0902, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.313224 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.320375 [1] L1 tensor(125035.0156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.322984 [1] L2 tensor(691.7349, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.325579 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.331707 [1] L1 tensor(125065.2344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.334399 [1] L2 tensor(691.3517, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.336890 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.342961 [1] L1 tensor(125094.3984, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.345808 [1] L2 tensor(690.9435, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.348545 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.355313 [1] L1 tensor(125122.5312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.358227 [1] L2 tensor(690.5129, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.360937 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.367140 [1] L1 tensor(125149.7188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.369777 [1] L2 tensor(690.0624, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.372215 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.378499 [1] L1 tensor(125176., device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.381221 [1] L2 tensor(689.5944, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.383658 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.390203 [1] L1 tensor(125201.4375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.393223 [1] L2 tensor(689.1108, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.395693 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.402570 [1] L1 tensor(125226.0703, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.405254 [1] L2 tensor(688.6139, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.407952 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.414081 [1] L1 tensor(125249.9609, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.416778 [1] L2 tensor(688.1051, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.419501 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.425632 [1] L1 tensor(125273.1797, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.431184 [1] L2 tensor(687.5858, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.434219 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.442259 [1] L1 tensor(125295.7734, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.444931 [1] L2 tensor(687.0573, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.447454 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.453769 [1] L1 tensor(125317.8125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.456644 [1] L2 tensor(686.5200, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.459299 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.465779 [1] L1 tensor(125339.3125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.468604 [1] L2 tensor(685.9749, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.471119 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.477854 [1] L1 tensor(125360.3594, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.480736 [1] L2 tensor(685.4220, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.483436 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.489981 [1] L1 tensor(125380.9922, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.493032 [1] L2 tensor(684.8618, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.495726 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.502403 [1] L1 tensor(125401.2656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.505910 [1] L2 tensor(684.2943, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.508434 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.515351 [1] L1 tensor(125421.2266, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.518018 [1] L2 tensor(683.7194, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.520671 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.527270 [1] L1 tensor(125440.9375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.529899 [1] L2 tensor(683.1373, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.532440 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.538443 [1] L1 tensor(125460.4453, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.541207 [1] L2 tensor(682.5483, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.543688 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.549758 [1] L1 tensor(125479.8125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.552419 [1] L2 tensor(681.9523, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.554985 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.561111 [1] L1 tensor(125499.1250, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.563797 [1] L2 tensor(681.3497, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.566331 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.572333 [1] L1 tensor(125518.4141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.575023 [1] L2 tensor(680.7411, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.577601 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.585635 [1] L1 tensor(125537.8125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.588439 [1] L2 tensor(680.1269, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.591114 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.598298 [1] L1 tensor(125557.4219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.601540 [1] L2 tensor(679.5081, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.604539 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.611527 [1] L1 tensor(125577.3594, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.614540 [1] L2 tensor(678.8856, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.617375 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.628099 [1] L1 tensor(125597.7969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.633092 [1] L2 tensor(678.2609, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.637127 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.648003 [1] L1 tensor(125618.9141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.652597 [1] L2 tensor(677.6357, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.656521 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.665449 [1] L1 tensor(125640.9062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.668790 [1] L2 tensor(677.0120, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.671910 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.678420 [1] L1 tensor(125663.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.681170 [1] L2 tensor(676.3918, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.683606 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.689712 [1] L1 tensor(125688.1328, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.692511 [1] L2 tensor(675.7770, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.695075 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.702120 [1] L1 tensor(125713.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.704931 [1] L2 tensor(675.1686, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.707741 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.715139 [1] L1 tensor(125739.9688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.717845 [1] L2 tensor(674.5680, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.720464 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.726821 [1] L1 tensor(125767.4688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.729481 [1] L2 tensor(673.9763, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.732115 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.739513 [1] L1 tensor(125795.9062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.742441 [1] L2 tensor(673.3954, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.744922 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.755993 [1] L1 tensor(125825.2422, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.760104 [1] L2 tensor(672.8276, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.763015 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.773512 [1] L1 tensor(125855.3359, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.776987 [1] L2 tensor(672.2759, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.780357 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.791138 [1] L1 tensor(125886.1562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.795924 [1] L2 tensor(671.7439, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.800385 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.807475 [1] L1 tensor(125917.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.810283 [1] L2 tensor(671.2361, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.812833 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.820225 [1] L1 tensor(125949.4531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.826163 [1] L2 tensor(670.7574, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.830767 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.837983 [1] L1 tensor(125981.6172, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.840573 [1] L2 tensor(670.3148, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.843000 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.852357 [1] L1 tensor(126013.8516, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.855821 [1] L2 tensor(669.9174, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.859017 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.866137 [1] L1 tensor(126045.9062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.868861 [1] L2 tensor(669.5789, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.871274 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.877938 [1] L1 tensor(126077.5938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.880916 [1] L2 tensor(669.3206, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.883637 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.890305 [1] L1 tensor(126108.7578, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.893226 [1] L2 tensor(669.1735, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.895726 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.902029 [1] L1 tensor(126139.2891, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.904619 [1] L2 tensor(669.1777, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.907027 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.914900 [1] L1 tensor(126169.1562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.920121 [1] L2 tensor(669.3781, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.923155 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.931685 [1] L1 tensor(126198.3594, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.934340 [1] L2 tensor(669.8165, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.936800 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.947717 [1] L1 tensor(126226.9219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.954290 [1] L2 tensor(670.5261, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.958653 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.967296 [1] L1 tensor(126254.9219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.969823 [1] L2 tensor(671.5295, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.972276 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.978436 [1] L1 tensor(126282.4766, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.981202 [1] L2 tensor(672.8403, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.983652 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.992008 [1] L1 tensor(126309.7812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.995055 [1] L2 tensor(674.4662, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:10.998029 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.006914 [1] L1 tensor(126337.0859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.010843 [1] L2 tensor(676.4105, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.014394 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.023237 [1] L1 tensor(126364.7969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.027559 [1] L2 tensor(678.6738, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.031063 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.040474 [1] L1 tensor(126393.4062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.043703 [1] L2 tensor(681.2538, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.046991 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.058975 [1] L1 tensor(126423.5938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.064851 [1] L2 tensor(684.1448, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.069476 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.079462 [1] L1 tensor(126456.0078, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.083733 [1] L2 tensor(687.3356, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.087286 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.097456 [1] L1 tensor(126491.1484, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.100904 [1] L2 tensor(690.8032, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.103971 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.115954 [1] L1 tensor(126529.1094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.121616 [1] L2 tensor(694.5038, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.125957 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.134792 [1] L1 tensor(126569.5469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.137554 [1] L2 tensor(698.3648, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.140017 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.146241 [1] L1 tensor(126611.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.149000 [1] L2 tensor(702.2902, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.151648 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.159623 [1] L1 tensor(126655.8438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.162420 [1] L2 tensor(706.1835, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.165236 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.171806 [1] L1 tensor(126700.8984, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.174575 [1] L2 tensor(709.9762, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.177069 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.184365 [1] L1 tensor(126747.0156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.187140 [1] L2 tensor(713.6380, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.189645 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.197024 [1] L1 tensor(126794.1250, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.200179 [1] L2 tensor(717.1677, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.203510 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.209612 [1] L1 tensor(126841.7734, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.212998 [1] L2 tensor(720.5723, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.215794 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.222890 [1] L1 tensor(126889.0859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.227540 [1] L2 tensor(723.8508, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.230290 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.240387 [1] L1 tensor(126935.1562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.244446 [1] L2 tensor(727.0045, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.247755 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.259039 [1] L1 tensor(126979.4844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.264991 [1] L2 tensor(730.0438, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.269790 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.279470 [1] L1 tensor(127021.8672, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.283832 [1] L2 tensor(732.9879, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.290816 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.299195 [1] L1 tensor(127062.2500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.302723 [1] L2 tensor(735.8609, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.306227 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.314687 [1] L1 tensor(127100.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.317576 [1] L2 tensor(738.6917, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.320627 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.330495 [1] L1 tensor(127137.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.333886 [1] L2 tensor(741.5101, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.337241 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.346778 [1] L1 tensor(127171.8516, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.351558 [1] L2 tensor(744.3444, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.355085 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.364146 [1] L1 tensor(127204.9219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.367585 [1] L2 tensor(747.2191, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.370673 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.379795 [1] L1 tensor(127236.5391, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.382624 [1] L2 tensor(750.1547, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.385894 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.392488 [1] L1 tensor(127266.9688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.395212 [1] L2 tensor(753.1685, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.398037 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.404647 [1] L1 tensor(127296.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.407262 [1] L2 tensor(756.2761, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.409721 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.415690 [1] L1 tensor(127325.8203, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.418434 [1] L2 tensor(759.4919, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.421094 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.428801 [1] L1 tensor(127355.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.431554 [1] L2 tensor(762.8299, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.434065 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.441414 [1] L1 tensor(127386.7188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.444036 [1] L2 tensor(766.3027, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.446761 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.457407 [1] L1 tensor(127421.0234, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.461277 [1] L2 tensor(769.9196, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.464503 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.476225 [1] L1 tensor(127460.0312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.481414 [1] L2 tensor(773.6823, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.484417 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.494041 [1] L1 tensor(127505.0078, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.500617 [1] L2 tensor(777.5822, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.503561 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.515110 [1] L1 tensor(127555.9609, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.518761 [1] L2 tensor(781.6003, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.521639 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.531341 [1] L1 tensor(127611.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.534696 [1] L2 tensor(785.7085, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:19:11.538115 [1] Warning: no training nodes in this partition! Backward fake loss.
17:23:06.030289 [1] proc begin: <DistEnv 1/4 nccl>
17:23:11.662117 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
17:23:11.682999 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:24:27.180967 [1] proc begin: <DistEnv 1/4 nccl>
17:24:32.238183 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
17:24:32.261051 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:25:02.859528 [1] proc begin: <DistEnv 1/4 nccl>
17:25:02.991102 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
17:25:03.015390 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:25:04.136491 [1] L1 tensor(91906.2109, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:04.959152 [1] L2 tensor(438.5738, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:04.993920 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.796901 [1] L1 tensor(91832.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.801198 [1] L2 tensor(442.4138, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.804187 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.810220 [1] L1 tensor(91855.8203, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.813020 [1] L2 tensor(445.8104, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.815739 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.822175 [1] L1 tensor(91963.5156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.825002 [1] L2 tensor(448.3091, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.827796 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.834025 [1] L1 tensor(92143.7031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.836608 [1] L2 tensor(450.3099, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.839054 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.845070 [1] L1 tensor(92384.4141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.847770 [1] L2 tensor(452.1290, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.850289 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.856799 [1] L1 tensor(92675.2188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.859904 [1] L2 tensor(454.0485, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.862719 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.869135 [1] L1 tensor(93008.1797, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.871865 [1] L2 tensor(456.0366, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.874389 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.881021 [1] L1 tensor(93376.7500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.883670 [1] L2 tensor(457.9691, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.886239 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.892736 [1] L1 tensor(93775.1719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.895845 [1] L2 tensor(459.7110, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.898823 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.905463 [1] L1 tensor(94198.1875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.908462 [1] L2 tensor(461.1994, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.911299 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.917770 [1] L1 tensor(94641.2031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.920316 [1] L2 tensor(462.4901, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.922900 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.929155 [1] L1 tensor(95100.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.932092 [1] L2 tensor(463.6808, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.934845 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.941264 [1] L1 tensor(95573.6953, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.943919 [1] L2 tensor(464.8087, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.946368 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.953307 [1] L1 tensor(96059.2422, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.956130 [1] L2 tensor(465.8306, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.958651 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.965392 [1] L1 tensor(96555.1172, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.968281 [1] L2 tensor(466.6918, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.971317 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.981277 [1] L1 tensor(97059.4141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.985939 [1] L2 tensor(467.4421, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:05.993385 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.005877 [1] L1 tensor(97571.1250, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.010287 [1] L2 tensor(468.2280, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.014447 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.029732 [1] L1 tensor(98089.2500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.036752 [1] L2 tensor(469.1948, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.039625 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.050376 [1] L1 tensor(98612.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.055144 [1] L2 tensor(470.4097, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.058536 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.068361 [1] L1 tensor(99140.3281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.072868 [1] L2 tensor(471.9097, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.077971 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.088028 [1] L1 tensor(99672., device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.091798 [1] L2 tensor(473.7236, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.095737 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.102232 [1] L1 tensor(100206.1953, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.109346 [1] L2 tensor(475.8297, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.112037 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.122612 [1] L1 tensor(100741.9844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.126057 [1] L2 tensor(478.1783, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.129405 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.139186 [1] L1 tensor(101279.1094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.144065 [1] L2 tensor(480.7127, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.148050 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.157451 [1] L1 tensor(101816.7188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.162369 [1] L2 tensor(483.3639, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.167724 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.174942 [1] L1 tensor(102353.3906, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.177806 [1] L2 tensor(486.0810, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.180632 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.190099 [1] L1 tensor(102887.6094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.192986 [1] L2 tensor(488.8195, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.195418 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.202969 [1] L1 tensor(103417.3672, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.205567 [1] L2 tensor(491.5269, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.208091 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.214581 [1] L1 tensor(103939.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.217497 [1] L2 tensor(494.1458, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.220337 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.226609 [1] L1 tensor(104451.4766, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.229125 [1] L2 tensor(496.6194, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.231629 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.237817 [1] L1 tensor(104950.7969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.240384 [1] L2 tensor(498.9023, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.242965 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.249254 [1] L1 tensor(105436.0312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.252044 [1] L2 tensor(500.9773, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.254738 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.261526 [1] L1 tensor(105906.2656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.264274 [1] L2 tensor(502.8629, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.267132 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.273842 [1] L1 tensor(106361.8906, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.277080 [1] L2 tensor(504.6045, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.280183 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.287092 [1] L1 tensor(106804.2031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.289897 [1] L2 tensor(506.2585, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.292720 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.299917 [1] L1 tensor(107234.6641, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.302966 [1] L2 tensor(507.8830, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.305955 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.315052 [1] L1 tensor(107654.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.318728 [1] L2 tensor(509.5378, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.322484 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.330176 [1] L1 tensor(108064.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.333312 [1] L2 tensor(511.2835, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.336465 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.343981 [1] L1 tensor(108465.7891, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.347498 [1] L2 tensor(513.1113, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.351276 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.359212 [1] L1 tensor(108857.4609, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.362337 [1] L2 tensor(514.9783, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.365527 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.377026 [1] L1 tensor(109240.2969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.381995 [1] L2 tensor(516.8831, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.385725 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.396814 [1] L1 tensor(109615.6094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.401110 [1] L2 tensor(518.7777, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.404967 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.411380 [1] L1 tensor(109982.1562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.413952 [1] L2 tensor(520.5676, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.416770 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.423149 [1] L1 tensor(110338.7969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.425697 [1] L2 tensor(522.2892, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.428257 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.436013 [1] L1 tensor(110692.8125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.438667 [1] L2 tensor(524.0209, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.441185 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.447468 [1] L1 tensor(111052.0469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.450336 [1] L2 tensor(525.9114, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.452912 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.459342 [1] L1 tensor(111417.2578, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.462075 [1] L2 tensor(528.1450, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.465046 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.471594 [1] L1 tensor(111786.1562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.474296 [1] L2 tensor(530.8081, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.476976 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.483499 [1] L1 tensor(112154.0781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.486249 [1] L2 tensor(533.7229, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.488992 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.495693 [1] L1 tensor(112519.7344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.498798 [1] L2 tensor(536.6855, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.501788 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.512046 [1] L1 tensor(112882.6250, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.515837 [1] L2 tensor(539.6174, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.519682 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.526086 [1] L1 tensor(113245.3359, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.529125 [1] L2 tensor(542.6865, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.531877 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.543482 [1] L1 tensor(113612.6172, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.549968 [1] L2 tensor(546.6628, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.554627 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.564267 [1] L1 tensor(113990., device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.568203 [1] L2 tensor(551.9982, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.571427 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.578532 [1] L1 tensor(114380.4844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.581425 [1] L2 tensor(558.6219, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.584357 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.590462 [1] L1 tensor(114780.8594, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.593134 [1] L2 tensor(565.5477, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.595911 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.602313 [1] L1 tensor(115179.2969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.605219 [1] L2 tensor(571.8963, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.607735 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.615320 [1] L1 tensor(115564.9297, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.621871 [1] L2 tensor(577.6746, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.626596 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.637259 [1] L1 tensor(115930.6719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.640669 [1] L2 tensor(582.9805, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.644159 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.655093 [1] L1 tensor(116275.7188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.661932 [1] L2 tensor(587.8960, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.665346 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.672799 [1] L1 tensor(116604.1719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.675849 [1] L2 tensor(592.4757, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.678771 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.685179 [1] L1 tensor(116924.7891, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.687922 [1] L2 tensor(596.7731, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.690353 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.698830 [1] L1 tensor(117243.1719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.702467 [1] L2 tensor(600.9498, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.705681 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.715584 [1] L1 tensor(117559.8750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.718527 [1] L2 tensor(605.2867, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.721531 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.727717 [1] L1 tensor(117869.3281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.730430 [1] L2 tensor(609.3760, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.733251 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.739211 [1] L1 tensor(118170.3984, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.741717 [1] L2 tensor(613.2397, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.744151 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.751988 [1] L1 tensor(118465.9297, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.754814 [1] L2 tensor(616.9527, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.757322 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.765890 [1] L1 tensor(118757.2500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.768433 [1] L2 tensor(620.6242, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.771170 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.781055 [1] L1 tensor(119055.2891, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.786802 [1] L2 tensor(624.6888, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.791038 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.797487 [1] L1 tensor(119353.3750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.800260 [1] L2 tensor(629.0469, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.802846 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.809435 [1] L1 tensor(119651.1641, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.812187 [1] L2 tensor(633.7066, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.814636 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.822613 [1] L1 tensor(119951.1328, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.825145 [1] L2 tensor(638.6360, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.828198 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.834711 [1] L1 tensor(120246., device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.837438 [1] L2 tensor(643.5498, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.840071 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.849909 [1] L1 tensor(120527.7109, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.852522 [1] L2 tensor(648.2047, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.855167 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.861574 [1] L1 tensor(120793.6719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.864145 [1] L2 tensor(652.5225, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.866788 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.873025 [1] L1 tensor(121046.4219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.875761 [1] L2 tensor(656.5157, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.878522 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.884946 [1] L1 tensor(121290.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.887481 [1] L2 tensor(660.2286, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.890069 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.896325 [1] L1 tensor(121526.9141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.899036 [1] L2 tensor(663.6585, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.901601 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.908934 [1] L1 tensor(121753.2031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.912292 [1] L2 tensor(666.7991, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.915102 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.922028 [1] L1 tensor(121967.5156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.924776 [1] L2 tensor(669.6534, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.927164 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.933443 [1] L1 tensor(122169.6953, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.936057 [1] L2 tensor(672.2335, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.938712 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.944928 [1] L1 tensor(122360.0859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.949467 [1] L2 tensor(674.5535, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.954315 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.964203 [1] L1 tensor(122539., device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.968709 [1] L2 tensor(676.6274, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.972098 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.983672 [1] L1 tensor(122706.7656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.989175 [1] L2 tensor(678.4692, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:06.993853 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.001681 [1] L1 tensor(122863.8516, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.005230 [1] L2 tensor(680.0939, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.008615 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.018796 [1] L1 tensor(123010.9062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.021592 [1] L2 tensor(681.5181, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.024236 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.034424 [1] L1 tensor(123148.7109, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.039227 [1] L2 tensor(682.7595, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.044060 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.049917 [1] L1 tensor(123278.0312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.052866 [1] L2 tensor(683.8363, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.055441 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.065716 [1] L1 tensor(123399.5547, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.068710 [1] L2 tensor(684.7654, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.071634 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.077389 [1] L1 tensor(123513.7656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.080110 [1] L2 tensor(685.5615, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.082607 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.090264 [1] L1 tensor(123621.0312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.093884 [1] L2 tensor(686.2384, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.096963 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.102907 [1] L1 tensor(123721.6172, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.105543 [1] L2 tensor(686.8153, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.108105 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.114877 [1] L1 tensor(123816.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.118103 [1] L2 tensor(687.3386, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.120867 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.130679 [1] L1 tensor(123905.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.133212 [1] L2 tensor(687.9507, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.135744 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.143275 [1] L1 tensor(123993.0156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.146488 [1] L2 tensor(688.7937, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.149066 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.155426 [1] L1 tensor(124076.1719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.158079 [1] L2 tensor(689.6476, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.160476 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.167369 [1] L1 tensor(124155.0391, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.170862 [1] L2 tensor(690.4208, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.174425 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.181518 [1] L1 tensor(124230.1250, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.184877 [1] L2 tensor(691.1049, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.188529 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.195642 [1] L1 tensor(124301.6641, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.199113 [1] L2 tensor(691.7021, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.202609 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.209757 [1] L1 tensor(124369.3672, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.213174 [1] L2 tensor(692.2105, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.216785 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.226321 [1] L1 tensor(124433.0703, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.232784 [1] L2 tensor(692.6302, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.236317 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.246221 [1] L1 tensor(124492.8672, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.251674 [1] L2 tensor(692.9646, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.255301 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.265693 [1] L1 tensor(124548.9453, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.270918 [1] L2 tensor(693.2184, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.274631 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.284113 [1] L1 tensor(124601.6328, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.288160 [1] L2 tensor(693.3970, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.291446 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.298077 [1] L1 tensor(124651.2188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.300479 [1] L2 tensor(693.5057, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.302921 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.310052 [1] L1 tensor(124698.0312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.315058 [1] L2 tensor(693.5497, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.318935 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.325567 [1] L1 tensor(124742.3281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.328174 [1] L2 tensor(693.5336, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.330613 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.337324 [1] L1 tensor(124784.3750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.340506 [1] L2 tensor(693.4623, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.343156 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.349968 [1] L1 tensor(124824.4062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.352740 [1] L2 tensor(693.3398, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.355550 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.361596 [1] L1 tensor(124862.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.364099 [1] L2 tensor(693.1705, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.366574 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.373286 [1] L1 tensor(124899.2656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.376040 [1] L2 tensor(692.9579, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.378477 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.388534 [1] L1 tensor(124934.4062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.392357 [1] L2 tensor(692.7056, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.395762 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.402456 [1] L1 tensor(124968.2031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.405538 [1] L2 tensor(692.4167, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.408242 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.417628 [1] L1 tensor(125000.7188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.422299 [1] L2 tensor(692.0943, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.426659 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.437297 [1] L1 tensor(125032.0547, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.441202 [1] L2 tensor(691.7413, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.444588 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.456032 [1] L1 tensor(125062.2656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.462004 [1] L2 tensor(691.3604, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.466199 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.476577 [1] L1 tensor(125091.4062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.480461 [1] L2 tensor(690.9545, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.483642 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.493678 [1] L1 tensor(125119.5312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.498175 [1] L2 tensor(690.5261, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.502414 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.512457 [1] L1 tensor(125146.7031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.517158 [1] L2 tensor(690.0777, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.520740 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.532409 [1] L1 tensor(125172.9844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.537313 [1] L2 tensor(689.6117, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.541255 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.551836 [1] L1 tensor(125198.4141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.556383 [1] L2 tensor(689.1302, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.560099 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.569860 [1] L1 tensor(125223.0625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.574805 [1] L2 tensor(688.6351, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.578513 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.588459 [1] L1 tensor(125246.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.591813 [1] L2 tensor(688.1282, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.595230 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.604170 [1] L1 tensor(125270.1875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.606990 [1] L2 tensor(687.6105, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.609705 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.618960 [1] L1 tensor(125292.8047, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.623371 [1] L2 tensor(687.0834, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.627456 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.634822 [1] L1 tensor(125314.8438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.637714 [1] L2 tensor(686.5475, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.640655 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.650877 [1] L1 tensor(125336.3828, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.653906 [1] L2 tensor(686.0035, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.656962 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.663164 [1] L1 tensor(125357.4531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.665632 [1] L2 tensor(685.4519, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.668106 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.674794 [1] L1 tensor(125378.1328, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.677640 [1] L2 tensor(684.8926, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.680112 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.688714 [1] L1 tensor(125398.4531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.691414 [1] L2 tensor(684.3259, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.693855 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.702748 [1] L1 tensor(125418.4844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.705946 [1] L2 tensor(683.7520, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.709439 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.718270 [1] L1 tensor(125438.2500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.720749 [1] L2 tensor(683.1707, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.723250 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.729341 [1] L1 tensor(125457.8438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.732090 [1] L2 tensor(682.5823, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.734580 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.740652 [1] L1 tensor(125477.3047, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.743728 [1] L2 tensor(681.9871, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.746510 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.756778 [1] L1 tensor(125496.7031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.760286 [1] L2 tensor(681.3853, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.763794 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.770185 [1] L1 tensor(125516.1328, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.772801 [1] L2 tensor(680.7774, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.775229 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.782565 [1] L1 tensor(125535.6875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.785316 [1] L2 tensor(680.1642, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.787852 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.793996 [1] L1 tensor(125555.4766, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.796836 [1] L2 tensor(679.5466, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.799465 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.806102 [1] L1 tensor(125575.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.808955 [1] L2 tensor(678.9255, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.811864 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.818804 [1] L1 tensor(125596.3438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.821553 [1] L2 tensor(678.3028, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.824093 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.830158 [1] L1 tensor(125617.7891, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.832743 [1] L2 tensor(677.6799, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.835208 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.842042 [1] L1 tensor(125640.1641, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.844761 [1] L2 tensor(677.0592, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.847256 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.853645 [1] L1 tensor(125663.6094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.856267 [1] L2 tensor(676.4423, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.858881 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.864634 [1] L1 tensor(125688.2188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.867361 [1] L2 tensor(675.8309, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.869802 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.877285 [1] L1 tensor(125713.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.879970 [1] L2 tensor(675.2263, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.882646 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.890857 [1] L1 tensor(125740.7812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.894208 [1] L2 tensor(674.6296, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.897640 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.904136 [1] L1 tensor(125768.6016, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.907007 [1] L2 tensor(674.0422, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.912211 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.921707 [1] L1 tensor(125797.3359, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.925256 [1] L2 tensor(673.4661, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.929448 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.939570 [1] L1 tensor(125826.9297, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.943107 [1] L2 tensor(672.9038, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.948001 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.958276 [1] L1 tensor(125857.2812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.961623 [1] L2 tensor(672.3586, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.965249 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.974650 [1] L1 tensor(125888.3281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.978655 [1] L2 tensor(671.8345, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.982758 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.991678 [1] L1 tensor(125919.9375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.995036 [1] L2 tensor(671.3359, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:07.998142 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.004863 [1] L1 tensor(125951.9609, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.007621 [1] L2 tensor(670.8690, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.010096 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.018338 [1] L1 tensor(125984.2188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.021484 [1] L2 tensor(670.4412, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.024275 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.030367 [1] L1 tensor(126016.4375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.033128 [1] L2 tensor(670.0634, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.035660 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.041993 [1] L1 tensor(126048.4219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.044498 [1] L2 tensor(669.7524, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.049925 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.057530 [1] L1 tensor(126079.9688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.060810 [1] L2 tensor(669.5336, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.063854 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.073097 [1] L1 tensor(126110.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.077126 [1] L2 tensor(669.4422, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.081736 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.088114 [1] L1 tensor(126141.2656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.090703 [1] L2 tensor(669.5217, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.093291 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.099714 [1] L1 tensor(126170.9219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.102351 [1] L2 tensor(669.8168, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.105190 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.115518 [1] L1 tensor(126199.8828, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.119419 [1] L2 tensor(670.3663, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.122219 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.128552 [1] L1 tensor(126228.2266, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.131387 [1] L2 tensor(671.1985, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.134108 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.141714 [1] L1 tensor(126256.0469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.144868 [1] L2 tensor(672.3314, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.147506 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.154488 [1] L1 tensor(126283.4688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.157327 [1] L2 tensor(673.7753, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.160107 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.169081 [1] L1 tensor(126310.7188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.171873 [1] L2 tensor(675.5351, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.174720 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.181099 [1] L1 tensor(126338.1016, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.183727 [1] L2 tensor(677.6124, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.186420 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.193148 [1] L1 tensor(126366.0625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.196252 [1] L2 tensor(680.0062, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.198942 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.205519 [1] L1 tensor(126395.1562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.208516 [1] L2 tensor(682.7129, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.210936 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.217041 [1] L1 tensor(126426.0781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.219696 [1] L2 tensor(685.7258, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.222174 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.228292 [1] L1 tensor(126459.4375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.231079 [1] L2 tensor(689.0309, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.233833 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.240556 [1] L1 tensor(126495.6016, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.243451 [1] L2 tensor(692.5996, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.246002 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.252314 [1] L1 tensor(126534.4766, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.254888 [1] L2 tensor(696.3790, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.257444 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.264180 [1] L1 tensor(126575.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.267180 [1] L2 tensor(700.2872, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.270154 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.276715 [1] L1 tensor(126618.5781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.279829 [1] L2 tensor(704.2246, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.282507 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.289482 [1] L1 tensor(126662.8594, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.292563 [1] L2 tensor(708.1017, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.295449 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.302624 [1] L1 tensor(126708.2734, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.306130 [1] L2 tensor(711.8633, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.309394 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.315499 [1] L1 tensor(126754.7422, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.321367 [1] L2 tensor(715.4906, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.324399 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.332232 [1] L1 tensor(126802.1094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.335148 [1] L2 tensor(718.9883, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.337931 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.344268 [1] L1 tensor(126849.7656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.347007 [1] L2 tensor(722.3608, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.349489 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.355999 [1] L1 tensor(126896.7266, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.358771 [1] L2 tensor(725.6066, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.361526 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.367756 [1] L1 tensor(126942.2266, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.370407 [1] L2 tensor(728.7300, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.372951 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.379588 [1] L1 tensor(126985.8750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.382469 [1] L2 tensor(731.7451, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.385092 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.391873 [1] L1 tensor(127027.5469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.394659 [1] L2 tensor(734.6733, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.397314 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.403892 [1] L1 tensor(127067.2422, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.406772 [1] L2 tensor(737.5409, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.409840 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.417815 [1] L1 tensor(127104.9688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.421037 [1] L2 tensor(740.3770, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.423993 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.431139 [1] L1 tensor(127140.8438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.434357 [1] L2 tensor(743.2106, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.437615 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.444532 [1] L1 tensor(127174.9922, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.447475 [1] L2 tensor(746.0685, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.449901 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.455880 [1] L1 tensor(127207.5781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.458621 [1] L2 tensor(748.9731, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.461055 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.471687 [1] L1 tensor(127238.8125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.476482 [1] L2 tensor(751.9435, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.480931 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.492986 [1] L1 tensor(127268.9922, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.497466 [1] L2 tensor(754.9952, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.499870 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.507168 [1] L1 tensor(127298.5156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.510166 [1] L2 tensor(758.1432, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.513035 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.519513 [1] L1 tensor(127327.9844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.522287 [1] L2 tensor(761.4014, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.524794 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.530826 [1] L1 tensor(127358.3125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.533634 [1] L2 tensor(764.7833, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.536412 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.543304 [1] L1 tensor(127390.7812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.550176 [1] L2 tensor(768.3010, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.554501 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.564436 [1] L1 tensor(127426.9844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.567764 [1] L2 tensor(771.9609, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.571156 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.577412 [1] L1 tensor(127468.4453, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.579959 [1] L2 tensor(775.7610, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.582472 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.588586 [1] L1 tensor(127515.8906, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.591323 [1] L2 tensor(779.6886, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.593972 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.600244 [1] L1 tensor(127568.8047, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.603092 [1] L2 tensor(783.7209, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.605754 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.612376 [1] L1 tensor(127625.5312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.615201 [1] L2 tensor(787.8282, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
17:25:08.617671 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:40.963330 [1] proc begin: <DistEnv 1/4 nccl>
17:25:41.018934 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
17:25:41.032572 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:31:53.945603 [1] proc begin: <DistEnv 1/4 nccl>
20:31:54.356698 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
20:31:54.377011 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:31:55.533743 [1] L1 tensor(91906.2109, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:56.375233 [1] L2 tensor(438.5738, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:56.418127 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.343407 [1] L1 tensor(91832.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.347349 [1] L2 tensor(442.4138, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.350828 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.359132 [1] L1 tensor(91855.8203, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.363548 [1] L2 tensor(445.8104, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.367095 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.374979 [1] L1 tensor(91963.5078, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.377858 [1] L2 tensor(448.3091, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.380337 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.387155 [1] L1 tensor(92143.7031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.390372 [1] L2 tensor(450.3099, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.393064 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.399920 [1] L1 tensor(92384.4141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.402894 [1] L2 tensor(452.1291, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.405693 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.413733 [1] L1 tensor(92675.2109, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.417198 [1] L2 tensor(454.0486, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.421132 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.432637 [1] L1 tensor(93008.1797, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.437500 [1] L2 tensor(456.0366, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.440538 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.448051 [1] L1 tensor(93376.7500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.450963 [1] L2 tensor(457.9691, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.454306 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.461808 [1] L1 tensor(93775.1719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.464744 [1] L2 tensor(459.7110, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.467208 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.477064 [1] L1 tensor(94198.1797, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.480643 [1] L2 tensor(461.1994, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.484319 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.490485 [1] L1 tensor(94641.2031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.493379 [1] L2 tensor(462.4901, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.495939 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.503040 [1] L1 tensor(95100.4922, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.506183 [1] L2 tensor(463.6808, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.508919 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.515963 [1] L1 tensor(95573.6953, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.519130 [1] L2 tensor(464.8087, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.521886 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.529212 [1] L1 tensor(96059.2422, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.532201 [1] L2 tensor(465.8305, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.535024 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.541135 [1] L1 tensor(96555.1172, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.544202 [1] L2 tensor(466.6918, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.546880 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.554001 [1] L1 tensor(97059.4141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.556854 [1] L2 tensor(467.4421, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.559361 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.565909 [1] L1 tensor(97571.1094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.568878 [1] L2 tensor(468.2280, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.571636 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.578395 [1] L1 tensor(98089.2344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.581367 [1] L2 tensor(469.1948, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.583917 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.590883 [1] L1 tensor(98612.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.595475 [1] L2 tensor(470.4097, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.601652 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.608705 [1] L1 tensor(99140.3125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.611739 [1] L2 tensor(471.9096, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.615149 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.622463 [1] L1 tensor(99672., device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.625291 [1] L2 tensor(473.7236, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.627826 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.634777 [1] L1 tensor(100206.1875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.637795 [1] L2 tensor(475.8296, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.640615 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.647431 [1] L1 tensor(100741.9766, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.651030 [1] L2 tensor(478.1783, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.654083 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.666005 [1] L1 tensor(101279.1094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.670519 [1] L2 tensor(480.7127, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.674442 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.685239 [1] L1 tensor(101816.7188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.689321 [1] L2 tensor(483.3638, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.693179 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.703474 [1] L1 tensor(102353.3828, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.708352 [1] L2 tensor(486.0809, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.712555 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.720413 [1] L1 tensor(102887.5938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.723817 [1] L2 tensor(488.8194, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.726765 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.733626 [1] L1 tensor(103417.3516, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.736489 [1] L2 tensor(491.5267, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.738966 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.745875 [1] L1 tensor(103939.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.748929 [1] L2 tensor(494.1456, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.751676 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.758673 [1] L1 tensor(104451.4609, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.761602 [1] L2 tensor(496.6192, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.764090 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.775340 [1] L1 tensor(104950.7891, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.781174 [1] L2 tensor(498.9020, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.784200 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.794326 [1] L1 tensor(105436.0156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.797323 [1] L2 tensor(500.9771, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.800417 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.809264 [1] L1 tensor(105906.2500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.812543 [1] L2 tensor(502.8626, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.816429 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.821916 [1] L1 tensor(106361.8828, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.824710 [1] L2 tensor(504.6041, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.827214 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.836212 [1] L1 tensor(106804.1875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.839315 [1] L2 tensor(506.2581, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.842300 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.848878 [1] L1 tensor(107234.6484, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.851721 [1] L2 tensor(507.8827, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.854193 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.860396 [1] L1 tensor(107654.6484, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.863249 [1] L2 tensor(509.5375, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.865712 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.877573 [1] L1 tensor(108064.9375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.882803 [1] L2 tensor(511.2832, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.886935 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.897001 [1] L1 tensor(108465.7656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.900559 [1] L2 tensor(513.1110, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.904153 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.912010 [1] L1 tensor(108857.4375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.915504 [1] L2 tensor(514.9779, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.918689 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.928446 [1] L1 tensor(109240.2734, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.931172 [1] L2 tensor(516.8826, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.933608 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.943193 [1] L1 tensor(109615.5859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.946454 [1] L2 tensor(518.7772, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.949549 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.957661 [1] L1 tensor(109982.1328, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.960477 [1] L2 tensor(520.5671, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.963094 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.972613 [1] L1 tensor(110338.7656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.975928 [1] L2 tensor(522.2888, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.979406 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.990721 [1] L1 tensor(110692.7812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.994124 [1] L2 tensor(524.0205, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:57.997406 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.007697 [1] L1 tensor(111052., device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.012169 [1] L2 tensor(525.9110, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.015900 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.026435 [1] L1 tensor(111417.2188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.030187 [1] L2 tensor(528.1443, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.033399 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.044447 [1] L1 tensor(111786.1250, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.051076 [1] L2 tensor(530.8073, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.055614 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.067072 [1] L1 tensor(112154.0391, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.070863 [1] L2 tensor(533.7219, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.074234 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.088567 [1] L1 tensor(112519.6875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.095314 [1] L2 tensor(536.6843, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.098147 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.107524 [1] L1 tensor(112882.5859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.110496 [1] L2 tensor(539.6161, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.113217 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.123039 [1] L1 tensor(113245.3047, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.126823 [1] L2 tensor(542.6851, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.130338 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.139237 [1] L1 tensor(113612.5781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.142368 [1] L2 tensor(546.6616, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.145030 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.155828 [1] L1 tensor(113989.9844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.159294 [1] L2 tensor(551.9972, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.162682 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.170004 [1] L1 tensor(114380.4688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.173710 [1] L2 tensor(558.6211, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.176880 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.184677 [1] L1 tensor(114780.8750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.187600 [1] L2 tensor(565.5475, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.190184 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.196963 [1] L1 tensor(115179.3203, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.199575 [1] L2 tensor(571.8965, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.202091 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.208280 [1] L1 tensor(115564.9688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.211240 [1] L2 tensor(577.6750, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.213969 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.221026 [1] L1 tensor(115930.7344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.223896 [1] L2 tensor(582.9812, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.226427 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.233203 [1] L1 tensor(116275.7891, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.236070 [1] L2 tensor(587.8970, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.238583 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.244919 [1] L1 tensor(116604.2500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.247960 [1] L2 tensor(592.4769, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.250727 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.258109 [1] L1 tensor(116924.8828, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.261001 [1] L2 tensor(596.7745, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.263637 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.273414 [1] L1 tensor(117243.2812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.277010 [1] L2 tensor(600.9517, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.280931 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.286815 [1] L1 tensor(117560.0078, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.289590 [1] L2 tensor(605.2882, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.292114 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.298519 [1] L1 tensor(117869.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.301381 [1] L2 tensor(609.3769, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.304049 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.310526 [1] L1 tensor(118170.5938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.313822 [1] L2 tensor(613.2401, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.316634 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.323370 [1] L1 tensor(118466.1797, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.326331 [1] L2 tensor(616.9527, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.328937 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.335724 [1] L1 tensor(118757.5469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.338818 [1] L2 tensor(620.6245, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.341765 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.348764 [1] L1 tensor(119055.6719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.351628 [1] L2 tensor(624.6906, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.354219 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.360533 [1] L1 tensor(119353.8125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.363401 [1] L2 tensor(629.0503, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.366008 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.372974 [1] L1 tensor(119651.6719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.375848 [1] L2 tensor(633.7117, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.378402 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.386147 [1] L1 tensor(119951.7031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.389593 [1] L2 tensor(638.6426, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.393121 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.399315 [1] L1 tensor(120246.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.402659 [1] L2 tensor(643.5576, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.405363 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.412662 [1] L1 tensor(120528.3984, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.415591 [1] L2 tensor(648.2136, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.418469 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.424866 [1] L1 tensor(120794.4141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.427623 [1] L2 tensor(652.5324, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.430199 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.438991 [1] L1 tensor(121047.2344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.443403 [1] L2 tensor(656.5266, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.447484 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.458168 [1] L1 tensor(121291.5312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.462215 [1] L2 tensor(660.2404, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.466333 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.474995 [1] L1 tensor(121527.8516, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.478121 [1] L2 tensor(663.6710, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.480788 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.488215 [1] L1 tensor(121754.1875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.491142 [1] L2 tensor(666.8120, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.493910 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.501491 [1] L1 tensor(121968.5312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.505256 [1] L2 tensor(669.6666, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.507793 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.514484 [1] L1 tensor(122170.7188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.517528 [1] L2 tensor(672.2468, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.520182 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.527274 [1] L1 tensor(122361.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.530190 [1] L2 tensor(674.5669, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.533098 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.539841 [1] L1 tensor(122540.0625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.542795 [1] L2 tensor(676.6407, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.545378 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.552199 [1] L1 tensor(122707.8359, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.555259 [1] L2 tensor(678.4823, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.557851 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.564910 [1] L1 tensor(122864.9375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.567783 [1] L2 tensor(680.1068, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.570315 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.576794 [1] L1 tensor(123011.9922, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.579890 [1] L2 tensor(681.5306, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.582795 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.589530 [1] L1 tensor(123149.7812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.592382 [1] L2 tensor(682.7717, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.594863 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.601481 [1] L1 tensor(123279.1094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.604378 [1] L2 tensor(683.8482, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.606894 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.615845 [1] L1 tensor(123400.6250, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.620260 [1] L2 tensor(684.7769, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.623877 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.633016 [1] L1 tensor(123514.8438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.636208 [1] L2 tensor(685.5725, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.639336 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.650133 [1] L1 tensor(123622.0781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.653516 [1] L2 tensor(686.2490, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.656953 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.666672 [1] L1 tensor(123722.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.670445 [1] L2 tensor(686.8252, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.674175 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.683831 [1] L1 tensor(123817.1016, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.687882 [1] L2 tensor(687.3473, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.691581 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.698715 [1] L1 tensor(123906.9219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.701298 [1] L2 tensor(687.9568, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.703985 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.710922 [1] L1 tensor(123993.9375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.714234 [1] L2 tensor(688.7982, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.716965 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.723678 [1] L1 tensor(124077.0625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.726523 [1] L2 tensor(689.6525, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.729271 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.736017 [1] L1 tensor(124155.8750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.738670 [1] L2 tensor(690.4263, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.741288 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.747357 [1] L1 tensor(124230.9219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.749943 [1] L2 tensor(691.1112, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.752430 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.758544 [1] L1 tensor(124302.4062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.761328 [1] L2 tensor(691.7091, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.763922 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.770020 [1] L1 tensor(124370.0625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.772658 [1] L2 tensor(692.2183, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.775507 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.781752 [1] L1 tensor(124433.7344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.784620 [1] L2 tensor(692.6386, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.787277 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.793975 [1] L1 tensor(124493.4844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.797035 [1] L2 tensor(692.9735, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.800066 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.806506 [1] L1 tensor(124549.5781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.809238 [1] L2 tensor(693.2279, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.811914 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.818655 [1] L1 tensor(124602.2656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.821837 [1] L2 tensor(693.4070, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.824391 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.830985 [1] L1 tensor(124651.8828, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.833809 [1] L2 tensor(693.5162, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.836589 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.842797 [1] L1 tensor(124698.7188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.845389 [1] L2 tensor(693.5605, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.847979 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.854209 [1] L1 tensor(124743.0312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.856965 [1] L2 tensor(693.5450, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.859567 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.865934 [1] L1 tensor(124785.1016, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.869097 [1] L2 tensor(693.4740, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.871799 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.878669 [1] L1 tensor(124825.1719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.881674 [1] L2 tensor(693.3520, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.884714 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.891086 [1] L1 tensor(124863.4375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.893839 [1] L2 tensor(693.1831, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.896510 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.903247 [1] L1 tensor(124900.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.906788 [1] L2 tensor(692.9709, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.909666 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.916741 [1] L1 tensor(124935.2578, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.919946 [1] L2 tensor(692.7190, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.922804 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.929647 [1] L1 tensor(124969.0703, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.932887 [1] L2 tensor(692.4306, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.935933 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.943736 [1] L1 tensor(125001.6094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.946570 [1] L2 tensor(692.1086, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.949039 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.955503 [1] L1 tensor(125032.9453, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.958215 [1] L2 tensor(691.7561, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.960957 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.967289 [1] L1 tensor(125063.1719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.970332 [1] L2 tensor(691.3757, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.972869 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.979841 [1] L1 tensor(125092.3281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.985165 [1] L2 tensor(690.9703, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:58.988453 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.997733 [1] L1 tensor(125120.4531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.001293 [1] L2 tensor(690.5424, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.005188 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.011339 [1] L1 tensor(125147.6484, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.014179 [1] L2 tensor(690.0947, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.019037 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.025846 [1] L1 tensor(125173.9141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.028621 [1] L2 tensor(689.6293, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.031474 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.037664 [1] L1 tensor(125199.3438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.040936 [1] L2 tensor(689.1484, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.046438 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.053487 [1] L1 tensor(125223.9922, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.056391 [1] L2 tensor(688.6539, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.059371 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.066374 [1] L1 tensor(125247.8984, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.069291 [1] L2 tensor(688.1476, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.072753 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.079530 [1] L1 tensor(125271.1172, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.082583 [1] L2 tensor(687.6306, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.085335 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.091519 [1] L1 tensor(125293.7344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.094286 [1] L2 tensor(687.1043, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.096975 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.103548 [1] L1 tensor(125315.7656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.106053 [1] L2 tensor(686.5692, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.108546 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.118134 [1] L1 tensor(125337.2969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.121076 [1] L2 tensor(686.0261, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.124199 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.130370 [1] L1 tensor(125358.3672, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.133200 [1] L2 tensor(685.4752, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.135816 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.142563 [1] L1 tensor(125379.0312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.145373 [1] L2 tensor(684.9169, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.147903 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.154500 [1] L1 tensor(125399.3359, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.157210 [1] L2 tensor(684.3511, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.159864 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.166166 [1] L1 tensor(125419.3438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.169305 [1] L2 tensor(683.7780, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.172163 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.178577 [1] L1 tensor(125439.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.182107 [1] L2 tensor(683.1976, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.184835 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.191916 [1] L1 tensor(125458.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.194623 [1] L2 tensor(682.6101, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.197381 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.203702 [1] L1 tensor(125478.0859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.206552 [1] L2 tensor(682.0157, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.209228 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.215387 [1] L1 tensor(125497.4531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.218047 [1] L2 tensor(681.4148, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.220539 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.226819 [1] L1 tensor(125516.8438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.229521 [1] L2 tensor(680.8077, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.232070 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.238488 [1] L1 tensor(125536.3438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.241344 [1] L2 tensor(680.1953, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.243915 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.250336 [1] L1 tensor(125556.0781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.253367 [1] L2 tensor(679.5782, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.256045 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.262827 [1] L1 tensor(125576.1562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.265709 [1] L2 tensor(678.9578, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.268567 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.274795 [1] L1 tensor(125596.7734, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.277468 [1] L2 tensor(678.3353, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.280583 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.288989 [1] L1 tensor(125618.1094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.292411 [1] L2 tensor(677.7127, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.295840 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.301901 [1] L1 tensor(125640.3438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.304414 [1] L2 tensor(677.0919, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.306961 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.313071 [1] L1 tensor(125663.6641, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.315885 [1] L2 tensor(676.4749, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.318383 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.324978 [1] L1 tensor(125688.1172, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.327813 [1] L2 tensor(675.8633, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.330358 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.336545 [1] L1 tensor(125713.7344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.339517 [1] L2 tensor(675.2583, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.342118 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.349576 [1] L1 tensor(125740.4375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.355595 [1] L2 tensor(674.6611, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.359732 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.366313 [1] L1 tensor(125768.1484, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.369303 [1] L2 tensor(674.0732, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.371942 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.378622 [1] L1 tensor(125796.7969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.381386 [1] L2 tensor(673.4963, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.384179 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.393953 [1] L1 tensor(125826.2969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.396715 [1] L2 tensor(672.9331, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.399361 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.405988 [1] L1 tensor(125856.5703, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.408756 [1] L2 tensor(672.3866, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.411548 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.417936 [1] L1 tensor(125887.5391, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.421212 [1] L2 tensor(671.8606, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.423955 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.431119 [1] L1 tensor(125919.0781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.433933 [1] L2 tensor(671.3599, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.436601 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.443337 [1] L1 tensor(125951.0625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.446188 [1] L2 tensor(670.8901, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.449127 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.455278 [1] L1 tensor(125983.2969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.458049 [1] L2 tensor(670.4584, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.460596 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.470012 [1] L1 tensor(126015.5391, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.473948 [1] L2 tensor(670.0754, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.477883 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.487811 [1] L1 tensor(126047.5469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.491059 [1] L2 tensor(669.7568, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.494389 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.504639 [1] L1 tensor(126079.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.509786 [1] L2 tensor(669.5269, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.514230 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.520604 [1] L1 tensor(126110.1797, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.523467 [1] L2 tensor(669.4199, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.526310 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.532499 [1] L1 tensor(126140.5781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.535000 [1] L2 tensor(669.4784, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.537474 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.543463 [1] L1 tensor(126170.3125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.546102 [1] L2 tensor(669.7474, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.548625 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.558198 [1] L1 tensor(126199.3516, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.562530 [1] L2 tensor(670.2666, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.566624 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.573109 [1] L1 tensor(126227.7656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.576007 [1] L2 tensor(671.0658, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.578864 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.588807 [1] L1 tensor(126255.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.591803 [1] L2 tensor(672.1641, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.595119 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.601047 [1] L1 tensor(126283.1094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.604028 [1] L2 tensor(673.5726, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.606689 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.613895 [1] L1 tensor(126310.3828, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.616598 [1] L2 tensor(675.2971, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.619167 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.625371 [1] L1 tensor(126337.7422, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.628125 [1] L2 tensor(677.3395, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.630828 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.636815 [1] L1 tensor(126365.6328, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.639590 [1] L2 tensor(679.6990, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.642168 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.649242 [1] L1 tensor(126394.6094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.652310 [1] L2 tensor(682.3726, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.657716 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.665551 [1] L1 tensor(126425.3281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.668284 [1] L2 tensor(685.3538, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.671293 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.678470 [1] L1 tensor(126458.4375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.681016 [1] L2 tensor(688.6296, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.683631 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.695034 [1] L1 tensor(126494.3438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.701561 [1] L2 tensor(692.1733, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.704474 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.715312 [1] L1 tensor(126532.9844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.723327 [1] L2 tensor(695.9346, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.730224 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.741663 [1] L1 tensor(126573.9688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.746112 [1] L2 tensor(699.8345, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.750316 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.761235 [1] L1 tensor(126616.7969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.766535 [1] L2 tensor(703.7738, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.770715 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.781197 [1] L1 tensor(126661., device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.785242 [1] L2 tensor(707.6608, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.789406 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.795655 [1] L1 tensor(126706.3516, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.799089 [1] L2 tensor(711.4360, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.801861 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.808586 [1] L1 tensor(126752.7500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.811450 [1] L2 tensor(715.0779, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.813967 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.820398 [1] L1 tensor(126800.0859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.823383 [1] L2 tensor(718.5892, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.826084 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.834813 [1] L1 tensor(126847.7969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.837930 [1] L2 tensor(721.9753, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.841081 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.847241 [1] L1 tensor(126894.9141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.850048 [1] L2 tensor(725.2344, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.852567 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.859574 [1] L1 tensor(126940.5938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.862800 [1] L2 tensor(728.3699, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.865480 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.871698 [1] L1 tensor(126984.4688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.876192 [1] L2 tensor(731.3950, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.879262 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.886269 [1] L1 tensor(127026.3750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.889011 [1] L2 tensor(734.3306, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.891637 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.902558 [1] L1 tensor(127066.2656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.906753 [1] L2 tensor(737.2029, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.910305 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.923826 [1] L1 tensor(127104.2109, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.928475 [1] L2 tensor(740.0406, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.933990 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.944254 [1] L1 tensor(127140.2734, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.947518 [1] L2 tensor(742.8735, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.950502 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.961712 [1] L1 tensor(127174.5859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.966726 [1] L2 tensor(745.7286, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.969584 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.980099 [1] L1 tensor(127207.3125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.984361 [1] L2 tensor(748.6293, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:31:59.988262 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.996604 [1] L1 tensor(127238.6797, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.000151 [1] L2 tensor(751.5946, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.003302 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.009989 [1] L1 tensor(127268.9375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.012713 [1] L2 tensor(754.6411, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.015289 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.022366 [1] L1 tensor(127298.5078, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.027189 [1] L2 tensor(757.7836, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.030132 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.037128 [1] L1 tensor(127327.9219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.040681 [1] L2 tensor(761.0363, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.043712 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.051786 [1] L1 tensor(127358.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.054810 [1] L2 tensor(764.4130, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.057883 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.064926 [1] L1 tensor(127390.2656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.067962 [1] L2 tensor(767.9260, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.070859 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.080280 [1] L1 tensor(127426.0391, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.082891 [1] L2 tensor(771.5828, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.085763 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.094382 [1] L1 tensor(127466.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.097234 [1] L2 tensor(775.3823, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.100130 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.107184 [1] L1 tensor(127513.9062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.110109 [1] L2 tensor(779.3130, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.112661 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.118910 [1] L1 tensor(127566.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.121878 [1] L2 tensor(783.3530, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.124514 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.131106 [1] L1 tensor(127623.1562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.134082 [1] L2 tensor(787.4728, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:1', grad_fn=<SumBackward0>)
20:32:00.136931 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:10.338155 [1] proc begin: <DistEnv 1/4 nccl>
20:33:10.425701 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
20:33:10.445204 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:33:11.591037 [1] L1 tensor(91906.2109, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:12.324821 [1] L2 tensor(8166.0913, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:12.338342 [1] L3 tensor(8193.4414, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:12.342043 [1] L4 tensor(460.0110, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:12.371935 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.197119 [1] L1 tensor(92158.0234, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.200947 [1] L2 tensor(8003.2510, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.203689 [1] L3 tensor(8178.0820, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.206067 [1] L4 tensor(466.4110, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.208578 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.224165 [1] L1 tensor(92284.5781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.230534 [1] L2 tensor(7882.0869, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.234431 [1] L3 tensor(8160.8027, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.236644 [1] L4 tensor(471.2665, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.239115 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.248039 [1] L1 tensor(92265.5859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.250776 [1] L2 tensor(7761.1973, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.253486 [1] L3 tensor(8135.3574, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.256629 [1] L4 tensor(474.9828, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.259831 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.272476 [1] L1 tensor(92279.3828, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.278199 [1] L2 tensor(7658.1450, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.280409 [1] L3 tensor(8111.2231, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.282718 [1] L4 tensor(478.8577, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.285163 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.303320 [1] L1 tensor(92242.1875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.309418 [1] L2 tensor(7563.7065, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.312130 [1] L3 tensor(8093.5737, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.315031 [1] L4 tensor(481.4165, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.317425 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.334552 [1] L1 tensor(92148.2969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.341004 [1] L2 tensor(7474.9492, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.343951 [1] L3 tensor(8078.6621, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.346167 [1] L4 tensor(483.2287, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.348553 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.361348 [1] L1 tensor(92004.4766, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.367449 [1] L2 tensor(7388.9067, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.371731 [1] L3 tensor(8064.4102, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.374498 [1] L4 tensor(484.1880, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.376880 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.390967 [1] L1 tensor(91864.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.395872 [1] L2 tensor(7305.5322, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.398305 [1] L3 tensor(8050.8521, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.400996 [1] L4 tensor(485.2452, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.403580 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.418701 [1] L1 tensor(91703.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.423179 [1] L2 tensor(7224.3052, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.426889 [1] L3 tensor(8039.9307, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.430454 [1] L4 tensor(486.2715, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.432820 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.445876 [1] L1 tensor(91546.7656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.449592 [1] L2 tensor(7142.9131, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.452844 [1] L3 tensor(8030.0137, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.456109 [1] L4 tensor(486.3793, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.459041 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.473246 [1] L1 tensor(91400.9062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.476431 [1] L2 tensor(7063.3184, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.479299 [1] L3 tensor(8020.9849, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.482061 [1] L4 tensor(486.5335, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.485128 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.499576 [1] L1 tensor(91263.8906, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.504245 [1] L2 tensor(6990.2002, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.508055 [1] L3 tensor(8012.2500, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.510324 [1] L4 tensor(486.5843, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.512678 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.530450 [1] L1 tensor(91150.9844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.535579 [1] L2 tensor(6922.4463, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.539701 [1] L3 tensor(8003.8706, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.542165 [1] L4 tensor(486.6861, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.544516 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.555063 [1] L1 tensor(91041.7422, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.558256 [1] L2 tensor(6858.5234, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.560422 [1] L3 tensor(7995.8789, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.562845 [1] L4 tensor(486.6684, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.565186 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.578420 [1] L1 tensor(90944.8906, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.581093 [1] L2 tensor(6798.0029, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.583545 [1] L3 tensor(7988.0479, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.586130 [1] L4 tensor(486.6975, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.588854 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.604383 [1] L1 tensor(90839.7344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.610277 [1] L2 tensor(6739.6226, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.612492 [1] L3 tensor(7982.2271, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.614753 [1] L4 tensor(486.7326, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.617101 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.635199 [1] L1 tensor(90740.9141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.641719 [1] L2 tensor(6684.4434, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.644064 [1] L3 tensor(7978.1631, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.646271 [1] L4 tensor(486.8357, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.653977 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.666794 [1] L1 tensor(90655.0781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.672576 [1] L2 tensor(6633.7432, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.675802 [1] L3 tensor(7974.1128, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.678030 [1] L4 tensor(486.9752, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.680414 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.693899 [1] L1 tensor(90576.1328, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.699721 [1] L2 tensor(6587.6748, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.703234 [1] L3 tensor(7970.1611, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.705421 [1] L4 tensor(487.1800, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.707784 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.716707 [1] L1 tensor(90507.4062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.719583 [1] L2 tensor(6544.5615, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.722648 [1] L3 tensor(7965.9619, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.725811 [1] L4 tensor(487.3255, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.728452 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.739647 [1] L1 tensor(90447.1250, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.742998 [1] L2 tensor(6504.0693, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.745823 [1] L3 tensor(7961.7642, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.748749 [1] L4 tensor(487.4957, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.751539 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.766697 [1] L1 tensor(90383.7891, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.773267 [1] L2 tensor(6466.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.776773 [1] L3 tensor(7957.6348, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.779503 [1] L4 tensor(487.7361, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.781848 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.791779 [1] L1 tensor(90323.0156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.796630 [1] L2 tensor(6432.1631, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.800013 [1] L3 tensor(7953.8613, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.803369 [1] L4 tensor(488.0311, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.805768 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.822343 [1] L1 tensor(90261.3281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.827608 [1] L2 tensor(6398.6284, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.831772 [1] L3 tensor(7950.1064, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.834374 [1] L4 tensor(488.2441, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.836720 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.850426 [1] L1 tensor(90206.2969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.856533 [1] L2 tensor(6365.5928, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.860174 [1] L3 tensor(7946.6143, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.862947 [1] L4 tensor(488.3713, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.865280 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.874048 [1] L1 tensor(90156.0469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.877802 [1] L2 tensor(6332.5015, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.881422 [1] L3 tensor(7944.2500, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.884287 [1] L4 tensor(488.5137, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.886763 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.904821 [1] L1 tensor(90114.7656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.910650 [1] L2 tensor(6298.9160, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.912833 [1] L3 tensor(7941.5576, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.915027 [1] L4 tensor(488.6197, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.917465 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.926467 [1] L1 tensor(90077.8828, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.929114 [1] L2 tensor(6265.0801, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.931359 [1] L3 tensor(7938.5840, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.933553 [1] L4 tensor(488.7382, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.936026 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.947051 [1] L1 tensor(90045.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.950392 [1] L2 tensor(6230.8857, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.953224 [1] L3 tensor(7935.4272, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.955742 [1] L4 tensor(488.8261, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.958241 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.975762 [1] L1 tensor(90015.5938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.981928 [1] L2 tensor(6196.9541, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.984150 [1] L3 tensor(7932.2021, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.986442 [1] L4 tensor(488.9282, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:13.988828 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.999845 [1] L1 tensor(89986.9375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.002502 [1] L2 tensor(6162.7266, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.005396 [1] L3 tensor(7929.9180, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.008572 [1] L4 tensor(489.0490, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.011517 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.027753 [1] L1 tensor(89958.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.034255 [1] L2 tensor(6129.5806, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.036451 [1] L3 tensor(7927.6050, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.038643 [1] L4 tensor(489.1157, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.041169 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.050487 [1] L1 tensor(89930.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.054666 [1] L2 tensor(6097.7295, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.058726 [1] L3 tensor(7925.4434, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.062718 [1] L4 tensor(489.2115, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.065161 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.082241 [1] L1 tensor(89903.3125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.087458 [1] L2 tensor(6066.8984, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.091537 [1] L3 tensor(7923.3789, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.093834 [1] L4 tensor(489.3509, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.096217 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.107609 [1] L1 tensor(89876.5938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.114030 [1] L2 tensor(6037.3926, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.116514 [1] L3 tensor(7921.4487, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.118992 [1] L4 tensor(489.5436, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.121376 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.134701 [1] L1 tensor(89850.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.140412 [1] L2 tensor(6008.5723, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.143650 [1] L3 tensor(7920.0205, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.146432 [1] L4 tensor(489.7487, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.148896 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.162279 [1] L1 tensor(89827.0078, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.165758 [1] L2 tensor(5980.7783, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.168758 [1] L3 tensor(7919.0127, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.171847 [1] L4 tensor(489.9792, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.174898 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.188925 [1] L1 tensor(89805.1328, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.192078 [1] L2 tensor(5954.4502, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.194725 [1] L3 tensor(7918.4092, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.197530 [1] L4 tensor(490.2450, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.200393 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.215816 [1] L1 tensor(89782.6172, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.220026 [1] L2 tensor(5929.5557, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.223923 [1] L3 tensor(7917.1353, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.226209 [1] L4 tensor(490.4838, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.228582 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.237381 [1] L1 tensor(89760.8828, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.240863 [1] L2 tensor(5906.1587, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.244176 [1] L3 tensor(7915.2920, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.247059 [1] L4 tensor(490.7103, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.250436 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.261798 [1] L1 tensor(89742.0859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.265621 [1] L2 tensor(5883.9072, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.269083 [1] L3 tensor(7913.0674, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.271929 [1] L4 tensor(490.8849, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.274324 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.291299 [1] L1 tensor(89725.9844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.297425 [1] L2 tensor(5863.2334, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.300091 [1] L3 tensor(7910.7129, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.302286 [1] L4 tensor(491.0353, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.304655 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.318152 [1] L1 tensor(89711.5156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.323028 [1] L2 tensor(5844.0439, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.327014 [1] L3 tensor(7908.2539, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.329436 [1] L4 tensor(491.1834, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.331859 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.341004 [1] L1 tensor(89696.6797, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.344592 [1] L2 tensor(5826.1553, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.347704 [1] L3 tensor(7906.1543, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.350610 [1] L4 tensor(491.3640, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.353208 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.363746 [1] L1 tensor(89685.2969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.368529 [1] L2 tensor(5809.2324, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.372979 [1] L3 tensor(7903.4854, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.376127 [1] L4 tensor(491.2783, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.378866 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.389705 [1] L1 tensor(89668.5156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.395421 [1] L2 tensor(5782.9229, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.399499 [1] L3 tensor(7899.7290, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.402118 [1] L4 tensor(491.1868, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.404669 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.415506 [1] L1 tensor(89654.3672, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.422254 [1] L2 tensor(5758.6558, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.425003 [1] L3 tensor(7895.5166, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.427444 [1] L4 tensor(491.1088, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.429796 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.441925 [1] L1 tensor(89639.9453, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.446380 [1] L2 tensor(5733.5200, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.451038 [1] L3 tensor(7891.9409, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.454412 [1] L4 tensor(491.0822, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.457026 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.469139 [1] L1 tensor(89627.0625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.472575 [1] L2 tensor(5711.6113, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.475477 [1] L3 tensor(7888.9756, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.478590 [1] L4 tensor(491.1104, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.481813 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.497344 [1] L1 tensor(89618.0391, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.502392 [1] L2 tensor(5692.4775, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.506098 [1] L3 tensor(7886.5928, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.510717 [1] L4 tensor(491.1937, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.513128 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.528168 [1] L1 tensor(89613.0547, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.531851 [1] L2 tensor(5675.2920, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.535826 [1] L3 tensor(7883.5737, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.539451 [1] L4 tensor(491.2465, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.541852 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.558166 [1] L1 tensor(89608.4219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.563822 [1] L2 tensor(5659.4434, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.567892 [1] L3 tensor(7879.7822, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.570564 [1] L4 tensor(491.3054, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.572949 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.586438 [1] L1 tensor(89605.6953, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.592466 [1] L2 tensor(5645.0059, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.595933 [1] L3 tensor(7875.9121, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.598881 [1] L4 tensor(491.3718, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.601222 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.614559 [1] L1 tensor(89606.6719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.620396 [1] L2 tensor(5631.7451, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.623817 [1] L3 tensor(7871.9946, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.626275 [1] L4 tensor(491.4679, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.628734 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.641852 [1] L1 tensor(89606.4219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.645469 [1] L2 tensor(5619.5215, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.648823 [1] L3 tensor(7867.1426, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.652276 [1] L4 tensor(491.5798, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.655519 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.670292 [1] L1 tensor(89606.5469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.674491 [1] L2 tensor(5607.9160, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.677944 [1] L3 tensor(7862.7344, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.681397 [1] L4 tensor(491.7184, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.684077 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.698771 [1] L1 tensor(89606.2344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.704458 [1] L2 tensor(5597.3418, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.708112 [1] L3 tensor(7857.4141, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.710553 [1] L4 tensor(491.8526, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.713166 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.727469 [1] L1 tensor(89612.2891, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.733222 [1] L2 tensor(5587.6641, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.735945 [1] L3 tensor(7851.6646, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.738137 [1] L4 tensor(491.7527, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.740489 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.754996 [1] L1 tensor(89632.0625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.760264 [1] L2 tensor(5579.0195, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.763949 [1] L3 tensor(7845.3750, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.767509 [1] L4 tensor(491.6373, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.769876 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.782729 [1] L1 tensor(89654.5938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.788679 [1] L2 tensor(5570.6724, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.792295 [1] L3 tensor(7839.6465, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.795533 [1] L4 tensor(491.5720, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.797928 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.814465 [1] L1 tensor(89675.2266, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.820522 [1] L2 tensor(5563.5771, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.823754 [1] L3 tensor(7834.9033, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.825929 [1] L4 tensor(491.5689, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.828283 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.842684 [1] L1 tensor(89687.7031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.848294 [1] L2 tensor(5557.3779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.852047 [1] L3 tensor(7831.0566, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.854475 [1] L4 tensor(491.6295, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.856841 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.869497 [1] L1 tensor(89693.2031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.876054 [1] L2 tensor(5551.9375, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.879801 [1] L3 tensor(7828.0684, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.882299 [1] L4 tensor(491.7559, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.884670 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.899312 [1] L1 tensor(89693., device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.904486 [1] L2 tensor(5545.9360, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.908521 [1] L3 tensor(7824.0986, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.911502 [1] L4 tensor(491.8821, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.913905 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.931247 [1] L1 tensor(89679.2031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.937860 [1] L2 tensor(5538.7637, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.940227 [1] L3 tensor(7819.1602, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.942418 [1] L4 tensor(492.0254, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.944793 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.956338 [1] L1 tensor(89648.5234, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.961047 [1] L2 tensor(5529.5820, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.964127 [1] L3 tensor(7814.4331, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.966726 [1] L4 tensor(492.1658, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.969086 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.985309 [1] L1 tensor(89593.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.990742 [1] L2 tensor(5518.6353, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.995289 [1] L3 tensor(7808.9355, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:14.997527 [1] L4 tensor(492.2938, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.000025 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.015611 [1] L1 tensor(89530.0625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.020658 [1] L2 tensor(5505.3262, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.024407 [1] L3 tensor(7803.7461, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.027416 [1] L4 tensor(492.4462, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.029774 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.047343 [1] L1 tensor(89445.8906, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.053075 [1] L2 tensor(5488.7334, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.055962 [1] L3 tensor(7797.5410, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.058489 [1] L4 tensor(492.5581, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.060919 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.078209 [1] L1 tensor(89355.7266, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.082937 [1] L2 tensor(5471.3774, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.087354 [1] L3 tensor(7791.6758, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.089613 [1] L4 tensor(492.6813, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.092069 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.109106 [1] L1 tensor(89248.2734, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.114643 [1] L2 tensor(5452.1006, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.118953 [1] L3 tensor(7784.8213, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.121151 [1] L4 tensor(492.5934, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.123517 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.139164 [1] L1 tensor(89145.8047, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.145257 [1] L2 tensor(5433.4092, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.148004 [1] L3 tensor(7779.3276, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.150205 [1] L4 tensor(492.5703, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.152565 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.168161 [1] L1 tensor(89048.8438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.173891 [1] L2 tensor(5415.8945, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.176246 [1] L3 tensor(7774.9517, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.178528 [1] L4 tensor(492.6166, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.181397 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.197054 [1] L1 tensor(88959.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.202491 [1] L2 tensor(5399.6489, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.204678 [1] L3 tensor(7770.6099, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.206886 [1] L4 tensor(492.6955, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.209278 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.227301 [1] L1 tensor(88878.0781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.232983 [1] L2 tensor(5384.5879, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.235795 [1] L3 tensor(7765.0078, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.238689 [1] L4 tensor(492.7455, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.241042 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.255470 [1] L1 tensor(88802.2422, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.261437 [1] L2 tensor(5370.6543, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.264429 [1] L3 tensor(7758.4502, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.267018 [1] L4 tensor(492.7861, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.269453 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.288104 [1] L1 tensor(88732.7500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.294137 [1] L2 tensor(5357.8354, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.297335 [1] L3 tensor(7752.1958, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.300247 [1] L4 tensor(492.8394, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.302606 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.317504 [1] L1 tensor(88671.4844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.322602 [1] L2 tensor(5346.4995, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.327071 [1] L3 tensor(7746.3706, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.329262 [1] L4 tensor(492.9205, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.331630 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.346921 [1] L1 tensor(88615.1016, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.352746 [1] L2 tensor(5336.1084, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.355847 [1] L3 tensor(7739.6787, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.358039 [1] L4 tensor(492.9848, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.360396 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.376360 [1] L1 tensor(88563.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.382123 [1] L2 tensor(5327.1206, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.384286 [1] L3 tensor(7734.1802, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.386729 [1] L4 tensor(493.0863, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.389072 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.406925 [1] L1 tensor(88516.3828, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.412616 [1] L2 tensor(5318.8306, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.415716 [1] L3 tensor(7729.6104, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.417967 [1] L4 tensor(493.2258, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.420373 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.435325 [1] L1 tensor(88471.7578, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.441009 [1] L2 tensor(5310.9170, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.443912 [1] L3 tensor(7724.9180, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.447039 [1] L4 tensor(493.3699, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.449392 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.466843 [1] L1 tensor(88430.3281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.471889 [1] L2 tensor(5303.4268, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.475850 [1] L3 tensor(7718.8301, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.478112 [1] L4 tensor(493.3211, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.480504 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.496332 [1] L1 tensor(88391.3281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.502111 [1] L2 tensor(5296.4053, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.504359 [1] L3 tensor(7711.6699, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.507707 [1] L4 tensor(493.2770, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.510191 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.527363 [1] L1 tensor(88354.8281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.533915 [1] L2 tensor(5289.8555, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.536088 [1] L3 tensor(7704.6060, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.539026 [1] L4 tensor(493.2583, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.541382 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.554447 [1] L1 tensor(88320.0859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.560922 [1] L2 tensor(5283.6401, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.563883 [1] L3 tensor(7696.5981, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.566086 [1] L4 tensor(493.2222, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.568488 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.583136 [1] L1 tensor(88286.7422, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.587874 [1] L2 tensor(5277.6035, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.590458 [1] L3 tensor(7687.3916, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.593040 [1] L4 tensor(493.1475, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.595525 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.609613 [1] L1 tensor(88254.2266, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.616587 [1] L2 tensor(5272.0479, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.620171 [1] L3 tensor(7679.6270, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.622676 [1] L4 tensor(493.1252, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.625023 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.643479 [1] L1 tensor(88223.7500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.649591 [1] L2 tensor(5267.0005, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.652128 [1] L3 tensor(7673.1987, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.654722 [1] L4 tensor(493.1563, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.657095 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.670740 [1] L1 tensor(88195.3750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.673326 [1] L2 tensor(5262.1768, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.675843 [1] L3 tensor(7665.4185, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.678505 [1] L4 tensor(493.1565, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.681398 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.699239 [1] L1 tensor(88169.5234, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.705621 [1] L2 tensor(5257.5640, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.708135 [1] L3 tensor(7657.7012, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.710359 [1] L4 tensor(493.1763, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.712723 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.726141 [1] L1 tensor(88146.3125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.729793 [1] L2 tensor(5253.3750, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.733803 [1] L3 tensor(7649.1050, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.736562 [1] L4 tensor(493.1922, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.739178 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.754643 [1] L1 tensor(88124.6719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.760295 [1] L2 tensor(5249.3330, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.763963 [1] L3 tensor(7641.1553, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.766831 [1] L4 tensor(493.2054, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.769184 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.779871 [1] L1 tensor(88103.2969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.785610 [1] L2 tensor(5245.1709, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.788027 [1] L3 tensor(7633.7900, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.790277 [1] L4 tensor(493.2280, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.792851 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.806648 [1] L1 tensor(88083.1562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.813178 [1] L2 tensor(5241.0825, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.816326 [1] L3 tensor(7625.3706, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.820125 [1] L4 tensor(493.2283, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.823142 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.840707 [1] L1 tensor(88063.4609, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.846650 [1] L2 tensor(5237.4404, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.849032 [1] L3 tensor(7618.4922, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.851270 [1] L4 tensor(493.2749, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.853945 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.862968 [1] L1 tensor(88045.2500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.866275 [1] L2 tensor(5234.2280, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.869259 [1] L3 tensor(7613.3066, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.871869 [1] L4 tensor(493.3685, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.874266 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.886202 [1] L1 tensor(88028.2969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.892964 [1] L2 tensor(5230.9102, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.896279 [1] L3 tensor(7607.8213, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.898989 [1] L4 tensor(493.4725, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.901359 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.914488 [1] L1 tensor(88012.5234, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.917104 [1] L2 tensor(5227.5815, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.919599 [1] L3 tensor(7602.0537, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.922251 [1] L4 tensor(493.5916, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.925019 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.943719 [1] L1 tensor(87998.4141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.947644 [1] L2 tensor(5224.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.949954 [1] L3 tensor(7595.0410, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.952519 [1] L4 tensor(493.6966, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.955087 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.971400 [1] L1 tensor(87984.1484, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.978081 [1] L2 tensor(5220.7720, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.980276 [1] L3 tensor(7588.8262, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.983357 [1] L4 tensor(493.8004, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.985737 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.994857 [1] L1 tensor(87970.6172, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.997486 [1] L2 tensor(5216.4814, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:15.999990 [1] L3 tensor(7580.7461, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.002806 [1] L4 tensor(493.8594, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.005567 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.017660 [1] L1 tensor(87958.2656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.020427 [1] L2 tensor(5212.1792, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.022721 [1] L3 tensor(7571.4580, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.025068 [1] L4 tensor(493.8966, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.027617 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.044114 [1] L1 tensor(87945.2500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.049585 [1] L2 tensor(5207.4111, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.052777 [1] L3 tensor(7564.4092, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.055549 [1] L4 tensor(493.9751, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.058909 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.071151 [1] L1 tensor(87933.4375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.077725 [1] L2 tensor(5203.1699, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.080259 [1] L3 tensor(7559.2153, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.082579 [1] L4 tensor(494.0815, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.085002 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.094392 [1] L1 tensor(87923.9219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.097041 [1] L2 tensor(5198.9482, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.099557 [1] L3 tensor(7551.9668, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.102117 [1] L4 tensor(494.1476, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.104886 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.115736 [1] L1 tensor(87916.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.120496 [1] L2 tensor(5195.6895, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.124956 [1] L3 tensor(7545.8496, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.127773 [1] L4 tensor(494.2112, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.130213 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.140085 [1] L1 tensor(87910.0547, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.145394 [1] L2 tensor(5192.7354, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.150126 [1] L3 tensor(7538.3682, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.152985 [1] L4 tensor(494.2507, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.155505 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.168672 [1] L1 tensor(87903.9219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.174074 [1] L2 tensor(5189.7148, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.178047 [1] L3 tensor(7530.5679, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.180675 [1] L4 tensor(494.3002, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.183091 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.192056 [1] L1 tensor(87898.8750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.195912 [1] L2 tensor(5186.5176, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.198973 [1] L3 tensor(7522.4727, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.202247 [1] L4 tensor(494.3649, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.204630 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.216200 [1] L1 tensor(87894.3594, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.219365 [1] L2 tensor(5182.8486, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.222076 [1] L3 tensor(7512.7363, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.224321 [1] L4 tensor(494.2433, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.227116 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.243687 [1] L1 tensor(87889.8359, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.250173 [1] L2 tensor(5177.3799, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.252447 [1] L3 tensor(7504.2822, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.255118 [1] L4 tensor(494.1383, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.257531 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.266734 [1] L1 tensor(87877.6484, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.269488 [1] L2 tensor(5158.8506, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.272163 [1] L3 tensor(7497.7524, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.275199 [1] L4 tensor(494.0794, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.278317 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.292231 [1] L1 tensor(87862.9062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.297087 [1] L2 tensor(5139.9697, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.299897 [1] L3 tensor(7493.0527, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.302299 [1] L4 tensor(494.0676, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.304722 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.320108 [1] L1 tensor(87850.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.326317 [1] L2 tensor(5121.5024, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.328526 [1] L3 tensor(7486.5840, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.330847 [1] L4 tensor(494.0327, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.333570 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.347907 [1] L1 tensor(87847.7891, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.353768 [1] L2 tensor(5107.2832, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.356688 [1] L3 tensor(7478.6582, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.359691 [1] L4 tensor(493.9893, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.362778 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.372109 [1] L1 tensor(87847.3359, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.375774 [1] L2 tensor(5094.4785, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.379842 [1] L3 tensor(7472.2329, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.383620 [1] L4 tensor(493.9891, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.386034 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.398097 [1] L1 tensor(87846.8516, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.404055 [1] L2 tensor(5082.9023, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.407857 [1] L3 tensor(7464.3438, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.411142 [1] L4 tensor(493.9799, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.413615 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.423517 [1] L1 tensor(87845.4062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.426110 [1] L2 tensor(5071.0684, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.430321 [1] L3 tensor(7454.1279, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.435143 [1] L4 tensor(493.9351, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.437516 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.448364 [1] L1 tensor(87839.1719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.454326 [1] L2 tensor(5058.6934, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.459016 [1] L3 tensor(7445.1729, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.461341 [1] L4 tensor(493.8968, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.463926 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.474927 [1] L1 tensor(87832.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.479631 [1] L2 tensor(5046.3984, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.484111 [1] L3 tensor(7435.6611, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.488397 [1] L4 tensor(493.8749, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.491581 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.508204 [1] L1 tensor(87827.7500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.513952 [1] L2 tensor(5035.6021, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.516272 [1] L3 tensor(7425.6304, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.519389 [1] L4 tensor(493.8727, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.521787 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.535957 [1] L1 tensor(87824.9375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.541787 [1] L2 tensor(5026.0786, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.544729 [1] L3 tensor(7415.1670, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.548021 [1] L4 tensor(493.8905, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.550409 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.566258 [1] L1 tensor(87822.3906, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.572797 [1] L2 tensor(5017.4712, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.575696 [1] L3 tensor(7405.8457, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.578195 [1] L4 tensor(493.9127, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.580581 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.590153 [1] L1 tensor(87819.9609, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.596035 [1] L2 tensor(5009.7021, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.599819 [1] L3 tensor(7398.2642, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.602107 [1] L4 tensor(493.9695, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.604478 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.618489 [1] L1 tensor(87820.1172, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.624307 [1] L2 tensor(5002.7212, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.627651 [1] L3 tensor(7392.2432, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.630082 [1] L4 tensor(494.0606, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.634229 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.651092 [1] L1 tensor(87821.1875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.658025 [1] L2 tensor(4996.1846, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.660431 [1] L3 tensor(7383.5356, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.662667 [1] L4 tensor(494.1093, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.665082 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.675872 [1] L1 tensor(87822.3359, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.678506 [1] L2 tensor(4990.2451, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.680683 [1] L3 tensor(7373.4629, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.683164 [1] L4 tensor(494.1425, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.685717 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.702978 [1] L1 tensor(87823.6875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.709202 [1] L2 tensor(4984.8652, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.712052 [1] L3 tensor(7365.1074, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.714279 [1] L4 tensor(494.2082, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.716687 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.729317 [1] L1 tensor(87825.0547, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.732627 [1] L2 tensor(4980.0254, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.735654 [1] L3 tensor(7355.3647, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.738899 [1] L4 tensor(494.2617, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.742241 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.757025 [1] L1 tensor(87826.6328, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.762896 [1] L2 tensor(4975.6338, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.765432 [1] L3 tensor(7346.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.767980 [1] L4 tensor(494.3123, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.770646 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.786493 [1] L1 tensor(87828.1094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.791937 [1] L2 tensor(4971.6523, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.796139 [1] L3 tensor(7339.1299, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.799982 [1] L4 tensor(494.3657, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.804130 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.819876 [1] L1 tensor(87830.5781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.825604 [1] L2 tensor(4968.1919, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.828125 [1] L3 tensor(7332.8350, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.831431 [1] L4 tensor(494.4484, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.833849 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.843007 [1] L1 tensor(87834.9766, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.845868 [1] L2 tensor(4964.9360, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.848483 [1] L3 tensor(7323.6309, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.851187 [1] L4 tensor(494.4904, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.854111 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.866813 [1] L1 tensor(87839.1094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.870866 [1] L2 tensor(4962.0059, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.873087 [1] L3 tensor(7316.2178, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.876057 [1] L4 tensor(494.5668, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.878963 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.895425 [1] L1 tensor(87842.6094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.900132 [1] L2 tensor(4959.1416, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.903949 [1] L3 tensor(7307.6260, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.906860 [1] L4 tensor(494.6472, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.909233 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.922913 [1] L1 tensor(87845.6484, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.927599 [1] L2 tensor(4956.2622, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.930577 [1] L3 tensor(7297.9170, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.933810 [1] L4 tensor(494.7349, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.936403 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.945866 [1] L1 tensor(87848.4766, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.949931 [1] L2 tensor(4953.4678, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.953400 [1] L3 tensor(7287.2148, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.955902 [1] L4 tensor(494.8544, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.958300 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.968893 [1] L1 tensor(87851.2578, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.972043 [1] L2 tensor(4950.8477, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.974663 [1] L3 tensor(7274.8057, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.977930 [1] L4 tensor(494.9356, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.980916 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.994021 [1] L1 tensor(87853.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:16.999279 [1] L2 tensor(4948.4082, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.003279 [1] L3 tensor(7260.9014, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.006107 [1] L4 tensor(494.9958, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.008489 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.022726 [1] L1 tensor(87856.4609, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.028460 [1] L2 tensor(4946.2856, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.032227 [1] L3 tensor(7249.3174, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.035355 [1] L4 tensor(495.0984, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.037732 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.054740 [1] L1 tensor(87858.3828, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.061518 [1] L2 tensor(4944.3633, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.064118 [1] L3 tensor(7239.1226, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.066885 [1] L4 tensor(495.1625, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.069258 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.083005 [1] L1 tensor(87860., device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.089642 [1] L2 tensor(4942.6050, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.093350 [1] L3 tensor(7230.2114, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.096226 [1] L4 tensor(495.2415, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.098996 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.114557 [1] L1 tensor(87860.3750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.120686 [1] L2 tensor(4941.0249, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.124052 [1] L3 tensor(7223.1445, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.126955 [1] L4 tensor(495.3672, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.129589 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.146405 [1] L1 tensor(87858., device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.151348 [1] L2 tensor(4939.2759, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.155606 [1] L3 tensor(7212.8096, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.158490 [1] L4 tensor(495.4567, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.160868 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.175639 [1] L1 tensor(87852.9375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.181347 [1] L2 tensor(4937.5049, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.185337 [1] L3 tensor(7200.2646, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.187917 [1] L4 tensor(495.3598, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.190332 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.204227 [1] L1 tensor(87847.4844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.209784 [1] L2 tensor(4935.8750, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.214198 [1] L3 tensor(7186.3662, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.216392 [1] L4 tensor(495.2668, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.218766 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.235412 [1] L1 tensor(87842.7266, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.241564 [1] L2 tensor(4934.3506, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.244087 [1] L3 tensor(7171.2842, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.246304 [1] L4 tensor(495.1876, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.248674 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.263423 [1] L1 tensor(87838.3203, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.268224 [1] L2 tensor(4933.0273, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.272100 [1] L3 tensor(7158.6123, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.275541 [1] L4 tensor(495.1689, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.278602 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.289771 [1] L1 tensor(87834.4219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.294451 [1] L2 tensor(4931.7891, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.299443 [1] L3 tensor(7148.1392, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.302029 [1] L4 tensor(495.2084, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.304485 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.316029 [1] L1 tensor(87830.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.322768 [1] L2 tensor(4930.4761, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.325540 [1] L3 tensor(7136.3931, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.327861 [1] L4 tensor(495.2696, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.330228 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.347114 [1] L1 tensor(87827.0625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.353202 [1] L2 tensor(4929.1138, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.355959 [1] L3 tensor(7123.4688, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.358800 [1] L4 tensor(495.3551, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.361168 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.373072 [1] L1 tensor(87824.0859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.376220 [1] L2 tensor(4927.7314, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.379123 [1] L3 tensor(7109.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.382142 [1] L4 tensor(495.4666, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.385348 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.402526 [1] L1 tensor(87821.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.406037 [1] L2 tensor(4926.4697, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.408800 [1] L3 tensor(7097.2656, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.411926 [1] L4 tensor(495.5835, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.415016 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.429432 [1] L1 tensor(87819.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.433348 [1] L2 tensor(4925.3184, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.435791 [1] L3 tensor(7086.5986, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.438759 [1] L4 tensor(495.7116, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.441730 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.458591 [1] L1 tensor(87817.3438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.463711 [1] L2 tensor(4924.1738, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.467120 [1] L3 tensor(7072.6729, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.469663 [1] L4 tensor(495.8012, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.472238 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.488541 [1] L1 tensor(87816.3125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.494451 [1] L2 tensor(4923.0361, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.499262 [1] L3 tensor(7055.8457, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.501929 [1] L4 tensor(495.8672, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.504400 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.516185 [1] L1 tensor(87816.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.522078 [1] L2 tensor(4922.0430, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.524701 [1] L3 tensor(7041.5811, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.527007 [1] L4 tensor(495.9786, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.529642 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.547085 [1] L1 tensor(87816.3672, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.553174 [1] L2 tensor(4920.9980, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.556027 [1] L3 tensor(7025.3525, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.558614 [1] L4 tensor(495.9046, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.561082 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.569915 [1] L1 tensor(87816.6328, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.572489 [1] L2 tensor(4919.9131, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.574692 [1] L3 tensor(7007.4951, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.577072 [1] L4 tensor(495.8124, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.579500 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.594446 [1] L1 tensor(87816.7969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.600233 [1] L2 tensor(4918.9038, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.604199 [1] L3 tensor(6989.8047, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.607192 [1] L4 tensor(495.7487, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.609569 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.619243 [1] L1 tensor(87817.1250, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.621920 [1] L2 tensor(4918.0283, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.624172 [1] L3 tensor(6974.7393, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.626440 [1] L4 tensor(495.7438, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.628810 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.642238 [1] L1 tensor(87817.3672, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.646542 [1] L2 tensor(4917.2441, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.650094 [1] L3 tensor(6962.0801, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.653900 [1] L4 tensor(495.7943, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.656521 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.671022 [1] L1 tensor(87817.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.675595 [1] L2 tensor(4916.4922, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.679671 [1] L3 tensor(6951.0078, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.683268 [1] L4 tensor(495.8583, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.685697 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.702677 [1] L1 tensor(87817.5859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.708413 [1] L2 tensor(4915.6729, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.711595 [1] L3 tensor(6938.9263, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.713845 [1] L4 tensor(495.9427, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.716296 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.731644 [1] L1 tensor(87817.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.737316 [1] L2 tensor(4914.9434, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.740036 [1] L3 tensor(6925.6074, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.743119 [1] L4 tensor(496.0112, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.745495 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.762687 [1] L1 tensor(87817.3516, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.768741 [1] L2 tensor(4914.1479, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.771712 [1] L3 tensor(6911.4238, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.774103 [1] L4 tensor(496.1013, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.776537 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.791895 [1] L1 tensor(87817.0781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.798136 [1] L2 tensor(4913.3506, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.800466 [1] L3 tensor(6895.6016, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.803560 [1] L4 tensor(496.1635, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.805942 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.817815 [1] L1 tensor(87816.9844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.823591 [1] L2 tensor(4912.5967, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.827173 [1] L3 tensor(6881.6929, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.829462 [1] L4 tensor(496.2386, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.832030 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.846858 [1] L1 tensor(87816.7109, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.851608 [1] L2 tensor(4911.8398, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.855881 [1] L3 tensor(6866.4502, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.859545 [1] L4 tensor(496.1516, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.861923 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.878518 [1] L1 tensor(87816.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.884964 [1] L2 tensor(4911.1885, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.887874 [1] L3 tensor(6850.4536, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.890414 [1] L4 tensor(496.0685, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.892804 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.906146 [1] L1 tensor(87816.3594, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.909712 [1] L2 tensor(4910.6191, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.913402 [1] L3 tensor(6837.1035, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.917194 [1] L4 tensor(496.0426, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.920182 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.936684 [1] L1 tensor(87816.3594, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.940934 [1] L2 tensor(4909.9902, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.944656 [1] L3 tensor(6822.9180, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.947565 [1] L4 tensor(496.0471, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.949947 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.966671 [1] L1 tensor(87816.3516, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.972986 [1] L2 tensor(4909.4629, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.975774 [1] L3 tensor(6811.2065, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.978647 [1] L4 tensor(496.1016, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.981042 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.989946 [1] L1 tensor(87817.8906, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.992522 [1] L2 tensor(4908.9551, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.995928 [1] L3 tensor(6802.2363, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:17.998620 [1] L4 tensor(496.1695, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.001163 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.014914 [1] L1 tensor(87818.4844, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.020276 [1] L2 tensor(4908.3877, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.023846 [1] L3 tensor(6790.6660, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.026932 [1] L4 tensor(496.2030, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.029298 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.043053 [1] L1 tensor(87819.0391, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.048531 [1] L2 tensor(4907.8096, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.052206 [1] L3 tensor(6776.8120, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.054718 [1] L4 tensor(496.2157, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.057127 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.070409 [1] L1 tensor(87819.3047, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.075765 [1] L2 tensor(4907.1206, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.078303 [1] L3 tensor(6762.2666, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.080832 [1] L4 tensor(496.2554, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.083356 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.098827 [1] L1 tensor(87819.4375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.105295 [1] L2 tensor(4906.5566, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.108575 [1] L3 tensor(6750.1934, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.111352 [1] L4 tensor(496.3405, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.113734 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.129592 [1] L1 tensor(87820.1328, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.135970 [1] L2 tensor(4905.9561, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.139745 [1] L3 tensor(6737.9741, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.141945 [1] L4 tensor(496.3931, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.144316 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.153254 [1] L1 tensor(87820.9609, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.156186 [1] L2 tensor(4905.3721, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.159258 [1] L3 tensor(6729.5947, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.162333 [1] L4 tensor(496.4587, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.165712 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.176698 [1] L1 tensor(87821.6875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.181833 [1] L2 tensor(4904.7139, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.186309 [1] L3 tensor(6719.8472, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.188513 [1] L4 tensor(496.5469, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.190944 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.207174 [1] L1 tensor(87822.3906, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.212838 [1] L2 tensor(4904.1538, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.215732 [1] L3 tensor(6712.1387, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.218594 [1] L4 tensor(496.6760, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.220994 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.234517 [1] L1 tensor(87823.0547, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.239076 [1] L2 tensor(4903.5703, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.243318 [1] L3 tensor(6702.2544, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.246739 [1] L4 tensor(496.6421, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.249106 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.262418 [1] L1 tensor(87823.6875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.266116 [1] L2 tensor(4903.0459, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.270081 [1] L3 tensor(6690.9297, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.274084 [1] L4 tensor(496.6011, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.276737 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.285528 [1] L1 tensor(87824.3438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.288675 [1] L2 tensor(4902.5718, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.291231 [1] L3 tensor(6678.4316, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.293823 [1] L4 tensor(496.5623, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.296250 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.310520 [1] L1 tensor(87824.9688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.314190 [1] L2 tensor(4902.1846, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.317670 [1] L3 tensor(6668.1768, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.320925 [1] L4 tensor(496.5762, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.323664 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.339889 [1] L1 tensor(87825.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.346238 [1] L2 tensor(4901.8174, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.348406 [1] L3 tensor(6659.1904, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.351439 [1] L4 tensor(496.6059, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.353839 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.370569 [1] L1 tensor(87825.8750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.377052 [1] L2 tensor(4901.3413, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.379941 [1] L3 tensor(6648.9121, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.382584 [1] L4 tensor(496.6585, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.384952 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.399940 [1] L1 tensor(87825.9297, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.406000 [1] L2 tensor(4900.7842, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.410915 [1] L3 tensor(6636.2969, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.413119 [1] L4 tensor(496.6808, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.415504 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.431283 [1] L1 tensor(87825.6250, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.435341 [1] L2 tensor(4900.2134, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.438750 [1] L3 tensor(6621.6216, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.442550 [1] L4 tensor(496.6862, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.444923 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.458142 [1] L1 tensor(87825.2344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.463788 [1] L2 tensor(4899.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.467381 [1] L3 tensor(6606.3105, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.469579 [1] L4 tensor(496.7193, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.471949 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.487362 [1] L1 tensor(87824.8281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.493862 [1] L2 tensor(4899.0005, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.496127 [1] L3 tensor(6593.4424, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.498334 [1] L4 tensor(496.7961, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.500686 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.514546 [1] L1 tensor(87824.4688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.519252 [1] L2 tensor(4898.4658, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.523341 [1] L3 tensor(6581.9854, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.527164 [1] L4 tensor(496.8827, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.529540 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.546072 [1] L1 tensor(87823.9609, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.551744 [1] L2 tensor(4897.8975, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.555399 [1] L3 tensor(6568.4492, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.557606 [1] L4 tensor(496.9326, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.559976 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.568834 [1] L1 tensor(87823.5547, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.572072 [1] L2 tensor(4897.4316, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.574723 [1] L3 tensor(6556.8013, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.577503 [1] L4 tensor(497.0202, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.580073 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.595770 [1] L1 tensor(87823.2344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.600417 [1] L2 tensor(4896.9404, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.603937 [1] L3 tensor(6543.8242, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.607404 [1] L4 tensor(496.9702, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.609797 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.622782 [1] L1 tensor(87822.9609, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.627552 [1] L2 tensor(4896.4204, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.631618 [1] L3 tensor(6529.0703, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.633860 [1] L4 tensor(496.9082, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.636221 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.652213 [1] L1 tensor(87822.8984, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.658188 [1] L2 tensor(4895.8350, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.660395 [1] L3 tensor(6513.7393, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.662607 [1] L4 tensor(496.8792, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:1', grad_fn=<SumBackward0>)
20:33:18.664978 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:54.036895 [1] proc begin: <DistEnv 1/4 nccl>
20:33:54.075294 [1] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 1, |V|: 677, |E|: 3206>
20:33:54.090483 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3859 KiB |   3881 KiB |   3928 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     69 KiB |     91 KiB |    138 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3854 KiB |   3875 KiB |   3920 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     64 KiB |     85 KiB |    130 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18668 KiB |  18707 KiB |  18804 KiB | 138752 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1978 KiB |   2045 KiB |   2114 KiB | 138752 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:33:55.206253 [1] L1 tensor(91906.2109, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:56.505724 [1] L2 tensor(8166.0913, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:56.539563 [1] L3 tensor(8193.4414, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:56.543415 [1] L4 tensor(8182.3984, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:56.546607 [1] L5 tensor(8131.8223, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:56.549681 [1] L6 tensor(8150.5381, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:56.553233 [1] L7 tensor(8157.7505, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:56.556299 [1] L8 tensor(450.2043, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:56.577969 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.458696 [1] L1 tensor(92030.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.463768 [1] L2 tensor(8282.8516, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.466919 [1] L3 tensor(8357.2812, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.469901 [1] L4 tensor(8181.0479, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.473158 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.476096 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.479331 [1] L7 tensor(7993.9106, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.482306 [1] L8 tensor(459.1643, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.486343 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.511209 [1] L1 tensor(92075.4531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.517023 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.519728 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.522294 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.524737 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.527043 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.529353 [1] L7 tensor(7922.6777, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.531590 [1] L8 tensor(468.1117, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.533983 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.551438 [1] L1 tensor(92072.3750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.557307 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.559846 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.563227 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.565949 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.568167 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.570365 [1] L7 tensor(8014.0752, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.572521 [1] L8 tensor(476.9705, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.575076 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.591475 [1] L1 tensor(92088.2344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.597887 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.601236 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.604078 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.606521 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.608766 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.611217 [1] L7 tensor(8104.1484, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.614208 [1] L8 tensor(484.1437, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.617427 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.641971 [1] L1 tensor(92158.3281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.647074 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.649388 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.651975 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.654534 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.656851 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.659194 [1] L7 tensor(8171.7554, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.661457 [1] L8 tensor(491.6121, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.664011 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.687074 [1] L1 tensor(92215.9141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.693690 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.696141 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.698394 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.700699 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.703028 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.705388 [1] L7 tensor(8224.4043, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.707778 [1] L8 tensor(497.6973, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.710311 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.727649 [1] L1 tensor(92258.8984, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.733406 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.735845 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.738028 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.740302 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.742601 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.744883 [1] L7 tensor(8195.3975, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.747225 [1] L8 tensor(503.7690, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.749702 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.767470 [1] L1 tensor(92304.7188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.773252 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.776550 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.779169 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.781315 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.783542 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.785944 [1] L7 tensor(8266.2363, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.788220 [1] L8 tensor(510.6747, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.790646 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.814583 [1] L1 tensor(92351.0312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.819568 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.822193 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.824967 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.828604 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.830958 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.833210 [1] L7 tensor(8319.0059, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.835451 [1] L8 tensor(517.6276, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.837857 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.855107 [1] L1 tensor(92393.8594, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.861797 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.864181 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.866564 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.868785 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.871046 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.873309 [1] L7 tensor(8378.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.875598 [1] L8 tensor(524.2488, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.878051 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.895497 [1] L1 tensor(92392.6875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.902109 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.904377 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.906772 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.908927 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.911218 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.913522 [1] L7 tensor(8392.5078, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.915869 [1] L8 tensor(530.5532, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.918297 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.938947 [1] L1 tensor(92393.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.943928 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.947720 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.950524 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.952849 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.955110 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.957400 [1] L7 tensor(8399.4043, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.959654 [1] L8 tensor(537.3357, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.962035 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.979343 [1] L1 tensor(92393.1875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.984756 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.987714 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.989878 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.992125 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.994409 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.996689 [1] L7 tensor(8414.0801, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:57.998970 [1] L8 tensor(544.3667, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.001360 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.025721 [1] L1 tensor(92428.7656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.031861 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.035416 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.037721 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.039980 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.042240 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.044497 [1] L7 tensor(8413.8184, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.046797 [1] L8 tensor(550.5079, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.049188 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.066735 [1] L1 tensor(92428.5859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.072769 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.075911 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.078273 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.080592 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.082991 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.085346 [1] L7 tensor(8355.0488, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.087706 [1] L8 tensor(555.9419, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.090461 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.108847 [1] L1 tensor(92428.6953, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.114738 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.117149 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.119892 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.122272 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.124693 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.127080 [1] L7 tensor(8290.1934, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.129445 [1] L8 tensor(562.5477, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.132086 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.151444 [1] L1 tensor(92418.9375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.157505 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.161021 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.163663 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.165864 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.168080 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.170480 [1] L7 tensor(8219.0967, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.172774 [1] L8 tensor(569.4016, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.175364 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.197990 [1] L1 tensor(92402.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.204545 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.207737 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.210070 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.212397 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.214751 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.217105 [1] L7 tensor(8148.9014, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.219410 [1] L8 tensor(576.3892, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.221969 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.245490 [1] L1 tensor(92384.8047, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.251132 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.254921 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.257322 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.259715 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.262102 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.264423 [1] L7 tensor(8093.5479, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.266761 [1] L8 tensor(582.6779, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.269352 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.292470 [1] L1 tensor(92361.9453, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.298412 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.300783 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.303105 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.305418 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.307771 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.310105 [1] L7 tensor(8046.7803, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.312424 [1] L8 tensor(589.5211, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.315008 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.335763 [1] L1 tensor(92339.8750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.340707 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.344641 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.347518 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.349831 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.352200 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.354520 [1] L7 tensor(8028.6914, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.356851 [1] L8 tensor(596.7709, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.359949 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.384444 [1] L1 tensor(92315.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.390440 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.392870 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.395227 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.397562 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.399945 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.402265 [1] L7 tensor(7966.2676, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.404587 [1] L8 tensor(604.4969, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.407196 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.426041 [1] L1 tensor(92295.8906, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.430368 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.432768 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.435103 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.437426 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.439730 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.442047 [1] L7 tensor(7895.0420, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.444378 [1] L8 tensor(612.1814, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.446948 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.470402 [1] L1 tensor(92276.7891, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.477170 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.480161 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.482532 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.484843 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.487185 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.489488 [1] L7 tensor(7822.0205, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.491823 [1] L8 tensor(619.7263, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.494408 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.517845 [1] L1 tensor(92255.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.524371 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.527677 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.530564 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.533397 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.536264 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.539135 [1] L7 tensor(7735.8633, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.541966 [1] L8 tensor(627.0621, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.545092 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.566777 [1] L1 tensor(92245.4141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.572153 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.575915 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.578764 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.581604 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.584398 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.587297 [1] L7 tensor(7640.6553, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.590142 [1] L8 tensor(634.7565, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.593229 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.614556 [1] L1 tensor(92235.1875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.618531 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.621386 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.624313 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.627228 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.630119 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.634201 [1] L7 tensor(7652.3271, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.637330 [1] L8 tensor(642.3046, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.640463 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.662454 [1] L1 tensor(92220.7734, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.667830 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.671906 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.674837 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.677786 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.680711 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.683659 [1] L7 tensor(7614.2900, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.686567 [1] L8 tensor(650.1603, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.689858 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.706377 [1] L1 tensor(92207.3125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.712883 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.716036 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.719009 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.721907 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.724816 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.727815 [1] L7 tensor(7582.5835, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.730771 [1] L8 tensor(658.2415, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.734409 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.753618 [1] L1 tensor(92190.8438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.759168 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.761362 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.763577 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.765744 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.767904 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.770124 [1] L7 tensor(7547.9399, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.772294 [1] L8 tensor(665.7829, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.774736 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.795736 [1] L1 tensor(92185.7344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.802450 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.805949 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.808150 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.810317 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.812464 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.814678 [1] L7 tensor(7515.8604, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.816861 [1] L8 tensor(672.8223, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.819320 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.842100 [1] L1 tensor(92183.1094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.846592 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.849508 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.852464 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.856394 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.861782 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.866056 [1] L7 tensor(7476.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.868256 [1] L8 tensor(680.0869, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.870776 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.893840 [1] L1 tensor(92181.9062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.898612 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.902605 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.904825 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.907083 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.909314 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.911558 [1] L7 tensor(7443.5044, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.913736 [1] L8 tensor(687.2549, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.916202 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.939960 [1] L1 tensor(92180.3672, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.946614 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.951086 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.955930 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.960718 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.963661 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.965852 [1] L7 tensor(7404.4849, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.968022 [1] L8 tensor(694.3385, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:58.970481 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.998411 [1] L1 tensor(92164.9062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.004265 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.007892 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.010513 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.012676 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.014866 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.017014 [1] L7 tensor(7326.3232, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.019231 [1] L8 tensor(700.2833, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.021708 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.042946 [1] L1 tensor(92148.5391, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.048313 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.051531 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.054522 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.056721 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.058907 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.061077 [1] L7 tensor(7246.6230, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.063337 [1] L8 tensor(706.7782, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.065844 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.087318 [1] L1 tensor(92127.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.093916 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.096290 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.098560 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.100752 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.104609 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.108248 [1] L7 tensor(7189.6465, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.113613 [1] L8 tensor(713.7858, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.116565 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.141351 [1] L1 tensor(92107.5078, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.146989 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.151479 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.153692 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.155866 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.158113 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.160353 [1] L7 tensor(7135.2983, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.162622 [1] L8 tensor(720.7952, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.165134 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.185907 [1] L1 tensor(92092.4688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.192496 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.195708 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.197949 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.200333 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.202540 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.204747 [1] L7 tensor(7084.1738, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.207005 [1] L8 tensor(728.2336, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.209493 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.231457 [1] L1 tensor(92074.7422, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.235687 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.239637 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.242205 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.244477 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.246784 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.248981 [1] L7 tensor(7037.1904, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.251215 [1] L8 tensor(735.5088, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.253662 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.274817 [1] L1 tensor(92067.1562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.281390 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.284034 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.286272 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.288451 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.290622 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.292866 [1] L7 tensor(7023.4854, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.295065 [1] L8 tensor(743.4106, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.297507 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.321880 [1] L1 tensor(92060.0547, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.327732 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.331444 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.333646 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.335832 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.338022 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.340220 [1] L7 tensor(6988.1309, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.342415 [1] L8 tensor(752.2716, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.344877 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.362726 [1] L1 tensor(92051.6719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.368579 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.371776 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.374241 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.376570 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.378755 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.380958 [1] L7 tensor(6953.8452, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.383172 [1] L8 tensor(761.9441, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.385641 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.402710 [1] L1 tensor(92048.3516, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.409218 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.412023 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.414402 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.416662 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.418826 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.421012 [1] L7 tensor(6912.2715, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.423216 [1] L8 tensor(771.5643, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.425628 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.446944 [1] L1 tensor(92044.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.451631 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.455616 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.457840 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.460040 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.462237 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.464433 [1] L7 tensor(6886.2417, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.466597 [1] L8 tensor(781.2713, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.469035 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.486628 [1] L1 tensor(92047.0391, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.491884 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.495880 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.498438 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.500591 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.502785 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.504929 [1] L7 tensor(6890.9771, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.507099 [1] L8 tensor(790.4954, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.509516 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.533203 [1] L1 tensor(92048.0469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.538417 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.541254 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.544171 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.546368 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.548551 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.550769 [1] L7 tensor(6886.4268, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.552932 [1] L8 tensor(799.3633, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.555370 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.578050 [1] L1 tensor(92049.0625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.583471 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.587557 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.590037 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.592229 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.594462 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.596634 [1] L7 tensor(6894.8955, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.598856 [1] L8 tensor(808.7374, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.601330 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.625016 [1] L1 tensor(92048.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.630824 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.633862 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.636326 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.638495 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.640662 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.642904 [1] L7 tensor(6898.0254, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.645067 [1] L8 tensor(818.4190, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.647501 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.670865 [1] L1 tensor(92054.2812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.677445 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.679930 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.682136 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.684346 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.686520 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.688718 [1] L7 tensor(6895.3540, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.690922 [1] L8 tensor(826.7911, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.693360 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.716606 [1] L1 tensor(92055.1953, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.723168 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.728334 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.733221 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.737930 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.740125 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.742369 [1] L7 tensor(6929.4932, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.744563 [1] L8 tensor(835.1722, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.747031 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.764658 [1] L1 tensor(92060.4219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.770635 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.772847 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.775071 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.777238 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.779418 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.781615 [1] L7 tensor(6966.1211, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.783792 [1] L8 tensor(843.9580, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.786253 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.804736 [1] L1 tensor(92063.9688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.810294 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.812844 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.815102 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.817407 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.819672 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.821879 [1] L7 tensor(7004.0137, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.824091 [1] L8 tensor(852.5330, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.826560 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.843914 [1] L1 tensor(92065.7422, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.850360 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.852565 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.854800 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.856970 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.859154 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.861387 [1] L7 tensor(6996.9292, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.863591 [1] L8 tensor(861.7837, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.866084 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.889795 [1] L1 tensor(92066.0156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.895439 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.898239 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.901033 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.903488 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.905801 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.907995 [1] L7 tensor(6986.4253, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.910274 [1] L8 tensor(871.3632, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.912713 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.935955 [1] L1 tensor(92066.9062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.943401 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.945850 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.948316 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.950846 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.953363 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.955908 [1] L7 tensor(6986.7188, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.958440 [1] L8 tensor(881.7352, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.961099 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.984345 [1] L1 tensor(92066.3828, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.990326 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.992711 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.995603 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:33:59.997883 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.000396 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.002984 [1] L7 tensor(6997.6528, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.005536 [1] L8 tensor(893.2026, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.008241 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.030451 [1] L1 tensor(92064.5859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.036468 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.039576 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.041810 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.044368 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.046981 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.049553 [1] L7 tensor(6996.4224, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.052166 [1] L8 tensor(905.3839, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.054999 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.076798 [1] L1 tensor(92061.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.082016 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.086407 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.088680 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.090885 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.093080 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.095340 [1] L7 tensor(7007.0684, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.097535 [1] L8 tensor(917.2354, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.100090 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.123176 [1] L1 tensor(92057.4062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.131444 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.134555 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.136833 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.139087 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.141656 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.144257 [1] L7 tensor(7016.7778, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.146849 [1] L8 tensor(928.8240, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.149714 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.174425 [1] L1 tensor(92052.2812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.178829 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.183255 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.186193 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.188383 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.190634 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.193185 [1] L7 tensor(7031.3984, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.195791 [1] L8 tensor(938.5357, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.198500 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.220550 [1] L1 tensor(92047.0469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.226377 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.228621 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.231293 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.233680 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.236143 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.238697 [1] L7 tensor(7052.1240, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.241292 [1] L8 tensor(948.2116, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.243974 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.266583 [1] L1 tensor(92040.3438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.272416 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.275573 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.277804 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.280275 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.282829 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.285357 [1] L7 tensor(7068.1011, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.287950 [1] L8 tensor(958.4678, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.290618 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.314384 [1] L1 tensor(92032.8984, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.318685 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.323161 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.326003 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.328692 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.331307 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.333933 [1] L7 tensor(7079.6675, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.336480 [1] L8 tensor(967.2245, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.339198 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.358554 [1] L1 tensor(92024.8281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.364335 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.367548 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.369767 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.372337 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.374974 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.377478 [1] L7 tensor(7084.8979, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.379706 [1] L8 tensor(976.2678, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.382169 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.405202 [1] L1 tensor(92016.1562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.410636 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.414745 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.416950 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.419321 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.421923 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.424416 [1] L7 tensor(7097.3198, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.427025 [1] L8 tensor(985.8372, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.429726 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.452253 [1] L1 tensor(92006.9766, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.457599 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.461627 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.464149 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.466349 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.468914 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.471442 [1] L7 tensor(7090.5596, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.474051 [1] L8 tensor(996.5497, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.476731 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.499984 [1] L1 tensor(91997.3281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.504212 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.507786 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.510629 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.512946 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.515526 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.518090 [1] L7 tensor(7083.6445, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.520661 [1] L8 tensor(1006.6285, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.523371 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.546343 [1] L1 tensor(91987.2891, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.551364 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.554112 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.556771 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.559212 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.561587 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.564166 [1] L7 tensor(7090.2759, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.566771 [1] L8 tensor(1017.0908, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.569501 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.593906 [1] L1 tensor(91976.7500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.599756 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.603279 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.605593 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.608157 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.610772 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.613353 [1] L7 tensor(7101.8350, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.616006 [1] L8 tensor(1028.5674, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.618734 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.641983 [1] L1 tensor(91965.6250, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.646169 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.649288 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.652254 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.654791 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.657390 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.659943 [1] L7 tensor(7102.4204, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.662587 [1] L8 tensor(1040.0034, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.665258 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.683019 [1] L1 tensor(91954.2734, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.688867 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.691898 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.694108 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.696619 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.699238 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.701785 [1] L7 tensor(7109.1299, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.704427 [1] L8 tensor(1050.3302, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.707184 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.730174 [1] L1 tensor(91942.6094, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.734826 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.738080 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.740810 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.743278 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.745797 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.748318 [1] L7 tensor(7115.3667, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.750936 [1] L8 tensor(1061.2329, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.753546 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.770806 [1] L1 tensor(91930.6797, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.775205 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.779589 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.781875 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.784484 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.787072 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.789645 [1] L7 tensor(7121.3765, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.792218 [1] L8 tensor(1071.2010, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.794991 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.815022 [1] L1 tensor(91918.4531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.820832 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.823691 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.826040 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.828536 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.831161 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.833736 [1] L7 tensor(7129.3828, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.836048 [1] L8 tensor(1080.0259, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.838575 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.858738 [1] L1 tensor(91905.5078, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.865393 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.867856 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.870365 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.872640 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.874783 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.876962 [1] L7 tensor(7137.1338, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.879204 [1] L8 tensor(1087.9192, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.881644 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.899037 [1] L1 tensor(91894.9219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.905445 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.907918 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.910386 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.912643 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.914846 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.917065 [1] L7 tensor(7156.0615, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.919260 [1] L8 tensor(1097.4600, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.921704 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.938823 [1] L1 tensor(91883.9766, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.945333 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.947931 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.950206 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.952478 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.954704 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.956942 [1] L7 tensor(7170.8271, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.959182 [1] L8 tensor(1107.8916, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.961656 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.984423 [1] L1 tensor(91872.5938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.991033 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.994491 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.996684 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:00.998860 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.001022 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.003265 [1] L7 tensor(7185.2031, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.005439 [1] L8 tensor(1118.9893, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.007904 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.023937 [1] L1 tensor(91861., device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.029243 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.032225 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.035002 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.037820 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.040049 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.042325 [1] L7 tensor(7199.2324, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.044941 [1] L8 tensor(1130.3167, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.047635 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.065508 [1] L1 tensor(91849.3047, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.070790 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.075196 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.080659 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.084088 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.086395 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.088936 [1] L7 tensor(7210.0137, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.091524 [1] L8 tensor(1142.5631, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.094188 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.118049 [1] L1 tensor(91838.0625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.124197 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.127662 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.129865 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.132269 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.134843 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.137370 [1] L7 tensor(7226.8770, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.139974 [1] L8 tensor(1154.7158, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.142638 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.162712 [1] L1 tensor(91821.5547, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.165808 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.168298 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.170697 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.173022 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.175324 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.177503 [1] L7 tensor(7240.4722, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.179795 [1] L8 tensor(1167.4799, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.182754 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.214936 [1] L1 tensor(91805.3438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.221014 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.226370 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.231574 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.235600 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.240127 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.246404 [1] L7 tensor(7257.1367, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.249417 [1] L8 tensor(1180.9768, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.252236 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.271073 [1] L1 tensor(91789.4141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.275137 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.280088 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.284975 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.289880 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.294854 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.299883 [1] L7 tensor(7272.3467, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.303932 [1] L8 tensor(1194.1365, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.307667 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.328053 [1] L1 tensor(91775.6016, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.331470 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.334446 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.337307 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.339664 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.341857 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.344083 [1] L7 tensor(7296.3428, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.346660 [1] L8 tensor(1206.1471, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.349387 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.371685 [1] L1 tensor(91761.8359, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.375869 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.378261 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.380983 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.384304 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.388361 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.391372 [1] L7 tensor(7308.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.394335 [1] L8 tensor(1218.3230, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.397246 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.424384 [1] L1 tensor(91748.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.429140 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.433274 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.439065 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.443316 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.445805 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.448033 [1] L7 tensor(7320.4033, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.450673 [1] L8 tensor(1230.9602, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.453257 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.479815 [1] L1 tensor(91733.7422, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.484947 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.490324 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.495764 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.501045 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.506706 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.511756 [1] L7 tensor(7331.7754, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.515819 [1] L8 tensor(1243.7366, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.520195 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.544242 [1] L1 tensor(91725.0156, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.548370 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.552002 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.555289 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.560439 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.563023 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.565807 [1] L7 tensor(7327.8857, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.568887 [1] L8 tensor(1255.2939, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.573311 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.596353 [1] L1 tensor(91716.0234, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.599613 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.602044 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.604339 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.606590 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.608830 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.611131 [1] L7 tensor(7322.5342, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.613572 [1] L8 tensor(1266.2397, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.619894 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.645604 [1] L1 tensor(91704.2969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.650031 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.653738 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.657342 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.660861 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.664317 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.667710 [1] L7 tensor(7321.3926, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.671067 [1] L8 tensor(1276.5854, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.674969 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.695880 [1] L1 tensor(91693.7188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.703749 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.709930 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.715749 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.721225 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.727702 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.732747 [1] L7 tensor(7320.0356, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.738085 [1] L8 tensor(1287.2778, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.745569 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.766738 [1] L1 tensor(91684.1562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.772334 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.777120 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.783174 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.787876 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.792567 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.797513 [1] L7 tensor(7318.8408, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.801734 [1] L8 tensor(1298.6079, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.806445 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.825045 [1] L1 tensor(91675.5469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.830553 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.834279 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.837537 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.840485 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.843490 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.846230 [1] L7 tensor(7317.9033, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.849221 [1] L8 tensor(1309.6160, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.852640 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.887452 [1] L1 tensor(91665.1484, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.894189 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.899691 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.905588 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.911353 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.916923 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.922666 [1] L7 tensor(7317.0186, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.927660 [1] L8 tensor(1321.7622, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.930572 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.952409 [1] L1 tensor(91655.7656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.961448 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.965061 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.968364 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.971629 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.974836 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.978062 [1] L7 tensor(7316.6641, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.981176 [1] L8 tensor(1335.4668, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:01.984858 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.015366 [1] L1 tensor(91647.3438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.020753 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.024535 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.028195 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.031571 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.035723 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.039453 [1] L7 tensor(7315.8281, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.043105 [1] L8 tensor(1349.3062, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.047760 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.074101 [1] L1 tensor(91639.7344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.080760 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.086150 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.091865 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.097580 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.103081 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.108650 [1] L7 tensor(7315.3213, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.114350 [1] L8 tensor(1363.8130, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.120442 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.148448 [1] L1 tensor(91632.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.154340 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.159988 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.165670 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.171190 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.177080 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.182654 [1] L7 tensor(7315.0063, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.188399 [1] L8 tensor(1377.6981, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.194222 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.231785 [1] L1 tensor(91626.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.237861 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.243403 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.248987 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.254615 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.260405 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.266891 [1] L7 tensor(7314.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.272593 [1] L8 tensor(1391.4271, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.278756 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.301809 [1] L1 tensor(91620.2812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.305755 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.309010 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.311787 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.314279 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.318062 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.320631 [1] L7 tensor(7314.2070, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.323193 [1] L8 tensor(1405.6222, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.326130 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.350338 [1] L1 tensor(91615.0547, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.354830 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.359636 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.363972 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.368682 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.372034 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.374879 [1] L7 tensor(7313.7842, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.378086 [1] L8 tensor(1418.0425, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.380787 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.408497 [1] L1 tensor(91611.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.412477 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.417219 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.421724 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.427271 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.431601 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.435938 [1] L7 tensor(7313.3398, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.440198 [1] L8 tensor(1429.7803, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.444447 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.468777 [1] L1 tensor(91607.5391, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.473079 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.476110 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.478720 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.481342 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.483974 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.486551 [1] L7 tensor(7312.8184, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.489129 [1] L8 tensor(1440.7744, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.492196 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.512293 [1] L1 tensor(91604.3438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.515760 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.518771 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.522096 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.525328 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.528476 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.533145 [1] L7 tensor(7312.3081, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.537079 [1] L8 tensor(1452.0017, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.541622 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.564431 [1] L1 tensor(91601.4688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.569042 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.573811 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.578621 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.584608 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.590093 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.594601 [1] L7 tensor(7311.5332, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.597019 [1] L8 tensor(1463.4434, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.604224 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.629055 [1] L1 tensor(91598.8984, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.634887 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.640684 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.646140 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.651527 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.654314 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.658059 [1] L7 tensor(7310.7427, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.660703 [1] L8 tensor(1475.3798, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.664586 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.689298 [1] L1 tensor(91596.6016, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.694654 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.699044 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.703696 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.708153 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.712729 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.717961 [1] L7 tensor(7310.3271, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.721007 [1] L8 tensor(1487.6847, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.724113 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.753712 [1] L1 tensor(91594.5391, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.759136 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.763614 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.765894 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.768290 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.770736 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.773118 [1] L7 tensor(7309.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.776580 [1] L8 tensor(1500.0754, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.781223 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.802312 [1] L1 tensor(91592.7031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.810918 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.815832 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.819856 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.824426 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.826621 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.828820 [1] L7 tensor(7309.5820, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.831162 [1] L8 tensor(1513.7412, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.833714 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.859098 [1] L1 tensor(91591.0859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.864873 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.867820 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.869988 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.873669 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.879058 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.881293 [1] L7 tensor(7309.2988, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.883518 [1] L8 tensor(1527.6387, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.885959 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.909783 [1] L1 tensor(91589.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.915768 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.919858 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.922509 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.924629 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.926794 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.928981 [1] L7 tensor(7308.9502, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.931336 [1] L8 tensor(1541.2878, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.933840 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.958390 [1] L1 tensor(91586.4062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.962771 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.965038 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.967601 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.970100 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.972401 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.974723 [1] L7 tensor(7308.2002, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.977020 [1] L8 tensor(1554.8827, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:02.979412 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.997601 [1] L1 tensor(91583.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.002760 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.004875 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.007003 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.009305 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.011668 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.014003 [1] L7 tensor(7307.4434, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.016233 [1] L8 tensor(1566.3264, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.018600 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.042019 [1] L1 tensor(91581.1328, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.048457 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.051691 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.053871 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.056158 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.058417 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.060777 [1] L7 tensor(7306.6514, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.063105 [1] L8 tensor(1578.8008, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.065458 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.089031 [1] L1 tensor(91578.8750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.094585 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.098628 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.100935 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.103143 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.105309 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.107638 [1] L7 tensor(7305.5166, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.109932 [1] L8 tensor(1590.5959, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.112318 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.132984 [1] L1 tensor(91576.8438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.138788 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.143469 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.147710 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.149943 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.152132 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.154418 [1] L7 tensor(7304.7495, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.156760 [1] L8 tensor(1602.1790, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.159165 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.181945 [1] L1 tensor(91575.0938, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.187385 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.191615 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.194015 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.196144 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.198455 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.200716 [1] L7 tensor(7304.0596, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.202994 [1] L8 tensor(1612.7454, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.205388 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.235459 [1] L1 tensor(91573.5469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.241885 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.244205 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.246630 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.248893 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.251148 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.253421 [1] L7 tensor(7303.4873, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.256003 [1] L8 tensor(1625.0591, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.258387 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.279366 [1] L1 tensor(91572.4141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.284542 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.287964 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.290977 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.293092 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.295254 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.297398 [1] L7 tensor(7303.1309, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.299671 [1] L8 tensor(1637.4131, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.302141 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.327023 [1] L1 tensor(91569.6953, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.333554 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.335986 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.338266 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.340528 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.342766 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.345063 [1] L7 tensor(7303.3247, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.347380 [1] L8 tensor(1649.8020, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.349789 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.367837 [1] L1 tensor(91567.2656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.374276 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.376414 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.378738 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.380990 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.383285 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.385563 [1] L7 tensor(7303.9424, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.387859 [1] L8 tensor(1662.8516, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.390249 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.408096 [1] L1 tensor(91565.0859, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.414167 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.416443 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.418753 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.421110 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.423354 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.425765 [1] L7 tensor(7304.7437, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.427990 [1] L8 tensor(1675.3856, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.430383 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.453884 [1] L1 tensor(91563.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.457026 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.459419 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.461817 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.464211 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.466606 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.468899 [1] L7 tensor(7305.5039, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.471165 [1] L8 tensor(1688.0088, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.473526 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.496440 [1] L1 tensor(91561.4062, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.501751 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.504876 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.507702 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.510176 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.512306 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.514458 [1] L7 tensor(7305.4678, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.516600 [1] L8 tensor(1699.0746, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.518971 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.542040 [1] L1 tensor(91559.8438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.547724 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.551269 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.553457 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.557246 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.562917 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.567297 [1] L7 tensor(7305.4199, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.569517 [1] L8 tensor(1709.9758, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.571925 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.595489 [1] L1 tensor(91558.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.601953 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.604252 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.606925 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.609044 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.613457 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.618383 [1] L7 tensor(7305.3774, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.622887 [1] L8 tensor(1720.4250, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.625492 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.648736 [1] L1 tensor(91557.2969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.654369 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.656865 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.660148 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.662435 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.664745 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.667145 [1] L7 tensor(7305.5938, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.669446 [1] L8 tensor(1731.5405, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.671872 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.694696 [1] L1 tensor(91556.2188, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.700627 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.704206 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.706877 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.708994 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.711144 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.713342 [1] L7 tensor(7305.9639, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.715684 [1] L8 tensor(1741.6255, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.718145 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.740995 [1] L1 tensor(91555.2812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.747156 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.750995 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.753577 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.755908 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.758033 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.760212 [1] L7 tensor(7305.7988, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.762561 [1] L8 tensor(1751.8569, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.765005 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.787287 [1] L1 tensor(91554.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.792985 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.795780 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.798483 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.800808 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.803188 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.805575 [1] L7 tensor(7305.7085, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.807965 [1] L8 tensor(1761.5076, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.810414 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.828479 [1] L1 tensor(91553.7969, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.834014 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.836438 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.838666 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.840871 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.843280 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.845604 [1] L7 tensor(7305.6274, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.847994 [1] L8 tensor(1771.3596, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.850548 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.871198 [1] L1 tensor(91558.5312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.876410 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.879598 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.882506 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.884627 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.887049 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.889402 [1] L7 tensor(7305.7729, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.891739 [1] L8 tensor(1778.0099, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.894153 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.917883 [1] L1 tensor(91562.8281, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.923596 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.927079 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.929664 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.932150 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.934536 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.936834 [1] L7 tensor(7305.7598, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.939150 [1] L8 tensor(1782.4081, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.941608 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.966027 [1] L1 tensor(91572.8438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.972249 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.975409 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.979268 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.981452 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.983631 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.985818 [1] L7 tensor(7305.9077, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.988005 [1] L8 tensor(1790.0247, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:03.990472 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.014508 [1] L1 tensor(91581.9688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.020367 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.024041 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.026506 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.028632 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.030892 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.033176 [1] L7 tensor(7305.8701, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.035576 [1] L8 tensor(1799.4231, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.038012 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.061846 [1] L1 tensor(91590.2578, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.068287 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.071421 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.073576 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.075801 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.078129 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.080460 [1] L7 tensor(7305.6919, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.082822 [1] L8 tensor(1807.3804, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.085226 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.102311 [1] L1 tensor(91597.8125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.108949 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.112131 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.114507 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.116779 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.119049 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.121352 [1] L7 tensor(7305.4805, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.123704 [1] L8 tensor(1815.8324, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.126156 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.148325 [1] L1 tensor(91604.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.154016 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.156708 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.158859 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.161200 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.163597 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.165926 [1] L7 tensor(7305.0278, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.168215 [1] L8 tensor(1826.3904, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.170641 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.187551 [1] L1 tensor(91600.9688, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.192970 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.198421 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.202980 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.208284 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.211703 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.213881 [1] L7 tensor(7305.0186, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.216211 [1] L8 tensor(1836.8647, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.218607 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.238629 [1] L1 tensor(91597.6875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.243377 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.246373 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.249599 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.252167 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.254464 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.257025 [1] L7 tensor(7305.2495, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.259191 [1] L8 tensor(1847.7650, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.261567 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.282224 [1] L1 tensor(91594.6719, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.288080 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.291778 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.294677 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.297016 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.299319 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.301657 [1] L7 tensor(7305.3184, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.303966 [1] L8 tensor(1859.8657, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.306397 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.323388 [1] L1 tensor(91600.2266, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.328434 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.331580 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.334948 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.337115 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.339338 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.341601 [1] L7 tensor(7305.7202, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.343966 [1] L8 tensor(1872.3767, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.346338 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.362405 [1] L1 tensor(91605.2578, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.367211 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.370385 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.373892 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.376335 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.378558 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.380823 [1] L7 tensor(7305.9404, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.383121 [1] L8 tensor(1884.2997, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.385539 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.402541 [1] L1 tensor(91609.7891, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.408618 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.412140 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.414855 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.417025 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.419261 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.421532 [1] L7 tensor(7306.0884, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.423859 [1] L8 tensor(1895.4619, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.426304 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.442809 [1] L1 tensor(91613.8359, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.449392 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.452790 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.455889 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.458184 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.460339 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.462649 [1] L7 tensor(7306.2393, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.465014 [1] L8 tensor(1905.0767, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.467441 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.490539 [1] L1 tensor(91617.4922, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.495872 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.498818 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.502095 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.504561 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.506698 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.509000 [1] L7 tensor(7306.7227, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.511334 [1] L8 tensor(1914.0349, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.513724 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.531458 [1] L1 tensor(91622.1250, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.536355 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.540074 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.543570 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.545741 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.547978 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.550306 [1] L7 tensor(7307.0469, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.552629 [1] L8 tensor(1921.9152, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.555025 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.574694 [1] L1 tensor(91626.3203, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.580451 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.583465 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.586011 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.588509 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.590839 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.593189 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.595556 [1] L8 tensor(1929.7485, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.598001 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.621758 [1] L1 tensor(91630.1016, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.628145 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.631458 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.633617 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.635748 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.638005 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.640336 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.642650 [1] L8 tensor(1938.5867, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.645084 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.663594 [1] L1 tensor(91630.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.669302 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.671807 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.673972 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.676246 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.678584 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.680920 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.683234 [1] L8 tensor(1947.7693, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.685623 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.704054 [1] L1 tensor(91630.8203, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.709902 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.712212 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.714365 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.716893 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.719445 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.721752 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.724047 [1] L8 tensor(1954.5996, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.726484 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.744214 [1] L1 tensor(91631.0547, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.750248 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.752547 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.754908 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.757204 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.759590 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.761932 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.764230 [1] L8 tensor(1961.9805, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.766654 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.784874 [1] L1 tensor(91631.1875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.790377 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.792496 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.794631 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.796880 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.799213 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.801557 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.803880 [1] L8 tensor(1966.5870, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.806276 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.823876 [1] L1 tensor(91631.2031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.830095 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.832435 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.834796 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.837116 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.839440 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.841833 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.847262 [1] L8 tensor(1970.8381, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.849773 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.869176 [1] L1 tensor(91626.4219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.874517 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.876661 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.878805 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.881215 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.883591 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.885785 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.887975 [1] L8 tensor(1978.1454, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.890409 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.914604 [1] L1 tensor(91621.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.920934 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.924300 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.926949 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.929069 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.931322 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.933646 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.935981 [1] L8 tensor(1984.0890, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.938453 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.961817 [1] L1 tensor(91617.7578, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.966847 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.969326 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.972112 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.974861 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.977042 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.979333 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.981674 [1] L8 tensor(1990.6523, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:04.984059 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.002108 [1] L1 tensor(91613.7344, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.008139 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.011617 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.013865 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.016139 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.018648 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.021083 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.023612 [1] L8 tensor(1995.3953, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.026238 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.044550 [1] L1 tensor(91609.9141, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.050126 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.052990 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.056170 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.058385 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.060619 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.063100 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.065434 [1] L8 tensor(2000.4619, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.067911 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.087056 [1] L1 tensor(91606.2734, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.091177 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.095553 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.098106 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.100551 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.102864 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.105233 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.107575 [1] L8 tensor(2007.9070, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.110057 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.129924 [1] L1 tensor(91602.7812, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.135495 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.139320 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.141463 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.143892 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.146174 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.148507 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.150876 [1] L8 tensor(2017.1493, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.153294 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.177537 [1] L1 tensor(91591.8125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.182574 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.186935 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.189318 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.191644 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.193994 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.196303 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.198663 [1] L8 tensor(2026.3612, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.201115 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.222938 [1] L1 tensor(91581.6953, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.229404 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.232196 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.234552 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.236852 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.239192 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.241529 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.243899 [1] L8 tensor(2036.9098, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.246294 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.264535 [1] L1 tensor(91572.3750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.270105 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.272510 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.274826 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.277119 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.279476 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.281827 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.284106 [1] L8 tensor(2045.9766, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.286515 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.303940 [1] L1 tensor(91563.7656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.310267 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.312685 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.315141 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.317563 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.319988 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.322466 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.324897 [1] L8 tensor(2055.1167, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.327406 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.347527 [1] L1 tensor(91555.8047, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.353800 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.359072 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.361274 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.363684 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.366695 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.369969 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.372721 [1] L8 tensor(2064.7595, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.375411 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.398468 [1] L1 tensor(91548.4375, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.405080 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.407913 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.410309 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.412639 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.414947 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.417288 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.419608 [1] L8 tensor(2074.9424, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.422047 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.444766 [1] L1 tensor(91536.5312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.451446 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.455004 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.457160 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.459296 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.461530 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.463974 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.466227 [1] L8 tensor(2086.4458, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.468631 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.490464 [1] L1 tensor(91525.6406, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.495991 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.499821 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.502430 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.504549 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.506707 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.508835 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.511172 [1] L8 tensor(2098.4275, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.513560 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.531718 [1] L1 tensor(91515.6562, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.536643 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.539995 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.542285 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.544608 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.546920 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.549264 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.551602 [1] L8 tensor(2107.1714, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.554087 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.572062 [1] L1 tensor(91506.5000, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.577612 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.581630 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.584021 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.586224 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.588522 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.591040 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.593499 [1] L8 tensor(2116.1304, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.596115 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.613196 [1] L1 tensor(91498.1172, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.617279 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.620294 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.622918 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.625023 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.627406 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.629689 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.631879 [1] L8 tensor(2126.3237, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.634396 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.651566 [1] L1 tensor(91488.9219, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.657915 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.660796 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.663358 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.666036 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.668245 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.670681 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.673133 [1] L8 tensor(2136.7031, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.675722 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.694133 [1] L1 tensor(91479.0469, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.699276 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.703622 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.706179 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.708545 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.710831 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.713158 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.715466 [1] L8 tensor(2145.9819, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.717916 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.741847 [1] L1 tensor(91470.0312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.747420 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.751570 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.753920 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.756186 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.758544 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.760864 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.764292 [1] L8 tensor(2156.6565, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.766834 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.784897 [1] L1 tensor(91460.4531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.790520 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.792646 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.794997 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.797303 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.799614 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.801948 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.804207 [1] L8 tensor(2167.7449, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.806623 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.824388 [1] L1 tensor(91451.7031, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.830529 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.832839 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.835400 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.837820 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.840221 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.842686 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.845137 [1] L8 tensor(2176.8025, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.847669 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.870778 [1] L1 tensor(91440.7656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.876948 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.879887 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.882246 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.884556 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.886877 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.889213 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.891569 [1] L8 tensor(2188.1221, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.894029 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.917602 [1] L1 tensor(91429.5625, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.922472 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.927014 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.929382 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.931721 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.934107 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.936605 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.938936 [1] L8 tensor(2197.4336, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.941377 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.964767 [1] L1 tensor(91419.4531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.970083 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.973739 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.976478 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.978851 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.980964 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.983185 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.985504 [1] L8 tensor(2208.6543, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:05.987909 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.010358 [1] L1 tensor(91414.3438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.015617 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.018105 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.020600 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.023118 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.025307 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.027597 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.029932 [1] L8 tensor(2220.6272, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.032306 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.050989 [1] L1 tensor(91411.1875, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.056671 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.059537 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.061959 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.064447 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.066771 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.069033 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.071387 [1] L8 tensor(2233.1575, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.073778 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.097520 [1] L1 tensor(91408.3125, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.102695 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.106992 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.109288 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.111589 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.113924 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.116259 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.118608 [1] L8 tensor(2246.8120, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.121008 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.138770 [1] L1 tensor(91405.7500, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.144042 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.146904 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.149979 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.152340 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.154593 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.156857 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.159219 [1] L8 tensor(2259.6453, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.161699 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.185468 [1] L1 tensor(91403.9531, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.190555 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.194803 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.197129 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.199451 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.201833 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.204127 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.206435 [1] L8 tensor(2271.0356, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.208832 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.230536 [1] L1 tensor(91402.3438, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.236634 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.239641 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.241818 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.244044 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.246384 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.248755 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.251061 [1] L8 tensor(2285.2490, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.253443 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.272032 [1] L1 tensor(91400.8906, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.277849 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.280186 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.282307 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.284410 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.286752 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.289104 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.291448 [1] L8 tensor(2299.2354, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.293835 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.312004 [1] L1 tensor(91399.5781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.317822 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.320240 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.322459 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.324719 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.327160 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.329672 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.332143 [1] L8 tensor(2312.9934, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.334693 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.358439 [1] L1 tensor(91398.3984, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.364127 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.367489 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.369794 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.372111 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.374444 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.376770 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.379149 [1] L8 tensor(2323.4746, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.381543 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.399766 [1] L1 tensor(91397.3359, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.405549 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.407993 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.410545 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.412851 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.415067 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.417380 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.419700 [1] L8 tensor(2333.1545, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.422233 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.440372 [1] L1 tensor(91396.3750, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.446129 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.448313 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.451723 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.454221 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.456594 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.458878 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.461203 [1] L8 tensor(2343.0676, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.463682 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.486146 [1] L1 tensor(91395.5312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.491286 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.493659 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.496125 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.498621 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.500832 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.503099 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.505544 [1] L8 tensor(2350.1367, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.507907 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.526252 [1] L1 tensor(91394.7656, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.532177 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.535426 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.538090 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.540566 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.543034 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.545490 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.548008 [1] L8 tensor(2357.3701, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.550712 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.569500 [1] L1 tensor(91394.0781, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.575134 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.578964 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.581109 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.583422 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.585762 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.588110 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.590427 [1] L8 tensor(2365.2251, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.592829 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.611695 [1] L1 tensor(91393.4453, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.617355 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.620043 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.622751 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.624994 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.627354 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.629690 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.631978 [1] L8 tensor(2376.1802, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.634413 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.657610 [1] L1 tensor(91392.8906, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.664211 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.667924 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.670592 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.672701 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.674873 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.677699 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.680726 [1] L8 tensor(2386.9541, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.683885 [1] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.702532 [1] L1 tensor(91392.0312, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.708250 [1] L2 tensor(8343.7402, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.711576 [1] L3 tensor(8466.9336, device='cuda:1', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.713837 [1] L4 tensor(8180.1406, device='cuda:1', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.716270 [1] L5 tensor(8129.6323, device='cuda:1', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.718551 [1] L6 tensor(8153.8779, device='cuda:1', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.720887 [1] L7 tensor(7307.2773, device='cuda:1', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.723200 [1] L8 tensor(2398.0083, device='cuda:1', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:1', grad_fn=<SumBackward0>)
20:34:06.725582 [1] Warning: no training nodes in this partition! Backward fake loss.
21:06:44.202802 [1] proc begin: <DistEnv 1/4 nccl>
21:06:55.243258 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
21:06:55.289145 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:07:47.754283 [1] proc begin: <DistEnv 1/4 nccl>
21:07:54.027928 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
21:07:54.044844 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:09:08.023515 [1] proc begin: <DistEnv 1/4 nccl>
21:09:13.134639 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
21:09:13.150730 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:12:17.222696 [1] proc begin: <DistEnv 1/4 nccl>
21:12:22.109520 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
21:12:22.137579 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:13:53.409099 [1] proc begin: <DistEnv 1/4 nccl>
21:13:58.865395 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
21:13:58.888394 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:14:50.897123 [1] proc begin: <DistEnv 1/4 nccl>
21:14:56.753373 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
21:14:56.777491 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:19:51.960553 [1] proc begin: <DistEnv 1/4 nccl>
21:19:56.867139 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
21:19:56.887466 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:20:43.731827 [1] proc begin: <DistEnv 1/4 nccl>
09:20:49.799682 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:20:49.816987 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:22:20.436584 [1] proc begin: <DistEnv 1/4 nccl>
09:22:43.045407 [1] proc begin: <DistEnv 1/4 nccl>
09:22:47.717092 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:22:47.736871 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:25:10.619119 [1] proc begin: <DistEnv 1/4 nccl>
09:25:16.893336 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:25:16.910146 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:28:59.948643 [1] proc begin: <DistEnv 1/4 nccl>
09:29:05.514419 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
09:29:05.540495 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:30:21.040700 [1] proc begin: <DistEnv 1/4 nccl>
09:30:36.471205 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
09:30:36.479932 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:32:08.725412 [1] proc begin: <DistEnv 1/4 nccl>
09:32:13.854702 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
09:32:13.864009 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:32:20.435552 [1] Warning: no training nodes in this partition! Backward fake loss.
09:33:37.476936 [1] proc begin: <DistEnv 1/4 nccl>
09:33:41.078132 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
09:33:41.087129 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:33:46.626273 [1] Warning: no training nodes in this partition! Backward fake loss.
09:35:51.117818 [1] proc begin: <DistEnv 1/4 nccl>
09:35:55.619774 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
09:35:55.633216 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:36:00.371493 [1] Warning: no training nodes in this partition! Backward fake loss.
10:10:18.041429 [1] proc begin: <DistEnv 1/4 nccl>
10:10:22.361444 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
10:10:22.371446 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:10:27.759785 [1] Warning: no training nodes in this partition! Backward fake loss.
10:12:45.447943 [1] proc begin: <DistEnv 1/4 nccl>
10:12:51.784030 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
10:12:51.803264 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:15:16.614220 [1] proc begin: <DistEnv 1/4 nccl>
10:15:21.254632 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
10:15:21.274865 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:16:49.708699 [1] proc begin: <DistEnv 1/4 nccl>
10:16:53.993397 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
10:16:54.014861 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:17:39.868273 [1] proc begin: <DistEnv 1/4 nccl>
10:17:45.700041 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
10:17:45.721147 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:19:07.309036 [1] proc begin: <DistEnv 1/4 nccl>
10:19:13.600886 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
10:19:13.618150 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:20:06.028942 [1] proc begin: <DistEnv 1/4 nccl>
10:20:10.312265 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
10:20:10.321297 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:20:34.444384 [1] proc begin: <DistEnv 1/4 nccl>
10:20:39.391212 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
10:20:39.404310 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:20:44.594793 [1] Warning: no training nodes in this partition! Backward fake loss.
16:21:09.458346 [1] proc begin: <DistEnv 1/4 nccl>
16:21:24.931795 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:21:24.957659 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:28:45.011679 [1] proc begin: <DistEnv 1/4 nccl>
16:28:51.906631 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:28:51.927967 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:29:18.914995 [1] proc begin: <DistEnv 1/4 nccl>
16:29:25.322258 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:29:25.343316 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:30:57.444612 [1] proc begin: <DistEnv 1/4 nccl>
16:31:04.044469 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:31:04.065283 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:32:02.886700 [1] proc begin: <DistEnv 1/4 nccl>
16:32:09.701639 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:32:09.722458 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:32:48.921743 [1] proc begin: <DistEnv 1/4 nccl>
16:32:54.348670 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:32:54.369386 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:35:16.292528 [1] proc begin: <DistEnv 1/4 nccl>
16:35:22.763740 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:35:22.784084 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:38:07.602926 [1] proc begin: <DistEnv 1/4 nccl>
16:38:12.773653 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:38:12.794001 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:56:16.281478 [1] proc begin: <DistEnv 1/4 nccl>
16:56:22.205039 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:56:22.226410 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:03:05.506726 [1] proc begin: <DistEnv 1/4 nccl>
17:03:11.926521 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
17:03:11.947954 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:03:31.507461 [1] proc begin: <DistEnv 1/4 nccl>
17:03:37.533727 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
17:03:37.560415 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:06:08.747187 [1] proc begin: <DistEnv 1/4 nccl>
17:06:14.238494 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
17:06:14.268474 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

18:34:53.532317 [1] proc begin: <DistEnv 1/4 nccl>
18:41:35.578991 [1] proc begin: <DistEnv 1/4 nccl>
18:44:17.523583 [1] proc begin: <DistEnv 1/4 nccl>
18:46:18.903787 [1] proc begin: <DistEnv 1/4 nccl>
18:46:30.688042 [1] proc begin: <DistEnv 1/4 nccl>
18:49:17.962502 [1] proc begin: <DistEnv 1/4 nccl>
18:55:48.593110 [1] proc begin: <DistEnv 1/4 nccl>
18:55:58.080968 [1] proc begin: <DistEnv 1/4 nccl>
19:38:27.998648 [1] proc begin: <DistEnv 1/4 nccl>
19:38:44.961692 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
19:38:44.981694 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:16:19.572891 [1] proc begin: <DistEnv 1/4 nccl>
20:16:25.193718 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
20:16:25.214675 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:27:35.476267 [1] proc begin: <DistEnv 1/4 nccl>
20:27:46.605996 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
20:27:46.612805 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:27:59.773269 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:02.629538 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:04.474046 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:06.304686 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:08.132463 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:09.962120 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:11.792517 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:13.616411 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:15.438307 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:17.261774 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:19.085576 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:20.907335 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:22.732410 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:24.552796 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:26.376339 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:28.199687 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:30.026223 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:31.850175 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:33.674394 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:35.498891 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:37.328439 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:39.157221 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:40.979278 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:42.801359 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:44.626073 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:46.449990 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:48.272873 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:50.095001 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:51.916678 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:53.740140 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:55.562779 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:57.385914 [1] Warning: no training nodes in this partition! Backward fake loss.
20:28:59.210431 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:01.032982 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:02.918665 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:04.741885 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:06.563802 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:08.384870 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:10.207280 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:12.028925 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:13.852002 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:15.673046 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:17.495510 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:19.318433 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:21.139374 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:22.961565 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:24.783071 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:26.604940 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:28.427086 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:30.250621 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:32.073686 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:33.711931 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:35.536451 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:37.173699 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:38.996443 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:40.635005 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:42.457655 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:44.095638 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:45.919160 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:47.557373 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:49.380110 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:51.018410 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:52.841089 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:54.479785 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:56.301998 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:57.940866 [1] Warning: no training nodes in this partition! Backward fake loss.
20:29:59.764052 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:01.399964 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:03.284897 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:04.925594 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:06.752060 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:08.394499 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:10.219434 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:11.858659 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:13.680957 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:15.320116 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:17.143665 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:18.782105 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:20.604959 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:22.243301 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:24.067672 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:25.705252 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:27.529351 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:29.168843 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:30.991599 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:32.630178 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:34.454214 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:36.093157 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:37.918082 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:39.556038 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:41.379509 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:43.018113 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:44.840376 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:46.478340 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:48.304373 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:49.942257 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:51.764705 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:53.402866 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:55.226507 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:56.866816 [1] Warning: no training nodes in this partition! Backward fake loss.
20:30:58.690905 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:00.329909 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:02.187568 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:03.854765 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:05.677615 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:07.318021 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:09.141785 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:10.781250 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:12.605231 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:14.244464 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:16.069607 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:17.709800 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:19.533015 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:21.170758 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:22.993682 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:24.631415 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:26.454548 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:28.092836 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:29.916780 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:31.554339 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:33.377554 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:35.014555 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:36.836874 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:38.474757 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:40.297034 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:41.935035 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:43.757343 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:45.400437 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:47.227290 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:48.865129 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:50.689864 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:52.328359 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:54.151907 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:55.789527 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.612706 [1] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.252869 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:01.075889 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:02.743181 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:04.591612 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:06.231060 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:08.056465 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:09.696162 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:11.520127 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:13.158592 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:14.982613 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:16.620133 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:18.441988 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:20.079164 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:21.903132 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:23.540453 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:25.362875 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:26.999687 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:28.822502 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:30.461335 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:32.284693 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:33.922574 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:35.745322 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:37.383439 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:39.206399 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:40.843607 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:42.666555 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:44.303566 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:46.126780 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:47.763565 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:49.586558 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:51.224307 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:53.048031 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:54.685686 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:56.507190 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:58.145472 [1] Warning: no training nodes in this partition! Backward fake loss.
20:32:59.969031 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:01.613180 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:03.490648 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:05.129975 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:06.953920 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:08.594451 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:10.419512 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:12.057489 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.880311 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.517740 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.340777 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.979187 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:20.802530 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:22.440240 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:24.263798 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:25.904254 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:27.728229 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:29.367568 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:31.190548 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:32.830237 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:34.653487 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:36.292556 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:38.117595 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:39.757261 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:41.580975 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:43.219783 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:45.043848 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:46.681374 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:48.505424 [1] Warning: no training nodes in this partition! Backward fake loss.
20:33:50.143405 [1] Warning: no training nodes in this partition! Backward fake loss.
20:39:45.321416 [1] proc begin: <DistEnv 1/4 nccl>
20:39:51.152037 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
20:39:51.172568 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:46:10.164843 [1] proc begin: <DistEnv 1/4 nccl>
20:46:16.259495 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
20:46:16.282899 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:47:48.597518 [1] proc begin: <DistEnv 1/4 nccl>
20:47:54.808378 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
20:47:54.829223 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:27:35.450757 [1] proc begin: <DistEnv 1/4 nccl>
21:27:40.166939 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
21:27:40.188223 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:28:46.394407 [1] proc begin: <DistEnv 1/4 nccl>
21:28:51.838674 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
21:28:51.859102 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:30:36.939418 [1] proc begin: <DistEnv 1/4 nccl>
21:30:42.939077 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
21:30:42.974396 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:32:23.940180 [1] proc begin: <DistEnv 1/4 nccl>
21:32:42.739029 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
21:32:42.759361 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:49:23.795752 [1] proc begin: <DistEnv 1/4 nccl>
21:49:29.330143 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
21:49:29.354548 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:00:42.222984 [1] proc begin: <DistEnv 1/4 nccl>
22:01:00.552196 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
22:01:00.560569 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:01:08.109438 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:10.962153 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:12.801745 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:14.634230 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:16.465511 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:18.299533 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:20.134755 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:21.966788 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:23.799956 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:25.631010 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:27.459349 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:29.284651 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:31.107114 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:32.931655 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:34.756579 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:36.581740 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:38.409591 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:40.235416 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:42.062766 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:43.891266 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:45.717617 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:47.544350 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:49.368962 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:51.195064 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:53.024923 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:54.852182 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:56.679300 [1] Warning: no training nodes in this partition! Backward fake loss.
22:01:58.509188 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:00.337427 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:02.176751 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:04.058131 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:05.891515 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:07.726566 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:09.560243 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:11.395238 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:13.228514 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:15.056907 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:16.883851 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:18.712490 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:20.548086 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:22.378563 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:24.203994 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:26.029745 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:27.855047 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:29.682135 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:31.508045 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:33.330740 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:35.158141 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:36.985693 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:38.815336 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:40.644101 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:42.287413 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:44.113320 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:45.755096 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:47.582423 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:49.222884 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:51.050218 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:52.688378 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:54.511377 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:56.149592 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:57.972713 [1] Warning: no training nodes in this partition! Backward fake loss.
22:02:59.613717 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:01.439234 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:03.139444 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:04.967130 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:06.610720 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:08.441022 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:10.085139 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:11.913474 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:13.557075 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:15.384533 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:17.026808 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:18.853899 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:20.495335 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:22.322551 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:23.962994 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:25.789231 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:27.427661 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:29.251112 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:30.892913 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:32.719555 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:34.360689 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:36.185750 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:37.826944 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:39.649514 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:41.288155 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:43.110697 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:44.748666 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:46.572200 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:48.209985 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:50.033568 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:51.672315 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:53.495709 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:55.134507 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:56.961299 [1] Warning: no training nodes in this partition! Backward fake loss.
22:03:58.602829 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:00.428818 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:02.103234 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:03.951610 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:05.594558 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:07.424894 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:09.067985 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:10.900068 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:12.540691 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:14.366553 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:16.006407 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:17.835120 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:19.475922 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:21.303468 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:22.945205 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:24.773549 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:26.417635 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:28.246088 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:29.888327 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:31.718898 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:33.363326 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:35.192186 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:36.831888 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:38.656726 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:40.293697 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:42.118531 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:43.762130 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:45.588155 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:47.228618 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:49.057035 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:50.696378 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:52.521897 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:54.161480 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:55.987235 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:57.630904 [1] Warning: no training nodes in this partition! Backward fake loss.
22:04:59.459222 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:01.102315 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:02.979708 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:04.631050 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:06.457648 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:08.100353 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:09.925532 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:11.565835 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:13.392994 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:15.033531 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:16.859843 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:18.498820 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:20.323262 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:21.963957 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:23.788487 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:25.427214 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:27.250915 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:28.889139 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:30.712406 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:32.354669 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:34.181151 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:35.825163 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:37.651780 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:39.293662 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:41.120234 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:42.762224 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:44.588817 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:46.229190 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:48.056164 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:49.699601 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:51.525196 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:53.167299 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:54.993892 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:56.635720 [1] Warning: no training nodes in this partition! Backward fake loss.
22:05:58.465960 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:00.109513 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:01.967127 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:03.641445 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:05.471110 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:07.118084 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:08.947790 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:10.593230 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:12.423907 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:14.067800 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:15.892248 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:17.532620 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:19.358705 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:20.997596 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:22.822180 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:24.464646 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:26.291837 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:27.931281 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:29.755836 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:31.394940 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:33.220380 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:34.861303 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:36.687821 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:38.329461 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:40.156180 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:41.797300 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:43.625343 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:45.267063 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:47.094829 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:48.736276 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:50.563128 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:52.204342 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:54.031443 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:55.671416 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:57.498432 [1] Warning: no training nodes in this partition! Backward fake loss.
22:06:59.139188 [1] Warning: no training nodes in this partition! Backward fake loss.
22:28:58.907349 [1] proc begin: <DistEnv 1/4 nccl>
22:29:04.921473 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
22:29:04.930661 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:08:23.095189 [1] proc begin: <DistEnv 1/4 nccl>
14:08:27.586087 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:08:27.595045 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:08:32.469268 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:34.075117 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:34.599341 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:35.122110 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:35.646223 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:36.168688 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:36.690413 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:37.212798 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:37.736579 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:38.261710 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:38.783734 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:39.307216 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:39.830965 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:40.354913 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:40.877055 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:41.399766 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:41.921612 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:42.444594 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:42.969350 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:43.492028 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:44.014272 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:44.535968 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:45.058788 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:45.580040 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:46.101647 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:46.624210 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:47.145539 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:47.667641 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:48.190616 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:48.713278 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:49.237606 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:49.760720 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:50.285441 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:50.812250 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:51.337763 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:51.864234 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:52.390025 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:52.915228 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:53.440712 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:53.966711 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:54.492900 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:55.018677 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:55.544069 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:56.069074 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:56.594434 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:57.121095 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:57.646647 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:58.172952 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:58.699029 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:59.223640 [1] Warning: no training nodes in this partition! Backward fake loss.
14:08:59.748531 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:00.276096 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:00.802198 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:01.331443 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:01.876574 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:02.421968 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:02.955320 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:03.480228 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:04.005947 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:04.531902 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:05.058529 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:05.584162 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:06.109472 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:06.633809 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:07.158767 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:07.683602 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:08.207688 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:08.734888 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:09.262371 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:09.788815 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:10.316185 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:10.843692 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:11.371370 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:11.896705 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:12.423383 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:12.948185 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:13.472644 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:13.997019 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:14.522909 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:15.047839 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:15.574130 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:16.099052 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:16.623413 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:17.147337 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:17.670522 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:18.194848 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:18.718710 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:19.243533 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:19.767421 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:20.290806 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:20.812762 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:21.340063 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:21.867428 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:22.393484 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:22.921141 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:23.447699 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:23.972444 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:24.498885 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:25.025610 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:25.550639 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:26.076293 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:26.600663 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:27.126556 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:27.650606 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:28.175270 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:28.699447 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:29.226537 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:29.750965 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:30.277187 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:30.803689 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:31.330388 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:31.857126 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:32.381272 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:32.906548 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:33.432959 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:33.956666 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:34.482061 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:35.007013 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:35.534265 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:36.061938 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:36.588049 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:37.114886 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:37.641314 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:38.166727 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:38.693512 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:39.219869 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:39.745761 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:40.271835 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:40.798608 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:41.324123 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:41.849650 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:42.375758 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:42.903392 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:43.430909 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:43.956566 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:44.483125 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:45.009727 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:45.537324 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:46.064144 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:46.593229 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:47.120811 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:47.647191 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:48.171434 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:48.695853 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:49.219657 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:49.743320 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:50.267743 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:50.792899 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:51.317154 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:51.842952 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:52.369318 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:52.895819 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:53.421110 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:53.948387 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:54.476693 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:55.003849 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:55.530065 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:56.055619 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:56.580696 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:57.107020 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:57.633122 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:58.159265 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:58.684992 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:59.211228 [1] Warning: no training nodes in this partition! Backward fake loss.
14:09:59.735822 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:00.261773 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:00.788176 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:01.314326 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:01.842310 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:02.389190 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:02.935399 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:03.471823 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:03.997129 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:04.523216 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:05.047911 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:05.572839 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:06.098852 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:06.625241 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:07.151249 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:07.676556 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:08.202125 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:08.728058 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:09.252949 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:09.777107 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:10.301877 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:10.827714 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:11.353116 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:11.876592 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:12.402410 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:12.928720 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:13.453339 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:13.977226 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:14.503043 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:15.028698 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:15.555245 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:16.082570 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:16.610379 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:17.137945 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:17.665746 [1] Warning: no training nodes in this partition! Backward fake loss.
14:10:18.192467 [1] Warning: no training nodes in this partition! Backward fake loss.
14:39:40.616331 [1] proc begin: <DistEnv 1/4 nccl>
14:39:45.061452 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
14:39:45.072867 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:39:51.863517 [1] Warning: no training nodes in this partition! Backward fake loss.
14:39:53.928459 [1] Warning: no training nodes in this partition! Backward fake loss.
14:39:54.936142 [1] Warning: no training nodes in this partition! Backward fake loss.
14:39:55.946348 [1] Warning: no training nodes in this partition! Backward fake loss.
14:39:56.955139 [1] Warning: no training nodes in this partition! Backward fake loss.
14:39:57.959995 [1] Warning: no training nodes in this partition! Backward fake loss.
14:39:58.971116 [1] Warning: no training nodes in this partition! Backward fake loss.
14:39:59.975038 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:00.979900 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:02.008671 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:03.041322 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:04.052522 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:05.058482 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:06.065956 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:07.072596 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:08.077847 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:09.083592 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:10.090289 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:11.096119 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:12.104233 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:13.110167 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:14.114705 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:15.116759 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:16.121288 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:17.128004 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:18.130442 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:19.133607 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:20.134452 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:21.137153 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:22.139599 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:23.140076 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:24.140625 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:25.141331 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:26.143025 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:27.145336 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:28.147449 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:29.152498 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:30.156508 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:31.158269 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:32.161196 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:33.163642 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:34.166881 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:35.169407 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:36.173382 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:37.180645 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:38.185520 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:39.187992 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:40.191866 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:41.192764 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:42.194738 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:43.194708 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:44.196926 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:45.198605 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:46.200079 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:47.201928 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:48.201965 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:49.202662 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:50.204664 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:51.206598 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:52.206262 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:53.206277 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:54.203823 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:55.202792 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:56.201714 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:57.204065 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:58.201953 [1] Warning: no training nodes in this partition! Backward fake loss.
14:40:59.201922 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:00.199325 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:01.197360 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:02.208468 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:03.248389 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:04.260356 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:05.263571 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:06.264866 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:07.264824 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:08.263450 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:09.264098 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:10.264444 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:11.266439 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:12.269465 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:13.267980 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:14.266827 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:15.265088 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:16.264984 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:17.265513 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:18.266053 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:19.268422 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:20.266818 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:21.267281 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:22.267070 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:23.266755 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:24.265836 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:25.266061 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:26.266077 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:27.272644 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:28.280926 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:29.288479 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:30.290542 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:31.289874 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:32.289342 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:33.289216 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:34.289153 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:35.289914 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:36.288080 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:37.288665 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:38.289299 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:39.296010 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:40.299012 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:41.302669 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:42.306279 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:43.309857 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:44.313409 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:45.315823 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:46.317756 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:47.321036 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:48.324639 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:49.326577 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:50.327794 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:51.329304 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:52.331275 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:53.336265 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:54.339010 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:55.340981 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:56.344656 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:57.346998 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:58.346364 [1] Warning: no training nodes in this partition! Backward fake loss.
14:41:59.347105 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:00.347323 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:01.347365 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:02.382368 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:03.407738 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:04.417707 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:05.421144 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:06.424275 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:07.427828 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:08.434640 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:09.439684 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:10.445710 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:11.453449 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:12.465439 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:13.473726 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:14.477411 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:15.480279 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:16.480152 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:17.478022 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:18.476962 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:19.476104 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:20.476391 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:21.473794 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:22.473047 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:23.475421 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:24.477225 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:25.478259 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:26.480259 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:27.478987 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:28.477699 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:29.479555 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:30.476901 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:31.474725 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:32.474085 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:33.476022 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:34.477673 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:35.476966 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:36.477159 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:37.476771 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:38.484988 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:39.488432 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:40.490676 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:41.493404 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:42.494338 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:43.494605 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:44.498022 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:45.500655 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:46.504214 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:47.505979 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:48.506699 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:49.508835 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:50.509332 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:51.513964 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:52.516781 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:53.518793 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:54.519608 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:55.520371 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:56.518083 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:57.516670 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:58.515901 [1] Warning: no training nodes in this partition! Backward fake loss.
14:42:59.517136 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:00.515766 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:01.530268 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:02.570992 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:03.582086 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:04.587002 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:05.588232 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:06.590703 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:07.592071 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:08.593072 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:09.595716 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:10.598658 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:11.601916 [1] Warning: no training nodes in this partition! Backward fake loss.
14:43:12.602005 [1] Warning: no training nodes in this partition! Backward fake loss.
15:29:58.848006 [1] proc begin: <DistEnv 1/4 nccl>
15:30:18.728578 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
15:30:18.748115 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:30:55.451618 [1] proc begin: <DistEnv 1/4 nccl>
15:31:00.587019 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
15:31:00.621778 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:32:42.789918 [1] proc begin: <DistEnv 1/4 nccl>
15:32:48.369032 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
15:32:48.390141 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

18:25:25.006132 [1] proc begin: <DistEnv 1/4 nccl>
18:25:30.870138 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
18:25:30.913733 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

18:30:41.946742 [1] proc begin: <DistEnv 1/4 nccl>
18:30:47.585928 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
18:30:47.606015 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:00:40.802156 [1] proc begin: <DistEnv 1/4 nccl>
20:00:46.993725 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
20:00:47.014520 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:03:42.328716 [1] proc begin: <DistEnv 1/4 nccl>
20:03:47.132087 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
20:03:47.153220 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:05:10.022937 [1] proc begin: <DistEnv 1/4 nccl>
20:05:15.758974 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
20:05:15.779627 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:06:20.478224 [1] proc begin: <DistEnv 1/4 nccl>
20:06:25.404720 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
20:06:25.425327 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:34:01.663825 [1] proc begin: <DistEnv 1/4 nccl>
11:34:13.812050 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
11:34:13.881040 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:34:15.264659 [1] L1 tensor(77137.2891, device='cuda:1', grad_fn=<SumBackward0>) tensor(260.2319, device='cuda:1', grad_fn=<SumBackward0>)
15:51:57.317097 [1] proc begin: <DistEnv 1/4 nccl>
15:52:24.395821 [1] proc begin: <DistEnv 1/4 nccl>
15:52:29.615290 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
15:52:29.632843 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:02:34.803388 [1] proc begin: <DistEnv 1/4 nccl>
16:02:39.643659 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:02:39.663852 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:21:50.805773 [1] proc begin: <DistEnv 1/4 nccl>
16:21:56.830757 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:21:56.851961 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

18:30:04.379776 [1] proc begin: <DistEnv 1/4 nccl>
18:30:09.246943 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
18:30:09.267692 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

18:51:38.781434 [1] proc begin: <DistEnv 1/4 nccl>
18:51:43.651385 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
18:51:43.675976 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:31:22.230084 [1] proc begin: <DistEnv 1/4 nccl>
14:31:30.012560 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
14:31:30.033097 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:31:31.008101 [1] L1 tensor(77137.2891, device='cuda:1', grad_fn=<SumBackward0>) tensor(260.2319, device='cuda:1', grad_fn=<SumBackward0>)
14:31:59.825078 [1] proc begin: <DistEnv 1/4 nccl>
14:32:06.395928 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
14:32:06.417420 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:32:07.514213 [1] L1 tensor(38548.0586, device='cuda:1', grad_fn=<SumBackward0>) tensor(120.5969, device='cuda:1', grad_fn=<SumBackward0>)
16:15:53.312901 [1] proc begin: <DistEnv 1/4 nccl>
16:15:59.290241 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:15:59.311350 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:44:48.310942 [1] proc begin: <DistEnv 1/4 nccl>
16:44:54.502326 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:44:54.522540 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:16:27.799541 [1] proc begin: <DistEnv 1/4 nccl>
21:16:33.770810 [1] proc begin: <DistEnv 1/4 nccl>
21:16:39.680616 [1] proc begin: <DistEnv 1/4 nccl>
21:16:45.388834 [1] proc begin: <DistEnv 1/4 nccl>
21:16:50.628714 [1] proc begin: <DistEnv 1/4 nccl>
21:17:00.903835 [1] proc begin: <DistEnv 1/4 nccl>
21:17:06.541228 [1] proc begin: <DistEnv 1/4 nccl>
21:17:11.790880 [1] proc begin: <DistEnv 1/4 nccl>
21:20:15.211973 [1] proc begin: <DistEnv 1/4 nccl>
21:21:38.334869 [1] proc begin: <DistEnv 1/4 nccl>
21:23:12.210828 [1] proc begin: <DistEnv 1/4 nccl>
21:23:43.005383 [1] graph loaded <COO Graph: e80M_f512_l32_t0.5, |V|: 2000000, |E|: 80000000, masks: 1000000,200000,800000><Local: 1, |V|: 500000, |E|: 15993654>
21:23:43.013610 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1129 MiB |   1145 MiB |   1179 MiB |  51113 KiB |
|       from large pool |   1129 MiB |   1145 MiB |   1179 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1129 MiB |   1145 MiB |   1179 MiB |  51113 KiB |
|       from large pool |   1129 MiB |   1145 MiB |   1179 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1127 MiB |   1142 MiB |   1174 MiB |  48832 KiB |
|       from large pool |   1127 MiB |   1142 MiB |   1174 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1164 MiB |   1164 MiB |   1164 MiB |      0 B   |
|       from large pool |   1162 MiB |   1162 MiB |   1162 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16544 KiB |  19998 KiB |  50987 KiB |  34442 KiB |
|       from large pool |  16544 KiB |  19998 KiB |  44838 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:25:14.532012 [1] proc begin: <DistEnv 1/4 nccl>
21:25:17.943119 [1] graph loaded <COO Graph: e80M_f512_l32_t0.5, |V|: 2000000, |E|: 80000000, masks: 1000000,200000,800000><Local: 1, |V|: 500000, |E|: 15993654>
21:25:17.950913 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1129 MiB |   1145 MiB |   1179 MiB |  51113 KiB |
|       from large pool |   1129 MiB |   1145 MiB |   1179 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1129 MiB |   1145 MiB |   1179 MiB |  51113 KiB |
|       from large pool |   1129 MiB |   1145 MiB |   1179 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1127 MiB |   1142 MiB |   1174 MiB |  48832 KiB |
|       from large pool |   1127 MiB |   1142 MiB |   1174 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1164 MiB |   1164 MiB |   1164 MiB |      0 B   |
|       from large pool |   1162 MiB |   1162 MiB |   1162 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16544 KiB |  19998 KiB |  50987 KiB |  34442 KiB |
|       from large pool |  16544 KiB |  19998 KiB |  44838 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:27:49.421197 [1] proc begin: <DistEnv 1/4 nccl>
21:28:46.921309 [1] graph loaded <COO Graph: e320M_f512_l32_t0.5, |V|: 2000000, |E|: 320000000, masks: 1000000,200000,800000><Local: 1, |V|: 500000, |E|: 63351248>
21:28:46.933493 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1490 MiB |   1506 MiB |   1540 MiB |  51113 KiB |
|       from large pool |   1490 MiB |   1506 MiB |   1540 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1490 MiB |   1506 MiB |   1540 MiB |  51113 KiB |
|       from large pool |   1490 MiB |   1506 MiB |   1540 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1488 MiB |   1503 MiB |   1536 MiB |  48832 KiB |
|       from large pool |   1488 MiB |   1503 MiB |   1536 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1524 MiB |   1524 MiB |   1524 MiB |      0 B   |
|       from large pool |   1522 MiB |   1522 MiB |   1522 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16091 KiB |  19998 KiB |  50695 KiB |  34603 KiB |
|       from large pool |  16091 KiB |  19998 KiB |  44546 KiB |  28455 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       3    |       9    |       7    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:34:17.830422 [1] proc begin: <DistEnv 1/4 nccl>
21:34:46.976102 [1] graph loaded <COO Graph: e160M_f256_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 1, |V|: 500000, |E|: 31008983>
21:34:46.981320 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 773888 KiB |    771 MiB |    805 MiB |  51113 KiB |
|       from large pool | 773888 KiB |    771 MiB |    805 MiB |  51105 KiB |
|       from small pool |      0 KiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 773888 KiB |    771 MiB |    805 MiB |  51113 KiB |
|       from large pool | 773888 KiB |    771 MiB |    805 MiB |  51105 KiB |
|       from small pool |      0 KiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 771554 KiB |    768 MiB |    801 MiB |  48832 KiB |
|       from large pool | 771554 KiB |    768 MiB |    801 MiB |  48828 KiB |
|       from small pool |      0 KiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    792 MiB |    792 MiB |    792 MiB |      0 B   |
|       from large pool |    790 MiB |    790 MiB |    790 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18687 KiB |  20735 KiB |  53130 KiB |  34442 KiB |
|       from large pool |  18687 KiB |  20286 KiB |  46981 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:37:53.854355 [1] proc begin: <DistEnv 1/4 nccl>
21:38:49.183775 [1] graph loaded <COO Graph: e160M_f1024_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 1, |V|: 500000, |E|: 31008983>
21:38:49.189471 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   2221 MiB |   2237 MiB |   2271 MiB |  51113 KiB |
|       from large pool |   2221 MiB |   2237 MiB |   2271 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   2221 MiB |   2237 MiB |   2271 MiB |  51113 KiB |
|       from large pool |   2221 MiB |   2237 MiB |   2271 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   2218 MiB |   2233 MiB |   2266 MiB |  48832 KiB |
|       from large pool |   2218 MiB |   2233 MiB |   2265 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2256 MiB |   2256 MiB |   2256 MiB |      0 B   |
|       from large pool |   2254 MiB |   2254 MiB |   2254 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16927 KiB |  18975 KiB |  51370 KiB |  34442 KiB |
|       from large pool |  16927 KiB |  18526 KiB |  45221 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       6    |      10    |       5    |
|       from large pool |       5    |       5    |       7    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:43:37.636987 [1] proc begin: <DistEnv 1/4 nccl>
21:44:22.989768 [1] graph loaded <COO Graph: e160M_f512_l16_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 1, |V|: 500000, |E|: 31008983>
21:44:23.000832 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1241 MiB |   1257 MiB |   1289 MiB |  48832 KiB |
|       from large pool |   1241 MiB |   1257 MiB |   1289 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1280 MiB |   1280 MiB |   1280 MiB |      0 B   |
|       from large pool |   1278 MiB |   1278 MiB |   1278 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18399 KiB |  20447 KiB |  52842 KiB |  34442 KiB |
|       from large pool |  18399 KiB |  19998 KiB |  46693 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:47:47.051963 [1] proc begin: <DistEnv 1/4 nccl>
21:47:53.813215 [1] graph loaded <COO Graph: e160M_f512_l64_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 1, |V|: 500000, |E|: 31008983>
21:47:53.818747 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1241 MiB |   1257 MiB |   1289 MiB |  48832 KiB |
|       from large pool |   1241 MiB |   1257 MiB |   1289 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1280 MiB |   1280 MiB |   1280 MiB |      0 B   |
|       from large pool |   1278 MiB |   1278 MiB |   1278 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18399 KiB |  20447 KiB |  52842 KiB |  34442 KiB |
|       from large pool |  18399 KiB |  19998 KiB |  46693 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:52:08.213131 [1] proc begin: <DistEnv 1/4 nccl>
21:52:50.554216 [1] graph loaded <COO Graph: e160M_f512_l32_t0.1, |V|: 2000000, |E|: 160000000, masks: 200000,200000,1600000><Local: 1, |V|: 500000, |E|: 31008983>
21:52:50.562323 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1241 MiB |   1257 MiB |   1289 MiB |  48832 KiB |
|       from large pool |   1241 MiB |   1257 MiB |   1289 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1280 MiB |   1280 MiB |   1280 MiB |      0 B   |
|       from large pool |   1278 MiB |   1278 MiB |   1278 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18399 KiB |  20447 KiB |  52842 KiB |  34442 KiB |
|       from large pool |  18399 KiB |  19998 KiB |  46693 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:53:03.461581 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:05.438603 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:06.454016 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:07.470635 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:08.490609 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:09.508222 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:10.527050 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:11.546328 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:12.565490 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:13.585919 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:14.607915 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:15.626211 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:16.641787 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:17.656474 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:18.670773 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:19.685021 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:20.700785 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:21.715404 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:22.729609 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:23.745155 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:24.759891 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:25.774251 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:26.786697 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:27.798698 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:28.812294 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:29.826577 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:30.839937 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:31.852824 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:32.865939 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:33.879027 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:34.891682 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:35.905883 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:36.919269 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:37.931602 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:38.944808 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:39.957425 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:40.971323 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:41.984029 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:42.996031 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:44.008819 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:45.020854 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:46.032442 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:47.044851 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:48.057514 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:49.071173 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:50.083370 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:51.095464 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:52.107912 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:53.123051 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:54.135385 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:55.147930 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:55.698603 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:57.028184 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:57.578435 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:58.906347 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:59.455769 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:00.785163 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:01.334635 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:02.686278 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:03.248124 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:04.579409 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:05.130258 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:06.461481 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:07.011737 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:08.342726 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:08.894111 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:10.224536 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:10.776083 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:12.107871 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:12.658383 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:13.988479 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:14.539222 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:15.868499 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:16.417964 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:17.745664 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:18.295038 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:19.624009 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:20.174200 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:21.503039 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:22.052870 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:23.380368 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:23.931333 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:25.259917 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:25.810741 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:27.139501 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:27.690079 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:29.019841 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:29.570360 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:30.897783 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:31.447037 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:32.775239 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:33.324650 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:34.651457 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:35.199414 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:36.526463 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:37.075537 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:38.403232 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:38.952114 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:40.280390 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:40.829475 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:42.155932 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:42.704332 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:44.031320 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:44.579328 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:45.906697 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:46.455309 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:47.783487 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:48.332847 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:49.660521 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:50.209229 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:51.535983 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:52.086478 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:53.415107 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:53.965464 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:55.294153 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:55.844791 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:57.174141 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:57.723643 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:59.052340 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:59.602698 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:00.931148 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:01.484534 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:02.829902 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:03.381896 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:04.715319 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:05.265766 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:06.598361 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:07.149218 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:08.482097 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:09.036171 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:10.369527 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:10.919993 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:12.250894 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:12.800779 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:14.129656 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:14.680319 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:16.009866 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:16.560538 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:17.890825 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:18.440068 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:19.771671 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:20.321509 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:21.652574 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:22.201743 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:23.532898 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:24.081907 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:25.415566 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:25.966226 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:27.299494 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:27.850312 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:29.188752 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:29.741895 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:31.075667 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:31.626373 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:32.960679 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:33.511321 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:34.841164 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:35.391549 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:36.722001 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:37.272773 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:38.605389 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:39.156468 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:40.484848 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:41.033801 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:42.362772 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:42.913103 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:44.241758 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:44.791908 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:46.119796 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:46.670659 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:47.999293 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:48.549194 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:49.878937 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:50.429038 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:51.756187 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:52.306071 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:53.633104 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:54.181949 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:55.509275 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:56.058685 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:57.387228 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:57.938159 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:59.266655 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:59.817502 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.146538 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.696386 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.045656 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.601355 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:04.932764 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:05.483498 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:06.817932 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:07.368467 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:08.702260 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:09.254073 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:10.586401 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:11.136827 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:12.466416 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:13.016551 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:14.347287 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:14.898870 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:22.200207 [1] proc begin: <DistEnv 1/4 nccl>
21:57:13.261373 [1] graph loaded <COO Graph: e160M_f512_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 1, |V|: 500000, |E|: 31008983>
21:57:13.273222 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1241 MiB |   1257 MiB |   1289 MiB |  48832 KiB |
|       from large pool |   1241 MiB |   1257 MiB |   1289 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1280 MiB |   1280 MiB |   1280 MiB |      0 B   |
|       from large pool |   1278 MiB |   1278 MiB |   1278 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18399 KiB |  20447 KiB |  52842 KiB |  34442 KiB |
|       from large pool |  18399 KiB |  19998 KiB |  46693 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:00:46.768767 [1] proc begin: <DistEnv 1/4 nccl>
22:00:51.916861 [1] graph loaded <COO Graph: e160M_f512_l32_t0.8, |V|: 2000000, |E|: 160000000, masks: 1600000,200000,200000><Local: 1, |V|: 500000, |E|: 31008983>
22:00:51.922613 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1241 MiB |   1257 MiB |   1289 MiB |  48832 KiB |
|       from large pool |   1241 MiB |   1257 MiB |   1289 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1280 MiB |   1280 MiB |   1280 MiB |      0 B   |
|       from large pool |   1278 MiB |   1278 MiB |   1278 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18399 KiB |  20447 KiB |  52842 KiB |  34442 KiB |
|       from large pool |  18399 KiB |  19998 KiB |  46693 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:57:15.212210 [1] proc begin: <DistEnv 1/4 nccl>
15:57:28.030475 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
15:57:28.048568 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:59:24.613792 [1] proc begin: <DistEnv 1/4 nccl>
15:59:30.480151 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
15:59:30.496412 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:00:13.805457 [1] proc begin: <DistEnv 1/4 nccl>
16:00:19.785193 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:00:19.805562 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:03:35.227493 [1] proc begin: <DistEnv 1/4 nccl>
16:03:40.721975 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:03:40.744606 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:12:44.071480 [1] proc begin: <DistEnv 1/4 nccl>
16:12:49.248309 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:12:49.277635 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:20:23.719708 [1] proc begin: <DistEnv 1/4 nccl>
16:20:29.499646 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:20:29.520210 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:23:55.836388 [1] proc begin: <DistEnv 1/4 nccl>
16:24:00.395961 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:24:00.417058 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:28:41.762584 [1] proc begin: <DistEnv 1/4 nccl>
16:28:47.702729 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:28:47.729127 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:29:40.272721 [1] proc begin: <DistEnv 1/4 nccl>
16:29:45.796587 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:29:45.818693 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:33:59.140218 [1] proc begin: <DistEnv 1/4 nccl>
16:34:06.199651 [1] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 1, |V|: 58242, |E|: 28900956>
16:34:06.220186 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367349 KiB | 369174 KiB | 372823 KiB |   5473 KiB |
|       from large pool | 365753 KiB | 367576 KiB | 371222 KiB |   5469 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 366161 KiB | 367981 KiB | 371621 KiB |   5460 KiB |
|       from large pool | 364568 KiB | 366388 KiB | 370028 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  25866 KiB |  25866 KiB |  32708 KiB |   6841 KiB |
|       from large pool |  25414 KiB |  25414 KiB |  30883 KiB |   5469 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:36:42.095525 [1] proc begin: <DistEnv 1/4 nccl>
16:36:55.841974 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
16:36:55.853760 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:37:04.372699 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:05.881849 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:06.644920 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:07.405559 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:08.165136 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:08.924799 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:09.683738 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:10.443255 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:11.203502 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:11.962432 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:12.724135 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:13.483819 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:14.243056 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:15.001177 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:15.760382 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:16.517129 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:17.274827 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:18.033362 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:18.791982 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:19.549806 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:20.308837 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:21.066814 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:21.823703 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:22.582826 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:23.340875 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:24.100795 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:24.859342 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:25.617847 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:26.375991 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:27.132906 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:27.891433 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:28.649909 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:29.408890 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:30.168242 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:30.925845 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:31.684316 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:32.442123 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:33.200404 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:33.959820 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:34.718771 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:35.477804 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:36.236799 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:36.995056 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:37.754980 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:38.513727 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:39.272208 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:40.030449 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:40.789683 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:41.546622 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:42.304206 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:43.061705 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:43.820175 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:44.578189 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:45.335409 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:46.093259 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:46.852717 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:47.609478 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:48.368362 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:49.125100 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:49.883005 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:50.640230 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:51.398613 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:52.156389 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:52.914079 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:53.672289 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:54.430817 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:55.188932 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:55.946763 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:56.704652 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:57.462238 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:58.221548 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:58.981576 [1] Warning: no training nodes in this partition! Backward fake loss.
16:37:59.739012 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:00.497793 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:01.255964 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:02.035208 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:02.825875 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:03.590834 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:04.351006 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:05.111908 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:05.872205 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:06.631682 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:07.394090 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:08.154809 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:08.913842 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:09.673781 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:10.435088 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:11.196025 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:11.957182 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:12.716493 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:13.476353 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:14.236275 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:14.994841 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:15.754973 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:16.515006 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:17.274704 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:18.035755 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:18.796505 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:19.556297 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:20.317001 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:21.075237 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:21.833922 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:22.593027 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:23.352595 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:24.112365 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:24.872161 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:25.631519 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:26.391286 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:27.151255 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:27.910918 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:28.670021 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:29.428024 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:30.187541 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:30.945627 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:31.704052 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:32.463073 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:33.220254 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:33.978906 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:34.736001 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:35.493786 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:36.251727 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:37.008987 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:37.768723 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:38.527039 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:39.285612 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:40.044983 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:40.801628 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:41.559995 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:42.318365 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:43.077053 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:43.835762 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:44.593694 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:45.352188 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:46.110112 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:46.868322 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:47.626795 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:48.384933 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:49.144774 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:49.904222 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:50.662829 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:51.420864 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:52.178364 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:52.936612 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:53.694761 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:54.453158 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:55.211323 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:55.968879 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:56.726844 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:57.484793 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:58.242755 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:59.001687 [1] Warning: no training nodes in this partition! Backward fake loss.
16:38:59.759480 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:00.517206 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:01.275116 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:02.033486 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:02.822584 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:03.605894 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:04.364649 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:05.122666 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:05.881005 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:06.639509 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:07.397893 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:08.157261 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:08.915326 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:09.672781 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:10.430562 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:11.189356 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:11.947552 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:12.705761 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:13.463880 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:14.222588 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:14.980739 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:15.739194 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:16.497680 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:17.257710 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:18.017013 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:18.775659 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:19.534430 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:20.293628 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:21.053028 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:21.812050 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:22.569888 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:23.327741 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:24.086930 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:24.844799 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:25.603119 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:26.361523 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:27.119352 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:27.879129 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:28.638782 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:29.398566 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:30.156427 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:30.915612 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:31.675209 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:32.435125 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:33.194834 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:33.954306 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:34.713223 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:35.472999 [1] Warning: no training nodes in this partition! Backward fake loss.
16:39:36.231894 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:25.140093 [1] proc begin: <DistEnv 1/4 nccl>
16:45:29.547127 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
16:45:29.555085 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:45:34.687312 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:36.319898 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:37.074040 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:37.829320 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:38.588166 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:39.345353 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:40.100830 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:40.858283 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:41.615758 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:42.373353 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:43.131029 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:43.889041 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:44.645508 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:45.402766 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:46.160171 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:46.917925 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:47.673388 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:48.429598 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:49.186647 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:49.943822 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:50.698809 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:51.455861 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:52.212213 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:52.968183 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:53.725300 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:54.480875 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:55.236992 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:55.992679 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:56.750233 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:57.506709 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:58.262096 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:59.019430 [1] Warning: no training nodes in this partition! Backward fake loss.
16:45:59.775006 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:00.530805 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:01.292327 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:02.081644 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:02.857922 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:03.615471 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:04.372412 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:05.128865 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:05.887533 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:06.644623 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:07.401505 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:08.161100 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:08.920952 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:09.681040 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:10.443190 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:11.203957 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:11.965447 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:12.725843 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:13.486922 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:14.246527 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:15.006270 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:15.765293 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:16.525566 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:17.284885 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:18.044344 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:18.804983 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:19.564614 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:20.323005 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:21.081372 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:21.840139 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:22.599282 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:23.357419 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:24.117100 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:24.876301 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:25.635759 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:26.394239 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:27.152998 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:27.911598 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:28.669922 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:29.428957 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:30.187827 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:30.945844 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:31.704718 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:32.464545 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:33.224511 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:33.982677 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:34.741216 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:35.500133 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:36.259276 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:37.018413 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:37.777224 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:38.536952 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:39.295331 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:40.054439 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:40.812986 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:41.571185 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:42.329290 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:43.088119 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:43.846977 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:44.605815 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:45.367837 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:46.128850 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:46.890305 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:47.650689 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:48.412244 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:49.173492 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:49.932795 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:50.691580 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:51.450533 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:52.208170 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:52.964559 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:53.722499 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:54.480151 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:55.236906 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:55.993081 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:56.749223 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:57.504782 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:58.261120 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.018374 [1] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.775955 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.533576 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:01.290622 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:02.065264 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:02.853316 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:03.621540 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:04.383316 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:05.144232 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:05.905603 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:06.666222 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:07.427420 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:08.186885 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:08.948003 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:09.709869 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:10.469886 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:11.229186 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:11.986516 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:12.743770 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:13.500627 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.258139 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.015758 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.772626 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.530508 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.287885 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.044372 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.800867 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.558153 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.315942 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.072734 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.829637 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.586556 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.344093 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.102224 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.859910 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.615973 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.371638 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.127362 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.883914 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.638938 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.396147 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.152355 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.908288 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.663838 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.419891 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.176011 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.932114 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.687565 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.443586 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.200256 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.956043 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.712076 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.469042 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.226842 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.981647 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.738894 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.495738 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.252130 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.007393 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.763596 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.519620 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:45.275989 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:46.031342 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:46.787126 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:47.543089 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.298763 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.054570 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.811171 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:50.567046 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:51.322620 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:52.078644 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:52.835534 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:53.592417 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:54.348490 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:55.104030 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:55.859831 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:56.615714 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:57.371770 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:58.127722 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:58.884756 [1] Warning: no training nodes in this partition! Backward fake loss.
16:47:59.643831 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:00.402508 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:01.160399 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:01.948682 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:02.733016 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:03.490289 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:04.248041 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:05.005248 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:05.764639 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:06.522206 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:13.698480 [1] proc begin: <DistEnv 1/4 nccl>
16:48:18.110828 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
16:48:18.120079 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:48:24.419559 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:27.223564 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:29.066641 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:30.909179 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:32.748824 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:34.589547 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:36.432434 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:38.274343 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:40.116586 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:41.957192 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:43.804380 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:45.645272 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:47.487141 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:49.328564 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:51.170230 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:53.012548 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:54.853942 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:56.696182 [1] Warning: no training nodes in this partition! Backward fake loss.
16:48:58.538091 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:00.378407 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:02.241106 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:04.119679 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:05.961807 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:07.803794 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:09.645302 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:11.487764 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:13.328894 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:15.169114 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:17.010318 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:18.847739 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:20.685294 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:22.522592 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:24.359234 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:26.192636 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.027635 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:29.864016 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:31.696583 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:33.530720 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:35.364573 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:37.200202 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:39.036146 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:40.870434 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:42.703123 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:44.537799 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:46.371910 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:48.206378 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:50.039818 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:51.872642 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:53.705636 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:55.538927 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:57.373094 [1] Warning: no training nodes in this partition! Backward fake loss.
16:49:59.207496 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:01.041477 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:02.934069 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:04.774195 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:06.613072 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:08.453076 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:10.293711 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:12.134465 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:13.975686 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:15.813981 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:17.653601 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:19.490353 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:21.328252 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:23.164052 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:25.000761 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:26.837218 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:28.673930 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:30.509993 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:32.348077 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:34.186779 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:36.023932 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:37.861463 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:39.701299 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:41.539513 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:43.377069 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:45.214829 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:47.052442 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:48.890027 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:50.728693 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:52.564585 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:54.397531 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:56.236454 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:58.073266 [1] Warning: no training nodes in this partition! Backward fake loss.
16:50:59.911997 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:01.748453 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:03.640466 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:05.478675 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:07.318061 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:09.159401 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:10.997624 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:12.836082 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:14.676261 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:16.514817 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:18.354918 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:20.194475 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:22.033391 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:23.874044 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:25.715391 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:27.555704 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:29.394927 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:31.234385 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:33.072350 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:34.910222 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:36.747878 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:38.585031 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:40.421426 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:42.258117 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:44.093927 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:45.931917 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:47.768906 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:49.605513 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:51.442944 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:53.279529 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:55.114122 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:56.950928 [1] Warning: no training nodes in this partition! Backward fake loss.
16:51:58.786513 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:00.623165 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:02.505149 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:04.350296 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:06.187033 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:08.024557 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:09.861938 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:11.696100 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:13.532137 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:15.369820 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:17.206780 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:19.042443 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:20.881877 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:22.721216 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:24.557091 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:26.389789 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:28.225791 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:30.062477 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:31.899363 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.735565 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:35.570752 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:37.407135 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:39.245054 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:41.085747 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:42.923806 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:44.759774 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.596101 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:48.433228 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:50.267587 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:52.101674 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:53.935844 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:55.769574 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:57.603824 [1] Warning: no training nodes in this partition! Backward fake loss.
16:52:59.437808 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:01.272863 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:03.159218 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:05.001333 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:06.842860 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:08.685204 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:10.523074 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:12.361247 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:14.195385 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:16.028336 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:17.864372 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:19.698886 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:21.531919 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:23.364579 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:25.198300 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:27.033413 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:28.867131 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:30.702875 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.539633 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:34.376839 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:36.213228 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:38.050282 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:39.885653 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:41.720907 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:43.555002 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:45.388567 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:47.220883 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:49.054875 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:50.889097 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:52.723729 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:54.557299 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:56.393123 [1] Warning: no training nodes in this partition! Backward fake loss.
16:53:58.226843 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:00.061893 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:01.916505 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:03.791078 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:05.628879 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:07.465687 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:09.302208 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:11.141289 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:12.978069 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:14.813418 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:16.647138 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:18.481717 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:20.315620 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:22.148668 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:23.982165 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:25.816671 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:27.650431 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:29.485329 [1] Warning: no training nodes in this partition! Backward fake loss.
16:54:31.320104 [1] Warning: no training nodes in this partition! Backward fake loss.
17:15:58.968943 [1] proc begin: <DistEnv 1/4 nccl>
17:16:04.480371 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
17:16:04.494505 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:16:10.699578 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:13.599603 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:15.448989 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:17.289744 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:19.123613 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:20.958536 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:22.790603 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:24.625126 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:26.458072 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:28.288691 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:30.121615 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:31.953669 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:33.786375 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:35.618151 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:37.451410 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:39.284592 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:41.115277 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:42.949742 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.781812 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.612503 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.445361 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.277590 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:52.109214 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:53.940824 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:55.772436 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.604034 [1] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.436181 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.268283 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.147755 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:04.990247 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.822781 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.660041 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.494781 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.327224 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:14.160299 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:15.991510 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:17.823601 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.656943 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:21.488808 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:23.322072 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:25.156411 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:26.988183 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:28.820566 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:30.653401 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:32.487214 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:34.319911 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:36.151682 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:37.984584 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:39.817016 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:41.650513 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:43.485903 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:45.319977 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:47.152109 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:48.984389 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.818279 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.651369 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:54.483643 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:56.315954 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:58.147842 [1] Warning: no training nodes in this partition! Backward fake loss.
17:17:59.980257 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:01.825423 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:03.703213 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:05.534780 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:07.372371 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:09.208259 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:11.044025 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:12.876992 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:14.711229 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:16.545432 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:18.379286 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:20.210703 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:22.043366 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:23.874884 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:25.707094 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:27.539629 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:29.371567 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:31.205866 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:33.041136 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:34.872409 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:36.704144 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:38.535514 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:40.367904 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:42.199679 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:44.033702 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:45.866722 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:47.699561 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:49.532426 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:51.363618 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:53.195746 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:55.027877 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:56.861323 [1] Warning: no training nodes in this partition! Backward fake loss.
17:18:58.693790 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:00.525349 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:02.370373 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:04.251445 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:06.088646 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:07.926672 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.763865 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.598805 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:13.434447 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:15.265840 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:17.096239 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:18.927758 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:20.759828 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:22.591335 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:24.423810 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:26.256355 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:28.086734 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:29.918880 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:31.749726 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:33.581909 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:35.413018 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:37.244511 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:39.077378 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:40.912443 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:42.744436 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:44.581446 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:46.412068 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:48.245291 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:50.078346 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:51.909188 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:53.739804 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:55.573292 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:57.406340 [1] Warning: no training nodes in this partition! Backward fake loss.
17:19:59.237400 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:01.068650 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:02.960337 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:04.799420 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:06.635571 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:08.472814 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:10.309590 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:12.142182 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:13.974625 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:15.807191 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:17.639128 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:19.471075 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:21.303346 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:23.136882 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:24.968805 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:26.802016 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:28.633403 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:30.466958 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:32.298689 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:34.132152 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:35.963997 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:37.795939 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:39.627728 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:41.461181 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:43.295167 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:45.128335 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:46.961818 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:48.794712 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:50.627480 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:52.460535 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:54.292599 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:56.125042 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:57.957540 [1] Warning: no training nodes in this partition! Backward fake loss.
17:20:59.788736 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:01.641060 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:03.514809 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:05.348684 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:07.180472 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:09.012301 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:10.844220 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:12.675859 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:14.508817 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:16.342626 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:18.173554 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:20.006264 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:21.840611 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:23.673335 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:25.505458 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:27.337930 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:29.171421 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:31.003029 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:32.835340 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:34.669182 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:36.502881 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:38.335450 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:40.168435 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:42.000469 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:43.833164 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:45.664819 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:47.497395 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:49.329926 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:51.161697 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:52.995621 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:54.828405 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:56.661017 [1] Warning: no training nodes in this partition! Backward fake loss.
17:21:58.492186 [1] Warning: no training nodes in this partition! Backward fake loss.
17:22:00.325781 [1] Warning: no training nodes in this partition! Backward fake loss.
17:22:02.179591 [1] Warning: no training nodes in this partition! Backward fake loss.
17:22:04.048114 [1] Warning: no training nodes in this partition! Backward fake loss.
17:22:05.882806 [1] Warning: no training nodes in this partition! Backward fake loss.
17:22:07.717815 [1] Warning: no training nodes in this partition! Backward fake loss.
17:22:09.552869 [1] Warning: no training nodes in this partition! Backward fake loss.
17:22:11.389025 [1] Warning: no training nodes in this partition! Backward fake loss.
17:22:13.221634 [1] Warning: no training nodes in this partition! Backward fake loss.
17:22:15.056217 [1] Warning: no training nodes in this partition! Backward fake loss.
17:22:16.889190 [1] Warning: no training nodes in this partition! Backward fake loss.
17:22:23.949897 [1] proc begin: <DistEnv 1/4 nccl>
17:22:28.372065 [1] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 1, |V|: 612258, |E|: 24546978>
17:22:28.381372 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 467269 KiB | 505541 KiB | 764293 KiB | 297024 KiB |
|       from large pool | 467269 KiB | 505541 KiB | 764286 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466811 KiB | 504634 KiB | 763378 KiB | 296567 KiB |
|       from large pool | 466811 KiB | 504634 KiB | 763373 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  52922 KiB | 216906 KiB | 285239 KiB | 232316 KiB |
|       from large pool |  52922 KiB | 216906 KiB | 279090 KiB | 226168 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:22:34.955285 [1] Warning: no training nodes in this partition! Backward fake loss.
17:22:39.689727 [1] Warning: no training nodes in this partition! Backward fake loss.
17:22:43.690093 [1] Warning: no training nodes in this partition! Backward fake loss.
17:22:47.689761 [1] Warning: no training nodes in this partition! Backward fake loss.
17:22:51.685834 [1] Warning: no training nodes in this partition! Backward fake loss.
17:22:55.679893 [1] Warning: no training nodes in this partition! Backward fake loss.
17:22:59.673340 [1] Warning: no training nodes in this partition! Backward fake loss.
17:23:03.726103 [1] Warning: no training nodes in this partition! Backward fake loss.
17:23:07.722037 [1] Warning: no training nodes in this partition! Backward fake loss.
17:23:11.718571 [1] Warning: no training nodes in this partition! Backward fake loss.
17:23:15.715540 [1] Warning: no training nodes in this partition! Backward fake loss.
17:23:19.709595 [1] Warning: no training nodes in this partition! Backward fake loss.
17:23:23.702884 [1] Warning: no training nodes in this partition! Backward fake loss.
17:23:27.695972 [1] Warning: no training nodes in this partition! Backward fake loss.
17:23:31.688518 [1] Warning: no training nodes in this partition! Backward fake loss.
17:23:35.682891 [1] Warning: no training nodes in this partition! Backward fake loss.
17:23:39.679668 [1] Warning: no training nodes in this partition! Backward fake loss.
17:23:43.674327 [1] Warning: no training nodes in this partition! Backward fake loss.
17:23:47.666999 [1] Warning: no training nodes in this partition! Backward fake loss.
17:23:51.659917 [1] Warning: no training nodes in this partition! Backward fake loss.
17:23:55.655888 [1] Warning: no training nodes in this partition! Backward fake loss.
17:23:59.649265 [1] Warning: no training nodes in this partition! Backward fake loss.
17:24:03.699737 [1] Warning: no training nodes in this partition! Backward fake loss.
17:24:07.696072 [1] Warning: no training nodes in this partition! Backward fake loss.
17:24:11.689107 [1] Warning: no training nodes in this partition! Backward fake loss.
17:24:15.684621 [1] Warning: no training nodes in this partition! Backward fake loss.
17:24:19.677107 [1] Warning: no training nodes in this partition! Backward fake loss.
17:24:23.673145 [1] Warning: no training nodes in this partition! Backward fake loss.
17:24:27.667764 [1] Warning: no training nodes in this partition! Backward fake loss.
17:24:31.660005 [1] Warning: no training nodes in this partition! Backward fake loss.
17:24:35.653691 [1] Warning: no training nodes in this partition! Backward fake loss.
17:24:39.647206 [1] Warning: no training nodes in this partition! Backward fake loss.
17:24:43.640233 [1] Warning: no training nodes in this partition! Backward fake loss.
17:24:47.634366 [1] Warning: no training nodes in this partition! Backward fake loss.
17:24:51.629154 [1] Warning: no training nodes in this partition! Backward fake loss.
17:24:55.621324 [1] Warning: no training nodes in this partition! Backward fake loss.
17:24:59.616300 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:03.666681 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.658432 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:11.650787 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:15.645669 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:19.637159 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:23.628003 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:27.621035 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:31.613438 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:35.606657 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:39.598908 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:43.594122 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:47.589826 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:51.582631 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:55.576083 [1] Warning: no training nodes in this partition! Backward fake loss.
17:25:59.567785 [1] Warning: no training nodes in this partition! Backward fake loss.
17:26:03.620475 [1] Warning: no training nodes in this partition! Backward fake loss.
17:26:07.618002 [1] Warning: no training nodes in this partition! Backward fake loss.
17:26:11.613905 [1] Warning: no training nodes in this partition! Backward fake loss.
17:26:15.607788 [1] Warning: no training nodes in this partition! Backward fake loss.
17:26:19.602215 [1] Warning: no training nodes in this partition! Backward fake loss.
17:26:23.595221 [1] Warning: no training nodes in this partition! Backward fake loss.
17:26:27.590586 [1] Warning: no training nodes in this partition! Backward fake loss.
17:26:31.585895 [1] Warning: no training nodes in this partition! Backward fake loss.
17:26:35.587299 [1] Warning: no training nodes in this partition! Backward fake loss.
17:26:39.584380 [1] Warning: no training nodes in this partition! Backward fake loss.
17:26:43.579809 [1] Warning: no training nodes in this partition! Backward fake loss.
17:26:47.577750 [1] Warning: no training nodes in this partition! Backward fake loss.
17:26:51.576290 [1] Warning: no training nodes in this partition! Backward fake loss.
17:26:55.575012 [1] Warning: no training nodes in this partition! Backward fake loss.
17:26:59.571053 [1] Warning: no training nodes in this partition! Backward fake loss.
17:27:03.625972 [1] Warning: no training nodes in this partition! Backward fake loss.
17:27:07.622895 [1] Warning: no training nodes in this partition! Backward fake loss.
17:27:11.620104 [1] Warning: no training nodes in this partition! Backward fake loss.
17:27:15.616664 [1] Warning: no training nodes in this partition! Backward fake loss.
17:27:19.715696 [1] Warning: no training nodes in this partition! Backward fake loss.
17:27:23.745619 [1] Warning: no training nodes in this partition! Backward fake loss.
17:27:27.816819 [1] Warning: no training nodes in this partition! Backward fake loss.
17:27:31.849641 [1] Warning: no training nodes in this partition! Backward fake loss.
17:27:35.846447 [1] Warning: no training nodes in this partition! Backward fake loss.
17:27:39.844871 [1] Warning: no training nodes in this partition! Backward fake loss.
17:27:43.845570 [1] Warning: no training nodes in this partition! Backward fake loss.
17:27:47.850511 [1] Warning: no training nodes in this partition! Backward fake loss.
17:27:51.844004 [1] Warning: no training nodes in this partition! Backward fake loss.
17:27:55.836981 [1] Warning: no training nodes in this partition! Backward fake loss.
17:27:59.830460 [1] Warning: no training nodes in this partition! Backward fake loss.
17:28:03.884510 [1] Warning: no training nodes in this partition! Backward fake loss.
17:28:07.884038 [1] Warning: no training nodes in this partition! Backward fake loss.
17:28:11.879674 [1] Warning: no training nodes in this partition! Backward fake loss.
17:28:15.878818 [1] Warning: no training nodes in this partition! Backward fake loss.
17:28:19.883667 [1] Warning: no training nodes in this partition! Backward fake loss.
17:28:23.877781 [1] Warning: no training nodes in this partition! Backward fake loss.
17:28:27.871307 [1] Warning: no training nodes in this partition! Backward fake loss.
17:28:31.865871 [1] Warning: no training nodes in this partition! Backward fake loss.
17:28:35.858925 [1] Warning: no training nodes in this partition! Backward fake loss.
17:28:39.854290 [1] Warning: no training nodes in this partition! Backward fake loss.
17:28:43.845945 [1] Warning: no training nodes in this partition! Backward fake loss.
17:28:47.842772 [1] Warning: no training nodes in this partition! Backward fake loss.
17:28:51.836900 [1] Warning: no training nodes in this partition! Backward fake loss.
17:28:55.831098 [1] Warning: no training nodes in this partition! Backward fake loss.
17:28:59.827168 [1] Warning: no training nodes in this partition! Backward fake loss.
17:29:03.878559 [1] Warning: no training nodes in this partition! Backward fake loss.
17:29:07.873917 [1] Warning: no training nodes in this partition! Backward fake loss.
17:29:11.872452 [1] Warning: no training nodes in this partition! Backward fake loss.
17:29:15.869092 [1] Warning: no training nodes in this partition! Backward fake loss.
17:29:19.864883 [1] Warning: no training nodes in this partition! Backward fake loss.
17:29:23.862135 [1] Warning: no training nodes in this partition! Backward fake loss.
17:29:27.858729 [1] Warning: no training nodes in this partition! Backward fake loss.
17:29:31.853106 [1] Warning: no training nodes in this partition! Backward fake loss.
17:29:35.848046 [1] Warning: no training nodes in this partition! Backward fake loss.
17:29:39.842234 [1] Warning: no training nodes in this partition! Backward fake loss.
17:29:43.836678 [1] Warning: no training nodes in this partition! Backward fake loss.
17:29:47.833573 [1] Warning: no training nodes in this partition! Backward fake loss.
17:29:51.829112 [1] Warning: no training nodes in this partition! Backward fake loss.
17:29:55.825147 [1] Warning: no training nodes in this partition! Backward fake loss.
17:29:59.822193 [1] Warning: no training nodes in this partition! Backward fake loss.
17:30:03.877523 [1] Warning: no training nodes in this partition! Backward fake loss.
17:30:07.869322 [1] Warning: no training nodes in this partition! Backward fake loss.
17:30:11.868861 [1] Warning: no training nodes in this partition! Backward fake loss.
17:30:15.867855 [1] Warning: no training nodes in this partition! Backward fake loss.
17:30:19.861853 [1] Warning: no training nodes in this partition! Backward fake loss.
17:30:23.854360 [1] Warning: no training nodes in this partition! Backward fake loss.
17:30:27.846481 [1] Warning: no training nodes in this partition! Backward fake loss.
17:30:31.838849 [1] Warning: no training nodes in this partition! Backward fake loss.
17:30:35.832241 [1] Warning: no training nodes in this partition! Backward fake loss.
17:30:39.825372 [1] Warning: no training nodes in this partition! Backward fake loss.
17:30:43.818583 [1] Warning: no training nodes in this partition! Backward fake loss.
17:30:47.814343 [1] Warning: no training nodes in this partition! Backward fake loss.
17:30:51.808910 [1] Warning: no training nodes in this partition! Backward fake loss.
17:30:55.801441 [1] Warning: no training nodes in this partition! Backward fake loss.
17:30:59.795755 [1] Warning: no training nodes in this partition! Backward fake loss.
17:31:03.848166 [1] Warning: no training nodes in this partition! Backward fake loss.
17:31:07.840367 [1] Warning: no training nodes in this partition! Backward fake loss.
17:31:11.833541 [1] Warning: no training nodes in this partition! Backward fake loss.
17:31:15.825032 [1] Warning: no training nodes in this partition! Backward fake loss.
17:31:19.816775 [1] Warning: no training nodes in this partition! Backward fake loss.
17:31:23.809703 [1] Warning: no training nodes in this partition! Backward fake loss.
17:31:27.801216 [1] Warning: no training nodes in this partition! Backward fake loss.
17:31:31.794728 [1] Warning: no training nodes in this partition! Backward fake loss.
17:31:35.786967 [1] Warning: no training nodes in this partition! Backward fake loss.
17:31:39.778717 [1] Warning: no training nodes in this partition! Backward fake loss.
17:31:43.772156 [1] Warning: no training nodes in this partition! Backward fake loss.
17:31:47.766197 [1] Warning: no training nodes in this partition! Backward fake loss.
17:31:51.760306 [1] Warning: no training nodes in this partition! Backward fake loss.
17:31:55.753216 [1] Warning: no training nodes in this partition! Backward fake loss.
17:31:59.744881 [1] Warning: no training nodes in this partition! Backward fake loss.
17:32:03.799243 [1] Warning: no training nodes in this partition! Backward fake loss.
17:32:07.795026 [1] Warning: no training nodes in this partition! Backward fake loss.
17:32:11.786682 [1] Warning: no training nodes in this partition! Backward fake loss.
17:32:15.779911 [1] Warning: no training nodes in this partition! Backward fake loss.
17:32:19.772475 [1] Warning: no training nodes in this partition! Backward fake loss.
17:32:23.763775 [1] Warning: no training nodes in this partition! Backward fake loss.
17:32:27.754450 [1] Warning: no training nodes in this partition! Backward fake loss.
17:32:31.748164 [1] Warning: no training nodes in this partition! Backward fake loss.
17:32:35.742291 [1] Warning: no training nodes in this partition! Backward fake loss.
17:32:39.734493 [1] Warning: no training nodes in this partition! Backward fake loss.
17:32:43.725292 [1] Warning: no training nodes in this partition! Backward fake loss.
17:32:47.717952 [1] Warning: no training nodes in this partition! Backward fake loss.
17:32:51.709015 [1] Warning: no training nodes in this partition! Backward fake loss.
17:32:55.701515 [1] Warning: no training nodes in this partition! Backward fake loss.
17:32:59.694019 [1] Warning: no training nodes in this partition! Backward fake loss.
17:33:03.742404 [1] Warning: no training nodes in this partition! Backward fake loss.
17:33:07.743266 [1] Warning: no training nodes in this partition! Backward fake loss.
17:33:11.737699 [1] Warning: no training nodes in this partition! Backward fake loss.
17:33:15.732730 [1] Warning: no training nodes in this partition! Backward fake loss.
17:33:19.729530 [1] Warning: no training nodes in this partition! Backward fake loss.
17:33:23.724765 [1] Warning: no training nodes in this partition! Backward fake loss.
17:33:27.718984 [1] Warning: no training nodes in this partition! Backward fake loss.
17:33:31.711789 [1] Warning: no training nodes in this partition! Backward fake loss.
17:33:35.707526 [1] Warning: no training nodes in this partition! Backward fake loss.
17:33:39.700612 [1] Warning: no training nodes in this partition! Backward fake loss.
17:33:43.695389 [1] Warning: no training nodes in this partition! Backward fake loss.
17:33:47.690533 [1] Warning: no training nodes in this partition! Backward fake loss.
17:33:51.684608 [1] Warning: no training nodes in this partition! Backward fake loss.
17:33:55.680302 [1] Warning: no training nodes in this partition! Backward fake loss.
17:33:59.672361 [1] Warning: no training nodes in this partition! Backward fake loss.
17:34:03.723923 [1] Warning: no training nodes in this partition! Backward fake loss.
17:34:07.719369 [1] Warning: no training nodes in this partition! Backward fake loss.
17:34:11.713737 [1] Warning: no training nodes in this partition! Backward fake loss.
17:34:15.708318 [1] Warning: no training nodes in this partition! Backward fake loss.
17:34:19.702547 [1] Warning: no training nodes in this partition! Backward fake loss.
17:34:23.696835 [1] Warning: no training nodes in this partition! Backward fake loss.
17:34:27.692267 [1] Warning: no training nodes in this partition! Backward fake loss.
17:34:31.687766 [1] Warning: no training nodes in this partition! Backward fake loss.
17:34:35.681765 [1] Warning: no training nodes in this partition! Backward fake loss.
17:34:39.676454 [1] Warning: no training nodes in this partition! Backward fake loss.
17:34:43.673442 [1] Warning: no training nodes in this partition! Backward fake loss.
17:34:47.668817 [1] Warning: no training nodes in this partition! Backward fake loss.
17:34:51.662503 [1] Warning: no training nodes in this partition! Backward fake loss.
17:34:55.657075 [1] Warning: no training nodes in this partition! Backward fake loss.
17:34:59.651981 [1] Warning: no training nodes in this partition! Backward fake loss.
17:35:03.704355 [1] Warning: no training nodes in this partition! Backward fake loss.
17:35:07.700158 [1] Warning: no training nodes in this partition! Backward fake loss.
17:35:11.694274 [1] Warning: no training nodes in this partition! Backward fake loss.
17:35:15.689663 [1] Warning: no training nodes in this partition! Backward fake loss.
17:35:19.683547 [1] Warning: no training nodes in this partition! Backward fake loss.
17:35:23.677751 [1] Warning: no training nodes in this partition! Backward fake loss.
17:35:27.672001 [1] Warning: no training nodes in this partition! Backward fake loss.
17:35:31.667608 [1] Warning: no training nodes in this partition! Backward fake loss.
17:35:35.663645 [1] Warning: no training nodes in this partition! Backward fake loss.
17:35:39.659464 [1] Warning: no training nodes in this partition! Backward fake loss.
17:35:43.652731 [1] Warning: no training nodes in this partition! Backward fake loss.
17:35:47.646582 [1] Warning: no training nodes in this partition! Backward fake loss.
17:35:51.641979 [1] Warning: no training nodes in this partition! Backward fake loss.
